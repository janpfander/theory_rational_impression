---
title: "The rational impression account of trust in science"
# If blank, the running header is the title in upper case.
shorttitle: ""
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jan Pfänder
    corresponding: true
    orcid: 0009-0009-4389-2807
    email: janlukas.pfaender@gmail.com
    affiliations:
      - id: id1  # Added an explicit ID for referencing
        name: Swiss Federal Institute of Aquatic Science and Technology (Eawag)
        department: Department of Environmental Social Sciences
  - name: Hugo Mercier
    corresponding: false
    orcid: 0000-0002-0575-7913
    email: hugo.mercier@gmail.com
    affiliations: 
      - ref: id2
        name: ENS, EHESS, PSL University, CNRS, France
        department: Institut Jean Nicod, Département d’études cognitives
blank-lines-above-author-note: 2
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: ~
    # Example: [Author name] is deceased.
    deceased: ~
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: ~
    # Acknowledge and cite data/materials to be shared.
    data-sharing: ~
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: ~
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
    financial-support: ~
    # Example: The authors are grateful to [Person] for [Reason].
    gratitude: ~
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    authorship-agreements: ~
abstract: "Trust in science plays a crucial role in addressing major societal challenges, from climate change to global health. In a wide range of countries, most people tend to trust science. However this trust might seem irrational, since people tend to know little about science. Here, we argue that people need not possess much knowledge or understanding of science to rationally trust it. We propose a cognitive model of trust in science—the rational impression account—according to which people come to trust science by relying on a suite of basic cognitive mechanisms: First, people infer competence from possessing rare knowledge; Second, people infer accuracy from consensus; Third, people’s impressions can persist after they forget what generated them. The rational impression account stresses the importance of science education and communication in fostering public trust in science."
  
  
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [trust in science, science literacy, deficit model, epistemic vigilance, consensus]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
impact-statement: ~
# If true, a word count will appear below the keywords (tables, figure captions, and references excluded in count.)
word-count: true
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: bibliography.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: true
# Masks references that appear in the masked-citations list
mask: false
masked-citations:
# If true, adds today's date below author affiliations. If text, can be any value.
# This is not standard APA format, but it is convenient.
# Works with docx, html, and typst. 
draft-date: true
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "Email"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; [credit.niso.org](https://credit.niso.org)) as follows:"
  title-impact-statement: "Impact Statement"
  references-meta-analysis: "References marked with an asterisk indicate studies included in the meta-analysis."
format:
  apaquarto-docx: 
    toc: false
  apaquarto-html: 
    toc: true
  apaquarto-typst: 
    keep-typ: true
    list-of-figures: false
    list-of-tables: false
    toc: false
    papersize: "us-letter"
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: man
    keep-tex: true

header-includes:  | # to prevent floats from moving past certain points (for the appendix)
  \usepackage{placeins} 
    
always_allow_html: true
---

```{r}
#| label: setup
#| include: false
#| message: false
library(tidyverse)
library(flextable)
library(tinytable)
library(kableExtra)
```

```{r}
#| label: functions
#| include: false
#| message: false
# load plot theme
source("functions/plot_theme.R") 

# load other functions
source("functions/functions.R")
```

```{r}
#| label: gss
#| include: false
#| message: false
# read gss data
gss <- read_csv("gss_cross/data/gss_cross_cleaned.csv")


# Average confidence levels by institution and year
gss_summary_numeric <- gss |>
  group_by(year, institution, institution_label) |>
  summarize(mean_confidence = mean(confidence, na.rm = TRUE)) |> 
  ungroup() |> 
  # remove years without measure
  drop_na(mean_confidence )


# Share of people with great deal of trust by institution and year
gss_summary_great_deal <- gss |>
  drop_na(confidence) |> 
  group_by(year, institution, institution_label) |>
  summarize(
    total_responses = n(),
    great_deal_count = sum(confidence_factor == "A great deal confidence", na.rm = TRUE),
    share = great_deal_count / total_responses,
    # remove 0s
    share = ifelse(share == 0, NA, share)
  ) |>
  ungroup() |> 
  # remove years without measure
  drop_na(share)

# check the four on average top trusted institutions using the mean
top_trusted_institutions <- gss |>
  group_by(institution, institution_label) |>
  summarise(mean_confidence = mean(confidence, na.rm = TRUE)) |>
  ungroup() |> 
  arrange(desc(mean_confidence)) |>
  slice_head(n = 4)  # Select top 4

# check the four top institutions using highest share of great deal of trust
top_intitutions_great_deal <- gss_summary_great_deal |> 
  group_by(institution, institution_label) |> 
  summarize(mean_share = mean(share, na.rm=TRUE)) |> 
  ungroup() |> 
  arrange(desc(mean_share)) |>
  # Select top 4
  slice_head(n = 4) |> 
  rounded_numbers() |> 
  mutate(mean_share = paste0(round(mean_share * 100), "%")) %>% 
  split(.$institution)

# get the number of institutions
n_institutions <- gss |> 
  distinct(institution, institution_label) |> 
  nrow()

# get the minimum and maximum range for people with great deal of confidence in trust in science 
max_min_trust <- gss_summary_great_deal |>
  filter(institution == "consci") |>
  filter(share == min(share) | share == max(share)) |> 
  rounded_numbers() |> 
  mutate(share = paste0(round(share * 100), "%")) %>%
  split(.$share)

# get average yearly deviation from long-term mean
yearly_variation <- gss_summary_great_deal |>
  group_by(institution) |> 
  summarise(average_deviation = mean(abs(share - mean(share, na.rm = TRUE)), na.rm = TRUE), 
            sd = sd(share, na.rm = TRUE)) |> 
  rounded_numbers() |> 
  mutate(across(where(is.numeric), ~.x * 100)) |> 
  arrange(average_deviation) %>%
  split(.$institution)

```

# Introduction

Addressing important societal challenges, from fighting climate change to managing pandemics, is greatly facilitated by trust in science. Studies have demonstrated that individuals with higher levels of trust in science are more likely to accept the scientific consensus on global warming [@bogertEffectTrustScience2024], as well as to engage in pro-environmental behavior and support climate policies [@colognaRoleTrustClimate2020; @hornseyMetaanalysesDeterminantsOutcomes2016]. Trust in science has also been shown to be positively associated with willingness to get vaccinated [@sturgisTrustScienceSocial2021; for Covid-19 in particular, @lindholtPublicAcceptanceCOVID192021]. During the Covid-19 pandemic, a panel study in 12 countries found that trust in scientists was the strongest predictor of whether people followed public health guidelines, such as mask-wearing or social distancing [@alganTrustScientistsTimes2021, see also @koetkeTrustScienceIncreases2021].

Given the individual and social cost of a lack of trust in science, most studies have focused on understanding why some people do not trust science. However, it also important to understand why most people *do* trust science: it is important theoretically, as this trust could stem from very different processes–from blind deference to a rational assessment of scientific evidence; it is important practically, as, depending on why people trust science, different interventions aimed at increasing trust in science could be conceived. Here, we argue that, even though people do not know much about science, their trust in science can still be rational.

We start by reviewing existing explanations for why people trust or do not trust science, arguing that this work has not fully solved a basic puzzle: Why do most people tend to trust science, in spite of knowing so little about it?

To solve this puzzle, we then develop a *rational impression account* of trust in science. According to this account, people do not need a profound understanding or detailed knowledge of science to rationally trust it. Instead, by appealing to basic cognitive mechanisms of information evaluation, science impresses people, who then mostly forget the information that had impressed them.

# The puzzle of why people trust science

A widely agreed-upon definition of trust is the willingness to be vulnerable to another party–whether an individual, a group, or an institution [@mayerIntegrativeModelOrganizational1995a; @rousseauIntroductionSpecialTopic1998]. Building on this idea, trust in science has been defined as “one’s willingness to rely on science and scientists (as representatives of the system) despite having a bounded understanding of science” [@wintterlinPredictingPublicTrust2022, p. 2]. This definition implies that trust in science goes beyond knowledge of science. Yet, the idea that knowledge of science is the primary cause of trust in–and more generally positive attitudes towards–science has long dominated research on public understanding of science [@bauerWhatCanWe2007]. This idea is widely known under the term "deficit model," because much of the literature attested to the public's "depressingly low levels of scientific knowledge" that were assumed to be the principal cause of negative attitudes towards science [@sturgisScienceSocietyReEvaluating2004, p. 56].

The deficit model has been criticized for idealizing science and viewing the public as deficient and irrational [@bauerWhatCanWe2007; @gauchatCulturalAuthorityScience2011]. Moreover, as reviewed below, the relationship between science knowledge and trust in science is rather tenuous. As a result, the literature has mostly moved beyond the idea of science knowledge as the main driver of trust in science. However, the focus on explaining a lack of trust, rather than trust, persists.

Researchers have increasingly turned to how people's values, world views, and identities shape their attitudes towards science [@hornseyAttitudeRootsJiu2017a; @lewandowskyWorldviewmotivatedRejectionScience2021]. The psychological literature has focused on explaining negative attitudes towards science with motivated reasoning–selecting and interpreting information to match one's existing beliefs or behaviors [@lewandowskyMotivatedRejectionScience2016; @hornseyWhyFactsAre2020; @lewandowskyRoleConspiracistIdeation2013]. This research mostly suggests that certain psychological traits, such as a social dominance orientation, or a tendency to engage in conspiratorial thinking, lead people to reject science. Arguments on a general conspiratorial thinking style as one of the root causes of science rejection shift the debate, to some extent, from a knowledge deficit to a broader reasoning deficit [@hornseyAttitudeRootsJiu2017a; @rutjensConspiracyBeliefsScience2022].

Since the Covid-19 pandemic, the role of misinformation in fostering distrust in science has received more attention [@nationalacademiesofsciencesUnderstandingAddressingMisinformation2024; @scheufeleScienceAudiencesMisinformation2019; @druckmanThreatsSciencePoliticization2022]. For this literature, by contrast with the deficit model, the problem of trust in science is less a lack of information, and more the abundance of harmful information.

Another explanation for distrust towards science is the alienation model [@gauchatCulturalAuthorityScience2011]. According to this model, the "public disassociation with science is a symptom of a general disenchantment with late modernity, mainly, the limitations associated with codified expertise, rational bureaucracy, and institutional authority” [@gauchatCulturalAuthorityScience2011, p.2]. This explanation builds on the work of social theorists [@habermasJurgenHabermasSociety1989; @beckRiskSocietyNew1992; @giddensModernitySelfidentitySelf1991; see @gauchatCulturalAuthorityScience2011 for an overview] who suggested that a modern, complex world increasingly requires expertise, and thus shapes institutions run by knowledge elites. People who are not part of these institutions would experience a lack of agency, resulting in a feeling of alienation.

Besides enquiring into the causes of (mis)trust in science, scholars have also sought to better understand what that (mis)trust consist in, turning for instance to the different dimensions along which scientists are perceived [@intemannScienceCommunicationPublic2023], including competence, integrity, benevolence, and openness [@hendriksMeasuringLaypeoplesTrust2015; @besleyReassessingVariablesUsed2021a]. This literature suggests that, for improving trust in science, the latter, warmth-related dimensions could be particularly relevant [@fiskeGainingTrustWell2014]: people would already perceive scientists as very competent, but not as very warm, thus offering a greater margin for improvement.

Overall, as reviews of science-society research have noted, the literature continues to focus on why some people do not trust science [@bauerWhatCanWe2007; @scheufeleThirtyYearsScience2022], and do not attempt to explain the elevated levels of trust in science observed in most places in the world, as reviewed presently.

## People tend to trust science

Across the globe, most people trust science, at least to some extent. A recent study in 68 countries found that, across the globe, trust in scientists was "moderately high" (mean = 3.62; sd= 0.70; Scale: 1 = very low, 2 = somewhat low, 3 = neither high nor low, 4 = somewhat high, 5 = very high), with not a single country below midpoint trust [@colognaTrustScientistsTheir2025]. Long-term global data on trust in science across time is sparse, yet the available data suggests, if anything, a recent increase of trust in science: In 2018, the Wellcome Global Monitor (WGM) surveyed more than 140,000 people in over 140 countries on trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. In 2020, during the first year of the Covid pandemic and before vaccines were widely available, a follow-up survey was conducted in 113 countries, with 119,000 participants [@wellcomeglobalmonitorWellcomeGlobalMonitor2020]. Between these two surveys, on average, trust in science had risen [@wellcomeglobalmonitorPublicTrustScientists2021]: In 2020, 41% (32% in 2018) of respondents said they trust science a lot, 39% (45% in 2018) said they trust science to some extent, 13% (also 13% in 2018) said they trust science "not much or not at all" (with the rest answering“I don’t know”).

In the US, where long term data is available from the US General Social Survey (GSS), trust in science appears to be both remarkably stable and elevated relative to trust in other institutions [@funkPublicConfidenceScientists2020; @funkScienceScientistsHeld2020; @smithTrendsPublicAttitudes2013]: From the early 1970s to 2022, on average `r top_intitutions_great_deal$consci$mean_share` (average yearly deviation = `r yearly_variation$consci$average_deviation` percentage points) of Americans say they have a great deal of confidence in the scientific community. This is the second highest score (just behind medicine, `r top_intitutions_great_deal$conmedic$mean_share`, average yearly deviation = `r yearly_variation$conmedic$average_deviation` percentage points) among `r n_institutions` institutions listed in the GSS, surpassing e.g., the Supreme Court, organized religion, or the military[^1]. Note, however, that the most recent polls suggest a small drop in trust in science in the US [@lupiaTrendsUSPublic2024].

[^1]: Numbers are based on our own calculations using on the publicly available GSS cumulative data

## People do not know much about science

As mentioned above, one of the main issues with the deficit model of trust in science is that the relatively high levels of trust in science do not seem to be matched by commensurate levels of science knowledge.

Early attempts at measuring science knowledge have developed what is known as the "Oxford scale" (@tbl-oxford) to measure science knowledge—a set of specific true/false or multiple-choice questions about basic science facts [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. Survey results from the Oxford scale have been interpreted as revealing a science knowledge deficit among the public. For example, @durantPublicUnderstandingScience1989 [11] initially reported that only "34% of Britons and 46% of Americans appeared to know that the Earth goes round the Sun once a year, and just 28% of Britons and 25% of Americans knew that antibiotics are ineffective against viruses." According to the @nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016 [51], performance on the Oxford scale items in the US has been "fairly stable across 2 decades."

```{r}
#| label: tbl-oxford
#| fig-cap: An 11-item version of the Oxford-scale, as reported in a comprehensive review of the literature on scientific literacy [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]
#| apa-twocolumn: true


# Create the data frame
knowledge_items <- data.frame(
  `Question` = c(
    "The center of the Earth is very hot. (True)",
    "The continents on which we live have been moving their locations for millions of years and will continue to move in the future. (True)",
    "Does the Earth go around the Sun, or does the Sun go around the Earth? (Earth around Sun)",
    "How long does it take for the Earth to go around the Sun? (One year)\\*",
    "All radioactivity is man-made. (False)",
    "It is the father’s gene that decides whether the baby is a boy or a girl. (True)",
    "Antibiotics kill viruses as well as bacteria. (False)",
    "Electrons are smaller than atoms. (True)",
    "Lasers work by focusing sound waves. (False)",
    "Human beings, as we know them today, developed from earlier species of animals. (True)",
    "The universe began with a huge explosion. (True)"
  )
) |> 
  mutate(number = 1:11) |> 
  select(number, Question)

# Render the table
kable(
  knowledge_items,
  col.names = c("", ""),
  booktabs = TRUE, 
      longtable = TRUE,
      #caption = "Science knowledge items",
      full_width = TRUE
) %>%
  kable_styling() %>%
    kableExtra::column_spec(1, width = "2em") %>%
    kableExtra::column_spec(2, width = "40em") %>%
  footnote(
    general = "*Only asked if previous question was answered correctly.",
    general_title = ""
  )
```

The Oxford scale has been criticized for only capturing factual recall [@pardoCognitiveDimensionPublic2004; @bauerWhatCanWe2007]. What should actually matter for trust, according to this critique, is a different kind of knowledge, namely an institutional and methodological understanding of how science works.

To circumvent these limitations, @durantPublicUnderstandingScience1989 also developed a scale of "understanding of processes of scientific inquiry"--several multiple-choice questions about the scientific method and basic concepts of probability. Similarly, @millerPublicUnderstandingAttitudes2004 [273] suggested that a scientifically literate citizen was someone who had both a “(1) a basic vocabulary of scientific terms and constructs; and (2) a general understanding of the nature of scientific inquiry.” His measure of science literacy included open-ended questions, for example on what people understand as the meaning of scientific study [@millerMeasurementCivicScientific1998a]. However, these measures have hardly drawn a more positive picture of the public's knowledge of science: Using an index of various understanding questions, Miller concluded that "approximately 10 percent of US adults qualified as civic scientifically literate in the late 1980s and early 1990s, but this proportion increased to 17 percent in 1999" [@millerPublicUnderstandingAttitudes2004, p. 288]. Miller explained that, according to his measure, someone qualifies as scientifically literate if they possess "the level of skill required to read most of the articles in the Tuesday science section of The New York Times, watch and understand most episodes of Nova, or read and understand many of the popular science books sold in bookstores today" [@millerPublicUnderstandingAttitudes2004, p. 288].

More recent data suggest that science literacy in the US may have improved slightly since Miller’s assessment during the early 2000s, but that it remains low. Based on results from the 2018 US Science & Engineering Indicators, @scheufeleScienceAudiencesMisinformation2019 [7663] report that "one in three Americans (36%) misunderstood the concept of probability; half of the population (49%) was unable to provide a correct description of a scientific experiment; and three in four (77%) were unable to describe the idea of a scientific study." Similarly, the 2024 US Science & Engineering Indicators, based on data from the Pew Research Center’s American Trends Panel (ATP) from 2020, report that only "60% of U.S. adults could correctly note that a control group can be useful in making sense of study results" and that "only half of U.S. adults (50%) could correctly identify a scientific hypothesis" [@nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024, p.24].

Not only are levels of science knowledge and understanding low, but they are only weakly correlated with trust in science. In a meta-analysis, @allumScienceKnowledgeAttitudes2008 found that science knowledge measured by the Oxford scale was only weakly associated with attitudes towards science. More recently, @colognaTrustScientistsTheir2025 found no statistically significant relationship between national science literacy scores, based on the Program for International Student Assessment (PISA), and national average trust in scientists for the 68 countries included in their study.

To sum up, the literature shows that trust in science is relatively high, but that knowledge and understanding of science do not seem to be strong determinants of this trust. Does this mean that trust in science is irrational?

From a sociological perspective, in particular in a Bourdieusian framework, trust in science may be strongly influenced by *habitus*—a system of dispositions shaped by one’s social class and cultural background [@bourdieuOutlineTheoryPractice1977; @bormannTrustTrustingPractices2019]. Rather than a reasoned appraisal of science’s reliability, trust might result from internalized norms. In line with this suggestion, @archerScienceCapitalConceptual2015 show that school children aged 11-15 years already differ considerably in their "science capital"---an index of several questions pertaining to how much they value and engage with science. These differences were associated with differences in cultural capital (e.g. parental university attendance), gender, and ethnicity. If, for undetermined sociological reasons, science acquires sufficient prestige among some segments of the population, it could lead other people to look up to science and trust it.

While sociological factors also play a role in shaping people’s attitudes towards science, we presently introduce a model in which it might be rational for people to trust science, even if they have little current knowledge of it.

# The rational impression account of trust in science

In the rational impression account of trust in science, people trust science because they have been impressed by it. This trust persists even after the specific contents that gave rise to the trust have been forgotten. The account builds on three basic cognitive mechanisms. First, we infer that people who possess rare knowledge are more broadly knowledgeable: If someone states something that is difficult to know, and we believe that they are right, we are impressed, and deem that individual competent. Second, in many situations, we infer accuracy from consensus: If something is highly consensual, it is likely to be true. Third, impressions can persist without recall of what generated them: While learning about science can create lasting impressions, we are likely to forget about specific science knowledge.

## People infer competence from rare knowledge

Estimating other people's competence from communicated information is an essential skill in a variety of social contexts, including communication [@sperberEpistemicVigilance2010], cooperation [@cuddyBIASMapBehaviors2007], and social learning [@kendalSocialLearningStrategies2018].

Humans use a variety of cues to estimate others' competence: from superficial, generally unreliable cues such as facial features [@todorovUnderstandingEvaluationFaces2008], to more reliable cues, such as providing good explanations [@reimerUseHeuristicsPersuasion2004; for children, see, e.g., @castelainEvidenceThatTwoYearOld2018a] or having made accurate predictions in the past [@mellersIdentifyingCultivatingSuperforecasters2015; for children, see, e.g., @liuSelectiveTrustChildrens2013].

One reliable cue to competence is possessing specific pieces of knowledge: people see others who share valuable ideas as more competent [@altayItMyIdea2020]. With trivia questions, it has been shown that people have accurate perceptions of whether something is hard to know or not, and that they use this information to infer someone's competence [@dubourgUsingNestedStructure2025]: knowing a rare piece of information indicates a high likelihood of knowing more information in the same domain. In the case of science, @pfanderTrustingForgettingImpressive2025 showed that participants perceive some scientific findings as more impressive than others. Reading about the more impressive scientific findings increased participants' perceptions of both the scientists' competence and the trustworthiness of their discipline.

For an information to be impressive, at least two criteria should be met: (i) it is perceived as rare or hard to uncover, (ii) it is believed to be true. Past research has shown that people have very accurate intuitions regarding (i) [see, in particular, @dubourgUsingNestedStructure2025], but little is known about which features of an information exactly trigger this intuition. For example, most people would probably only be mildly impressed by someone telling them that a given tree has exactly 110,201 leaves. Even though obtaining this information implies an exhausting counting effort, everyone in principle knows how to do it. By contrast, finding out that it takes light [approximately 100,000 years to travel from one end of the Milky Way to the other](https://imagine.gsfc.nasa.gov/features/cosmic/milkyway_info.html) is probably impressive to most people, as they would not know how such a distance can be measured. From this view, most scientific knowledge is likely to be deemed very impressive [and a survey in France showed that people tended to trust science more if they deemed it more precise, which is one way of being impressive, @pfanderFrenchTrustMore2025]. Less obvious is how people infer (ii), i.e., that the information is true, since, as a rule, people cannot evaluate scientific discoveries by themselves. Below, we describe how perceived consensus might allow people to infer that a piece of information is true, even if they do not understand how it was acquired.

## People infer accuracy from consensus

In order to make the best of communicated information, individuals need to be able to evaluate it, i.e. being able to distinguish inaccurate and harmful from accurate and beneficial information [@maynard-smithAnimalSignals2003]. It has been argued that humans have evolved a suite of cognitive mechanisms to serve this function, mechanisms which evaluate both the source of a piece of information and its content [@sperberEpistemicVigilance2010; @mercierNotBornYesterday2020].

In the case of science, if we do not already assume that people trust scientists (i.e. the source), it seems that we are left only with content. However, scientific findings tend to violate our intuitions [@wolpertUnnaturalNatureScience1994; @cromerUncommonSenseHeretical1995; @shtulmanScienceblindWhyOur2017; @mccauleyWhyReligionNatural2011a], and thus their content should be intuitively deemed implausible. In some contexts, people might be able to judge the accuracy of scientific findings for themselves, for example when they are exposed to accessible and convincing explanations in school [@readExplanatoryCoherenceSocial1993; @lombrozoSimplicityProbabilityCausal2007; for a review, see @lombrozoStructureFunctionExplanations2006]. For most scientific research, however, people cannot evaluate the quality of the arguments and evidence for themselves, let alone make their own observations (e.g. few people understand complex analysis or group symmetry, and even fewer have access to a particle accelerator).

It is possible, however, to rationally believe that a piece of information is true even if it is not intuitively plausible, and if we don’t already trust its source: if enough people agree with it. The potential of the wisdom of crowds to lead to accurate answer has been known for centuries [@condorcetEssaiLapplicationLanalyse1785; @hastieRobustBeautyMajority2005] and, on the whole, people make sound use of this heuristic, being more likely to accept a piece of information when it is supported by a larger majority [in relative and absolute terms, for review, see @mercierMajorityRulesHow2019]. The wisdom of crowds literature, however, assumes that informants–the individuals providing answers–need to be at least minimally competent [i.e. better than chance, @condorcetEssaiLapplicationLanalyse1785]. This is a problem for the rational impression account, as it ultimately seeks to explain how people come to judge scientists as competent. Therefore, the account cannot take it for granted that the informants are deemed competent. However, recently, @pfanderHowWiseCrowd2025 have shown that it is enough to assume that informants are not all biased in exactly the same way to make justified inferences from their agreement to not only the accuracy of their answers, but also their competence. Participants who had no prior beliefs about an answer’s plausibility, or the competence of those who provided it, deemed more convergent answers more plausible, and those who made them more competent. This was true in abstract scenarios [@pfanderHowWiseCrowd2025], but other research suggests that these inferences are justified across a wide range of real-world decision making scenarios [@kurversHowDetectHighperforming2019].

To the extent that people perceive a scientific finding as being largely consensual within the research community, they should thus infer not only that it is more likely to be correct, but also that the scientists responsible for the finding are competent. Much evidence shows that, as a rule, when people are told about the scientific consensus on a given issue, they change their minds in the direction of the consensus [e.g., @vanderlindenGatewayBeliefModel2021; @veckalov27countryTestCommunicating2024; @vanstekelenburgScientificConsensusCommunicationContested2022]. Note, however, that participants start these experiments with a fair degree of trust in science, so that they can rely on that to infer that the scientists forming the consensus are competent, rather than inferring their competence from the fact that they agree. Even if people aren’t explicitly told that a scientific consensus exists, they likely assume that it is the case, at least for issues taken to be settled science, such as those they are exposed to at school–and they would be broadly right as the ability to reach a working consensus is a defining trait of science [@collinsSociologyPhilosophiesGlobal2002]. In line with this suggestion, people (in France) trust scientists more when they work in disciplines that people perceive as more consensual [@pfanderFrenchTrustMore2025]. People also trust science more when it successfully replicates—a way of solidifying consensus [@hendriksReplicationCrisisTrust2020].

So far, we have argued that (i) people are impressed by information that is difficult to acquire, if they believe it is true and, (ii) that they can come to believe it is true if they take it to be consensual. Applied to science, the prediction is that the more people are exposed to impressive science taken to be consensual, the more they perceive scientists as competent and, as a result, trust science more. This might appear similar to the deficit model, in that both models predict that exposure to science leads to more science knowledge. However, as pointed out above, the correlation between science knowledge and attitudes towards science, if it is positive, is weak [@allumScienceKnowledgeAttitudes2008]. To explain this and, more generally, the low levels of knowledge of science by comparison with trust in science, we argue that people likely forget most specific science content they had been exposed to, while an impression of trustworthiness persists.

## People forget specific knowledge while impressions persist

We commonly form impressions of the people around us while forgetting the details of how we formed these impressions: If a colleague fixes our computer, we might forget how they fixed it, yet remember that they are good at fixing computers. Similarly, people might forget the specific content of science knowledge they have been exposed to, but retain an impression of scientists’ competence. Several research strands suggest that impressions can persist, while recall of specific information fades.

Memory research suggests that implicit memory is more stable than explicit memory [@slomanForgettingPrimedFragment1988; @parkinDifferentialNatureImplicit1990]. It has also been argued that memory encodes information both as "verbatim" details–exact words or numbers–information and "gist" representations–the essence or bottom-line meaning [@reynaScientificTheoryGist2021], and that the verbatim memory tends to fade faster [@murphyForgettingVerbatimInformation1994].

More extreme examples supporting the idea that impressions can be detached from the memory of specific events come from medical research: patients with severe amnesia, for instance, can continue to experience emotions linked to events they could not recall [@feinsteinSustainedExperienceEmotion2010]. Other research has shown that patients with profound episodic memory impairment due to dementia continue to show capacity for emotional learning [@evans-robertsRememberingRelationshipsPreserved2010].

Some research in the context of science suggests that processes of impression formation and knowledge retention can be quite detached: in an experiment, participants found some science-related explanations more satisfying than others, but this did not predict how well they could recall the explanations shortly after [@liquinMotivatedLearnAccount2022]. In a study mentioned above [@pfanderTrustingForgettingImpressive2025, which showed that being exposed to impressive scientific content led to higher trust in the relevant scientific discipline], another experiment showed that participants immediately forgot most of the information which had impressed them.

Taken together, these findings make it very plausible that people, after they have been exposed to science, might retain a positive impression of scientists while forgetting most of the content that generated the impression.

## Additional predictions of the rational impressions account

The rational impression account of trust in science rests on three already established cognitive mechanisms: (i) people deem competent those who possess rare and impressive knowledge; (ii) people deem opinions others converge on to be true; (iii) people tend to forget how impressions are formed, while the impressions are maintained. This account explains why people trust science: when scientists agree on impressive findings, people deem that to be a good cue that the scientists are right, and that they are competent. The account also explains why people trust science despite not understanding or knowing much of it: first, they don’t need to understand science to deem it true and to be impressed by it, second, the impression of competence and trust can persist even if they don’t remember the scientific knowledge that gave rise to these impressions.

The argument we have made above suggests that people who have been exposed to scientific content have good grounds for deeming scientists competent. This requires that they believe the scientists aren’t conspiring to form a false consensus—but, in basic science, such aspersions are not very plausible (why would scientists conspire to make us believe the Milky Way has such and such size?). However, the model doesn’t require that scientists be perceived as particularly benevolent. Scientists’ benevolence should be difficult for people to evaluate, as few personally know any scientists, few even know *of* any individual living scientists [@researchamericaMostAmericansCannot2021]. As a result, there are no obvious reasons why people should rate scientists particularly high on benevolence or related dimensions. In line with this prediction, it has been shown that people perceive scientists as very competent, but not as particularly warm [@fiskeGainingTrustWell2014]. A recent Pew survey found that 89% of Americans viewed research scientists as intelligent, but only 65% viewed them as honest [@kennedyPublicTrustScientists2024]. Beyond the US, a recent study confirmed this tendency on a global scale [@colognaTrustScientistsTheir2025]: People perceived scientists as highly competent, with 78% tending to believe that most scientists are qualified to conduct high-impact research. By contrast, people held scientists in lower esteem with regards to their integrity and benevolence: Only 57% of people tended to believe that most scientists are honest, and only 56% tended to believe that most scientists are concerned about people’s well-being.

A second prediction of the rational impression account is that education, and more specifically science education, should be the main predictor of trust in science. Since most people consume very little news [@newmanDigitalNewsReport2023], and even less scientific news [@funkScienceNewsInformation2017], the bulk of exposure to science can be assumed to happen during education. Education, and in particular science education, has been consistently identified as one of the strongest predictors of trust in science [@noyScienceGoodEffects2019; @wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020; but see @colognaTrustScientistsTheir2025 who only find a small positive relationship between education and trust in science, plausibly because of reduced varaition, as they tested for tertiary education in particular]. This is compatible with the fact that people, even those who received a science education, do not know much about science: if we assume that education has some causal effect on trust in science, this effect does not need to be driven by the transmission (including remembering) of knowledge and understanding [for a similar argument, see @bakEducationPublicAttitudes2001]. The candidate mechanism proposed by the rational impression account is exposure to impressive scientific content. Students might not understand all of it, and potentially recall even less later on; but they might have been impressed by it, to the point that they come to perceive scientists as competent, and thus as trustworthy. This impression might persist even when specific knowledge vanishes. In line with this, @mottaEnduringEffectScientific2018 found that, in the US, the more children were interested in science at age 12–14 years, the more they tended to trust in climate scientists in adulthood (mid-thirties), irrespective of their political ideology.

A third prediction is that people with a basic science education should have had ample opportunities to form impressions of trustworthiness of science, which should have built a solid baseline of trust in science. People might deviate from this default and distrust science on certain specific science topics for various reasons (see below), but they should still trust most of science. This is in line with the finding that in the US, everyone--even people who say they don't trust science in general or who hold specific beliefs blatantly violating scientific knowledge (e.g. that the earth is flat)--trusts almost all of basic science knowledge [e.g. that electrons are smaller than atoms, @pfanderQuasiuniversalAcceptanceBasic2025b].

## Counterarguments

There are several theoretical and empirical counterarguments that can be made against the rational impression account. We address the main ones here.

How is the rational impression account “rational,” if it posits that trust is largely detached from recalling specific knowledge or from understanding the sources of that knowledge? It is rational in that the cognitive mechanisms it builds on lead to reliable conclusions as a rule. Mathematical analyses and simulations show that, unless the sources are dependent on each other, it is rational to infer from the fact that answers converge with each other, that the sources (of these answers) are likely to be right and to be competent. Even forgetting specific knowledge is not irrational: given the limits on our memory, in many cases it makes more sense to remember only the gist [@reynaScientificTheoryGist2021]. Moreover, it has been argued that one of the main functions of episodic memory is to justify our beliefs in communication with others [@mahrWhyWeRemember2018]. As a result, we should be particularly good at remembering things we might need to convince others of. In this regard, incentives of remembering science seem to be weak: Most exposure to science happens at school, and there is little reason for young learners’ minds to anticipate having to convince others who would disagree about, say, the chemical composition of table salt [and indeed, basic scientific findings are nearly entirely uncontroversial, @pfanderQuasiuniversalAcceptanceBasic2025b].

The inference from agreement to accuracy requires agreement among several agents. As a rule, however, people do not compare the opinions of different scientists for themselves and come to the conclusion that a purported scientific discovery is consensual. How, then, could the representation of consensus emerge? A plausible explanation, we believe, is that education fosters a representation of consensus. This is in line with observations that during education, children typically perceive science as a unified, authoritative body of knowledge [@driverYoungPeoplesImages1996; @careyUnderstandingNatureScientific1993]. Moreover, the materials covered in science education tend to be settled science, not the more controversial discoveries at the cutting edge of science. This could induce a default consensus assumption in people’s perceptions of science—an assumption from which people would carve out exceptions for domains they perceive as controversial or politicized.

Another potential argument rests on the specific application of the rational impression account to science. If the account relies on an inference from agreement to accuracy, why should it be specific to science? Such inferences could happen in any context–in fact, the experimental evidence has been obtained in settings unrelated to science [@pfanderHowWiseCrowd2025]. This inference is not necessarily always sound: There are historic examples where there has been broad agreement, at least within specific communities, on misbeliefs, such as when Christian theologians had calculated that the Earth was approximately six thousand years old. If people were aware of this broad agreement, and believed the theologians to have reached this number independently of each other, this might have led them to believe their estimate to be accurate, and the theologians to be competent. However, maintaining a consensus around misbeliefs is hard, and, compared to other institutions, science is exceptionally good at producing a lasting consensus [@collinsSociologyPhilosophiesGlobal2002]--the theologian’s value was not accepted for long, being instead replaced by increasingly accurate scientific estimates.

Another potential issue with the current account is that even people with no formal education, and thus presumably very little exposure to science, have some trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. This is not naturally explained by the rational impression account, which suggests that trust in science emerges as people are exposed to science. A possible explanation for this could be a two-step model of diffusion, in which some people are exposed to science, and then talk to the people who haven’t, e.g., younger generations who attend school and discuss it with their parents [on two-step models in general, see @katzTwoStepFlowCommunication1957; in the case of science communication in particular, see @nisbetTwoStepFlowInfluence2009; on knowledge diffusion to previous generations, see @scribnerLiteracySchoolingTesting1978]. This minimal exposure to science [which seems necessary for people to even know what science is, which is true for all the participants who answered questions about trust in science in the @wellcomeglobalmonitorWellcomeGlobalMonitor2018] might be sufficient to generate the low levels of trust in science typically observed in unschooled participants.

Finally, why do some people distrust science even though they have been extensively exposed to science ? In the global north, where essentially everyone has been exposed to science through a basic science education, some people do not trust some aspects of science, or say they don’t trust science in general [even if that is not true for basic science, which everyone does trust, see @pfanderQuasiuniversalAcceptanceBasic2025b]. In these cases, trust in science is likely to be a default state, but other cognitive mechanisms can lead people to deviate from this default, in particular for specific issues. When answering questions about trust in science in general, people’s answers would then be overly driven by their feelings in relation to these specific issues. Suggestions on why people tend to systematically mistrust science on specific issues have been made, for instance about vaccination [@mitonCognitiveObstaclesProVaccination2015], GMOs [@blanckeFatalAttractionIntuitive2015], or nuclear energy [@hacquinDisgustSensitivityPublic2021], but more research is needed to better understand what motivates distrust in specific aspects of science [@hornseyWhyFactsAre2020].

# Discussion

In every country investigated, most people trust science at least to some extent [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020; @colognaTrustScientistsTheir2025]. This is true even though people, as a rule, don’t understand or know much about science, and don’t consume much scientific information. The rational impression account of trust in science accounts for these facts by suggesting that, when people are exposed to science, mostly during their education, they rationally develop a positive impression of science–in particular of the competence of scientists—while then they forget most of the information that gave rise to this impression.

The rational impression account of trust in science is supported by work in cognitive psychology suggesting people (i) infer competence from possessing rare and impressive knowledge; (ii) infer accuracy from consensus; (iii) remember broad impressions without recalling the details of the information that generated them. It is also coherent with the observation that science education—by far the main exposure to science for most people—is the best predictor of trust in science, and with the fact that people who have received a science education seem to accept nearly all of basic science.

In its stress on consensus among scientists, the rational impression account is coherent with accounts of how science manages to yield accurate understanding of the world, accounts that stress the importance of having convergent results from multiple sources and methods [@cartwrightTangleScienceReliability2022; @oreskesWhyTrustScience2019]. In the present account, people are not able, in most cases, to verify for themselves that science is accurate. However, by tracking what is consensual among scientists, which tends to track what is accurate, people’s beliefs can remain broadly accurate (at least when they are informed of the scientific consensus). As discussed above, this is an ideal case scenario, when nothing gets in the way of accepting the scientific consensus. However, people still reject some specific scientific knowledge, for instance because it is perceived as particularly inconvenient (e.g., knowledge of climate change which would urge us to change our behavior), or because it is rejected by other authorities—political, religious—which people trust (e.g. the rejection of evolution among some religious communities). 

The current account is very much situated at the micro-level, attempting to explain the cognitive mechanisms through which people develop trust in science. This account should be seen as complementary with macro-level analyses of trust in science that raise broader historical, sociological, or political issues about trust in science. For instance, the rational impression account might help explain trust in science in modern societies, in which people are massively exposed to science in their education, but it doesn’t explain why this exposure exists in the first place, and the political forces that led to the development of universal science education in many countries [@childsCurriculumDevelopmentScience2015; @atkinScienceEducationReform2003]. Moreover, the current account focuses on the competence dimension of trust in science. People might have reasons to believe that scientists, albeit competent, do not work for everyone's best interests, either because they belong to groups which have been neglected or ill-treated by science [e.g. African Americans, see, @brandtRacismResearchCase1978; @scharffMoreTuskegeeUnderstanding2010], or because they feel a more general alienation, a “general disenchantment with late modernity, mainly, the limitations associated with codified expertise, rational bureaucracy, and institutional authority” [@gauchatCulturalAuthorityScience2011, p.2].

The rational impression account suggests that people develop trust in science when they are exposed to impressive scientific findings believed to be consensual among scientists. It might seem to follow that, in order to foster trust in science, science communicators should focus on impressive and consensual findings. While this might be effective, it is important that the consensual nature of the findings not be exaggerated: When uncertainty or dissensus exist, they must be acknowledged [@druckmanCommunicatingPolicyRelevantScience2015]. For example, in the context of Covid-19 vaccines, @petersenTransparentCommunicationNegative2021 have shown that communicating uncertainty is crucial for building long term trust in health authorities. Nor should the impressiveness of scientific findings be exaggerated: intellectual humility has been shown to increase trust in scientists [@koetkeEffectSeeingScientists2024].

The fact that people appear to forget most of the details they have learnt about science should not discourage science communicators and educators from attempting to properly explain science and simply "wow" people with impressive findings. Even if many people forget much science knowledge, some don’t, and it can prove important in their lives. Moreover, the current mechanism isn’t the only cognitive mechanism through which people develop a trust in science. In particular in the context of science education, students also properly understand some scientific findings, giving them other reasons to trust science.

With these caveats in mind, we believe that the rational impressions account offers optimism for science communication and education. Exposure to science might be the foundation of public trust in science, making science education and communication its main pillars. Low scientific literacy levels should not discourage education and communication efforts, as they are not necessarily a good indicator of the value added in terms of fostering trust in science.

Beyond science education and communication, trust in science ultimately rests on scientists producing accurate knowledge that can garner a consensus and pass the test of time. We should therefore expect that efforts at improving science methodology---the introduction of mandatory pre-registration for clinical trials for instance [@kaplanLikelihoodNullEffects2015]---will result in increases in trust in science, not only when people are aware of them [see, @songTrustingShouldersOpen2022], but also when they are not, through the generation of more accurate, robust, and consensual scientific knowledge.

# References

::: {#refs}
:::
