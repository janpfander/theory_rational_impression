---
title: "The rational impression account of trust in science"
# If blank, the running header is the title in upper case.
shorttitle: ""
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jan Pfänder
    corresponding: false
    orcid: 0009-0009-4389-2807
    affiliations:
      - id: id1  # Added an explicit ID for referencing
        name: ENS, EHESS, PSL University, CNRS, France
        department: Institut Jean Nicod, Département d’études cognitives

  - name: Hugo Mercier
    corresponding: true
    orcid: 0000-0002-0575-7913
    email: hugo.mercier@gmail.com
    affiliations: 
      - ref: id1  
blank-lines-above-author-note: 2
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: ~
    # Example: [Author name] is deceased.
    deceased: ~
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: ~
    # Acknowledge and cite data/materials to be shared.
    data-sharing: ~
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: ~
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
    financial-support: ~
    # Example: The authors are grateful to [Person] for [Reason].
    gratitude: ~
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    authorship-agreements: ~
abstract: |
  bla
  
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [trust in science, science literacy, deficit model]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
impact-statement: ~
# If true, a word count will appear below the keywords (tables, figure captions, and references excluded in count.)
word-count: true
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: bibliography.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: true
# Masks references that appear in the masked-citations list
mask: false
masked-citations:
# If true, adds today's date below author affiliations. If text, can be any value.
# This is not standard APA format, but it is convenient.
# Works with docx, html, and typst. 
draft-date: true
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "Email"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; [credit.niso.org](https://credit.niso.org)) as follows:"
  title-impact-statement: "Impact Statement"
  references-meta-analysis: "References marked with an asterisk indicate studies included in the meta-analysis."
format:
  apaquarto-docx: 
    toc: false
  apaquarto-html: 
    toc: true
  apaquarto-typst: 
    keep-typ: true
    list-of-figures: false
    list-of-tables: false
    toc: false
    papersize: "us-letter"
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: jou
    keep-tex: true

header-includes:  | # to prevent floats from moving past certain points (for the appendix)
  \usepackage{placeins} 
    
always_allow_html: true
---

```{r}
#| label: setup
#| include: false
#| message: false
library(tidyverse)
library(flextable)
library(tinytable)
library(kableExtra)
```

```{r}
#| label: functions
#| include: false
#| message: false
# load plot theme
source("functions/plot_theme.R") 

# load other functions
source("functions/functions.R")
```

```{r}
#| label: gss
#| include: false
#| message: false
# read gss data
gss <- read_csv("gss_cross/data/gss_cross_cleaned.csv")


# Average confidence levels by institution and year
gss_summary_numeric <- gss |>
  group_by(year, institution, institution_label) |>
  summarize(mean_confidence = mean(confidence, na.rm = TRUE)) |> 
  ungroup() |> 
  # remove years without measure
  drop_na(mean_confidence )


# Share of people with great deal of trust by institution and year
gss_summary_great_deal <- gss |>
  drop_na(confidence) |> 
  group_by(year, institution, institution_label) |>
  summarize(
    total_responses = n(),
    great_deal_count = sum(confidence_factor == "A great deal confidence", na.rm = TRUE),
    share = great_deal_count / total_responses,
    # remove 0s
    share = ifelse(share == 0, NA, share)
  ) |>
  ungroup() |> 
  # remove years without measure
  drop_na(share)

# check the four on average top trusted institutions using the mean
top_trusted_institutions <- gss |>
  group_by(institution, institution_label) |>
  summarise(mean_confidence = mean(confidence, na.rm = TRUE)) |>
  ungroup() |> 
  arrange(desc(mean_confidence)) |>
  slice_head(n = 4)  # Select top 4

# check the four top institutions using highest share of great deal of trust
top_intitutions_great_deal <- gss_summary_great_deal |> 
  group_by(institution, institution_label) |> 
  summarize(mean_share = mean(share, na.rm=TRUE)) |> 
  ungroup() |> 
  arrange(desc(mean_share)) |>
  # Select top 4
  slice_head(n = 4) |> 
  rounded_numbers() |> 
  mutate(mean_share = paste0(round(mean_share * 100), "%")) %>% 
  split(.$institution)

# get the number of institutions
n_institutions <- gss |> 
  distinct(institution, institution_label) |> 
  nrow()
```

# Introduction

Addressing important societal challenges, from fighting climate change to managing pandemics, is greatly facilitated by trust in science. Studies have demonstrated that individuals with higher levels of trust in science are more likely to accept the scientific consensus on global warming [@bogertEffectTrustScience2024], to engage in pro-environmental behavior, and to support climate policies [@colognaRoleTrustClimate2020; @hornseyMetaanalysesDeterminantsOutcomes2016]. Trust in science has also been shown to be positively associated with willingness to get vaccinated [@sturgisTrustScienceSocial2021; for Covid-19 in particular, @lindholtPublicAcceptanceCOVID192021]. During the Covid-19 pandemic, a panel study in 12 countries found that trust in scientists was the strongest predictor of whether people followed public health guidelines, such as mask-wearing or social distancing [@alganTrustScientistsTimes2021]. Similar results have been found by other studies [e.g., positive effects of trust in science on acceptance of social distancing in the US, @koetkeTrustScienceIncreases2021].

Given the individual and social cost of a lack of trust in science, most studies have focused on understanding why some people *do* not trust science. However, it also important to understand why most people do trust science: it is important theoretically, as this trust could stem from very different processes–from blind deference to a rational assessment of scientific evidence; it is important practically, as, depending on why people do trust science, different interventions aimed at increasing trust in science could be conceived. Here, we argue that, even though people do not know much about science, their trust in science can still be rational.

We start by reviewing work on explanations for variations in trust in science, work which has focused on why some people do not trust science. We argue that this work has not fully solved a basic puzzle: why do most people tend to trust science, in spite of knowing so little about it?

To solve this puzzle, we then develop a 'rational impression' account of trust in science. According to this account, people do not need a profound understanding or detailed knowledge of science, to rationally perceive it as trustworthy. Instead, by appealing to basic cognitive mechanisms of information evaluation, science impresses people, who then mostly forget what had impressed them.

# The puzzle of why people trust science

A widely agreed-upon definition of trust is the willingness to be vulnerable to another party–whether an individual, a group, or an institution [@mayerIntegrativeModelOrganizational1995a; @rousseauIntroductionSpecialTopic1998]. Building on this idea, trust in science has been defined as “one’s willingness to rely on science and scientists (as representatives of the system) despite having a bounded understanding of science” [@wintterlinPredictingPublicTrust2022, p. 2]. This definition implies that trust in science goes beyond knowledge of science. Yet, the idea that knowledge of science is the primary cause of trust in–and more generally positive attitudes towards–science has long dominated research on public understanding of science [@bauerWhatCanWe2007]. This idea is widely known under the term "deficit model", because much of the literature attested to the public "depressingly low levels of scientific knowledge" that were assumed to be the principal cause of negative attitudes towards science [@sturgisScienceSocietyReEvaluating2004, p. 56].

The deficit model has been criticized for idealizing science and viewing the public as deficient and irrational [@bauerWhatCanWe2007; @gauchatCulturalAuthorityScience2011]. Moreover, as reviewed below, the relationship between science knowledge and trust in science is rather tenuous. As a result, the literature has mostly moved beyond the idea of science knowledge as the principle driver of trust in science. However, the focus on explaining a lack of trust, rather than trust, persists.

Researchers have increasingly studied how people's values, world views, and identities shape their attitudes towards science [@hornseyAttitudeRootsJiu2017a; @lewandowskyWorldviewmotivatedRejectionScience2021]. The psychological literature has focused on explaining negative attitudes towards science with motivated reasoning–selecting and interpreting information to match one's existing beliefs or behaviors [@lewandowskyMotivatedRejectionScience2016; @hornseyWhyFactsAre2020; @lewandowskyRoleConspiracistIdeation2013]. This research mostly suggests that certain psychological traits, such as a social dominance orientation, or a tendency to engage in conspiracy thinking, lead people to reject science. Arguments on a general conspiratory thinking style as one of the root causes of science rejection shift the debate, to some extent, from a knowledge deficit to a broader reasoning deficit [@hornseyAttitudeRootsJiu2017a; @rutjensConspiracyBeliefsScience2022].

Motivated reasoning accounts found support in the accumulating evidence in the US for a widening partisan gap regarding trust in science, with Republicans trusting less and Democrats trusting more [@gauchatPoliticizationSciencePublic2012; @krauseTrendsAmericansTrust2019a; @leePartyPolarizationTrust2021]. Contrary to what the deficit model would suggest, some early work on motivated reasoning suggested that partisan-driven rejection of science did not appear to be the result of a lack of cognitive sophistication, on the contrary, as in some cases greater science literacy was associated with more polarized beliefs on a variety of scientific topics [@kahanPolarizingImpactScience2012; @drummondIndividualsGreaterScience2017]. These early findings, however, have largely failed to replicate [@perssonPreregisteredReplicationMotivated2021; @hutmacherMotivatedReasoningClimate2024; @stagnaroNoAssociationNumerical2023a]. Other studies, focusing on the case of climate change, have argued that partisan divides might be the result of broadly rational Bayesian information updating, rather than motivated reasoning [@bayesMotivatedReasoningClimate2021; @druckmanEvidenceMotivatedReasoning2019].

Since the Covid-19 pandemic, the role of misinformation in fostering distrust in science has been increasingly studied [@nationalacademiesofsciencesUnderstandingAddressingMisinformation2024; @scheufeleScienceAudiencesMisinformation2019; @druckmanThreatsSciencePoliticization2022]. For this literature, by contrast with the deficit model, the problem of trust in science is less a lack of information, and more the abundance of harmful information.

By contrast with the deficit model and its stress on content, more recent literature in science communication has shifted the focus on source-based explanations of trust: from science knowledge to perceptions of scientists. This work has established that people evaluate scientists along different dimensions [@intemannScienceCommunicationPublic2023], including competence, but also also integrity, benevolence or openness [@hendriksMeasuringLaypeoplesTrust2015; @besleyReassessingVariablesUsed2021a]. This literature suggests that, for enhancing trust in science, the latter, warmth-related (i.e. other than competence) dimensions could be particularly relevant [@fiskeGainingTrustWell2014]. The idea is that people already perceive scientists as very competent, but not as very warm, thus offering a greater margin for improvement.

Another explanation for distrust towards science is the *alienation model* [@gauchatCulturalAuthorityScience2011]. According to this model, the "public disassociation with science is a symptom of a general disenchantment with late modernity, mainly, the limitations associated with codified expertise, rational bureaucracy, and institutional authority” [@gauchatCulturalAuthorityScience2011, p.2]. This explanation builds on the work of social theorists [@habermasJurgenHabermasSociety1989; @beckRiskSocietyNew1992; @giddensModernitySelfidentitySelf1991; see @gauchatCulturalAuthorityScience2011 for an overview] who suggested that a modern, complex world increasingly requires expertise, and thus shapes institutions of knowledge elites. People who are not part of these institutions experience a lack of agency, resulting in a feeling of alienation.

On the whole, the literature on public trust in science focuses not on explaining why people do trust science, but on why they do not, or not enough. As reviews of science-society research have noted, the literature continues to operate in "deficit" paradigms [@bauerWhatCanWe2007; @scheufeleThirtyYearsScience2022] which tend to overlook the elevated levels of trust in science observed in most places in the world, as reviewed presently.

## People tend to trust science

Across the globe, most people do trust science, at least to some extent. A recent study in 68 countries found that, across the globe, trust in scientists was "moderately high" (mean = 3.62; sd= 0.70; Scale: 1 = very low, 2 = somewhat low, 3 = neither high nor low, 4 = somewhat high, 5 = very high), with not a single country below midpoint trust [@colognaTrustScientistsTheir2025]. Long-term global data on trust in science across time is sparse, yet the available data suggests, if anything, a recent increase of trust in science: In 2018, the Wellcome Global Monitor (WGM) surveyed of over 140000 people in over 140 countries on trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. In 2020, during the first year of the Covid pandemic and before vaccines were widely available, a follow-up survey was conducted in 113 countries, involving 119000 participants [@wellcomeglobalmonitorWellcomeGlobalMonitor2020]. Between these two surveys, on average, trust in science had risen [@wellcomeglobalmonitorPublicTrustScientists2021]: In 2020, 41% (32% in 2018) of respondents said they trust science a lot, 39% (45% in 2018) said they trust science to some extent, 13% (also 13% in 2018) said they trust science "not much or not at all" (with the rest answering“don’t know”).

In the US, where long term data is available from the US General Social Survey (GSS), this public trust appears to be both remarkably stable and elevated relative to other institutions [@funkPublicConfidenceScientists2020; @funkScienceScientistsHeld2020; @smithTrendsPublicAttitudes2013]: From the early 1970s to 2022, the currently the largest available time span, on average `r top_intitutions_great_deal$consci$mean_share` of Americans say they have a great deal of confidence in the scientific community. This is the second highest score (just behind medicine, `r top_intitutions_great_deal$conmedic$mean_share`) among `r n_institutions` institutions listed in the GSS, surpassing e.g., the Supreme Court, organized religion, or the military[^1]. Note, however, that the most recent polls suggest a small drop in trust in science in the US [@lupiaTrendsUSPublic2024].

[^1]: Numbers are based on our own calculations using on the publicly available GSS cumulative data

## People do not know much about science

As mentioned above, one of the main issues with the deficit model of trust in science is that the relatively high levels of trust in science do not seem to be matched by commensurate levels of science knowledge.

Early attempts of measuring science knowledge have developed what is known as the "Oxford scale" (@tbl-oxford) to measure science knowledge–a set of specific true/false or multiple-choice questions about basic science facts [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. Survey results from the Oxford scale have been interpreted as revealing a science knowledge deficit among the public. For example, @durantPublicUnderstandingScience1989 (p.11) initially reported that only "34% of Britons and 46% of Americans appeared to know that the Earth goes round the Sun once a year, and just 28% of Britons and 25% of Americans knew that antibiotics are ineffective against viruses". According to the @nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016, performance on the Oxford scale items in the US has been "fairly stable across 2 decades" [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016, p. 51].

```{r}
#| label: tbl-oxford
#| fig-cap: An 11-item version of the Oxford-scale, as reported in a comprehensive review of the literature on scientific literacy [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]
#| apa-twocolumn: true


# Create the data frame
knowledge_items <- data.frame(
  `Question` = c(
    "The center of the Earth is very hot. (True)",
    "The continents on which we live have been moving their locations for millions of years and will continue to move in the future. (True)",
    "Does the Earth go around the Sun, or does the Sun go around the Earth? (Earth around Sun)",
    "How long does it take for the Earth to go around the Sun? (One year)\\*",
    "All radioactivity is man-made. (False)",
    "It is the father’s gene that decides whether the baby is a boy or a girl. (True)",
    "Antibiotics kill viruses as well as bacteria. (False)",
    "Electrons are smaller than atoms. (True)",
    "Lasers work by focusing sound waves. (False)",
    "Human beings, as we know them today, developed from earlier species of animals. (True)",
    "The universe began with a huge explosion. (True)"
  )
) |> 
  mutate(number = 1:11) |> 
  select(number, Question)

# Render the table
kable(
  knowledge_items,
  col.names = c("", ""),
  booktabs = TRUE, 
      longtable = TRUE,
      #caption = "Science knowledge items",
      full_width = TRUE
) %>%
  kable_styling() %>%
    kableExtra::column_spec(1, width = "2em") %>%
    kableExtra::column_spec(2, width = "40em") %>%
  footnote(
    general = "*Only asked if previous question was answered correctly.",
    general_title = ""
  )
```

The Oxford scale has been criticized for only capturing factual recall [@pardoCognitiveDimensionPublic2004; @bauerWhatCanWe2007]. What should actually matter for trust, according to this critique, is a different kind of knowledge, namely an institutional and methodological understanding of how science works.

To circumvent these limitations, @durantPublicUnderstandingScience1989 also developed a scale of "understanding of processes of scientific inquiry"--several multiple-choice questions about the scientific method and basic concepts of probability. Similarly, @millerPublicUnderstandingAttitudes2004 suggested that a scientifically literate citizen was someone who had both a “(1) a basic vocabulary of scientific terms and constructs; and (2) a general understanding of the nature of scientific inquiry.” His measure of science literacy included open-ended questions, for example on what people understand as the meaning of scientific study [@millerMeasurementCivicScientific1998a]. However, these measures have hardly drawn a more positive image of the public's knowledge of science: Using an index of various understanding questions, @millerPublicUnderstandingAttitudes2004 (p. 288) concluded that "approximately 10 percent of US adults qualified as civic scientifically literate in the late 1980s and early 1990s, but this proportion increased to 17 percent in 1999". Miller explained that, according to his measure, someone qualifies as scientifically literate if they possess "the level of skill required to read most of the articles in the Tuesday science section of The New York Times, watch and understand most episodes of Nova, or read and understand many of the popular science books sold in bookstores today" [@millerPublicUnderstandingAttitudes2004, p. 288].

More recent data suggest that science literacy in the US may have improved slightly since Miller’s assessment during the early 2000s, but still remains low. Based on results from the 2018 US Science & Engineering Indicators, @scheufeleScienceAudiencesMisinformation2019 (p. 7663) report that "one in three Americans (36%) misunderstood the concept of probability; half of the population (49%) was unable to provide a correct description of a scientific experiment; and three in four (77%) were unable to describe the idea of a scientific study." Similarly, the 2024 US Science & Engineering Indicators, based on data from the Pew Research Center’s American Trends Panel (ATP) from 2020, report that "60% of U.S. adults could correctly note that a control group can be useful in making sense of study results" and that "only half of U.S. adults (50%) could correctly identify a scientific hypothesis" [@nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024, p.24].

Not only are levels of science knowledge and understanding low, but they are only weakly correlated with trust in science. In a seminal meta-analysis @allumScienceKnowledgeAttitudes2008 found that Oxford scale type science knowledge was only weakly associated with attitudes towards science. More recently, @colognaTrustScientistsTheir2025 found no statistically significant relationship between national science literacy scores, based on the Program for International Student Assessment (PISA), and national average trust in scientists for the 68 countries included in their study.

This section has established that public trust in science is relatively high, but that knowledge and understanding of science do not seem to be strong determinants of this trust. Does this mean that trust in science is irrational? In the next section we argue that no, not necessarily.

## Is trust in science rational?

Given the contrast between the high levels of trust in science and the low levels of science knowledge, and the weak associations between the two, we can ask whether trust in science has a rational basis.

From a sociological perspective, in particular in a Bourdieusian framework, trust in science may be strongly influenced by *habitus*—a system of dispositions shaped by one’s social class and cultural background [@bourdieuOutlineTheoryPractice1977; @bormannTrustTrustingPractices2019]. Rather than a reasoned appraisal of science’s trustworthiness, trust might result from internalized norms. In line with this suggestion, @archerScienceCapitalConceptual2015 show that school children aged 11-15 years already differ considerably in their "science capital"--an index of several questions pertaining to how much they value and engage with science. These differences were associated with differences in cultural capital (e.g. parental university attendance), gender, and ethnicity. If, for undetermined sociological reasons, science acquires sufficient prestige among some segments of the population, it could lead some people to look up to science and trust it.

While sociological factors play a role in shaping people’s attitudes towards science, we presently introduce a model in which it might be rational for people to trust science, even if they have little current knowledge of it.

# The rational impression account of trust in science

In the rational impression account of trust in science, people trust science because they have been impressed by it. This trust persists even after the specific contents that gave rise to it have been forgotten. The account builds on three basic cognitive mechanisms: First, we infer competence from possessing rare knowledge: if someone knows something that is difficult to know, and we believe it to be true, we are impressed, and deem that individual competent. Second, in many situations, we infer accuracy from consensus: if something is highly consensual, it is likely to be true. Third, impressions can persist without recall of what generated them: while learning about sicence can create lasting impressions, we are likely to forget about specific science knowledge.

## People infer competence from rare knowledge

Estimating other people's competence from communicated information is an essential skill in a variety of social contexts, including learning [@sperberEpistemicVigilance2010], cooperation [@cuddyBIASMapBehaviors2007], or hiring decisions [@fousianiApplyingRemoteJobs2023].

Humans use a variety of cues to estimate others' competence: from superficial, generally unreliable cues such as facial expressions [@todorovUnderstandingEvaluationFaces2008], to more reliable ones, such as providing good explanations [ANY GOOD EVIDENCE ON THAT FOR ADULTS?; for children, see, e.g., @castelainEvidenceThatTwoYearOld2018a] or having made accurate predictions in the past [@mellersIdentifyingCultivatingSuperforecasters2015; for children, see, e.g., @liuSelectiveTrustChildrens2013].

One such cue is possessing certain pieces of knowledge: Adults see others who share valuable ideas as more competent [@altayItMyIdea2020]. With trivia questions, it has been shown that people have accurate perceptions of whether something is hard to know or not, and that they use this information to infer someone's competence [@dubourgUsingNestedStructure2025]: knowing a rare piece of information indicates a high likelihood of knowing more information in the same domain. In the case of science, @pfanderTrustingForgettingImpressive2025 that participants perceived some scientific findings as more impressive than others. Reading about the more impressive scientific findings increased participants' perceptions of both the scientists' competence and the trustworthiness of their discipline. At the same time, participants forgot almost immediately about the specific content that generated these impressions.

For an information to be impressive, at least two criteria should be met: (i) it is perceived as rare or hard to uncover (ii) there is reason to believe it is true. Past research has shown that people have remarkably accurate intuitions regarding (i), but little is know about which features of an information exactly trigger this intuition. Likely, it does not only matter whether obtaining an information required effort, but whether the recipient would have been capable of obtaining the information themselves. For example, most people would probably only be mildly impressed by someone telling them that a given tree has exactly 110201 leaves. Even though obtaining this information implies an exhausting counting effort, everyone in principle knows how to do it. By contrast, finding out that it takes light [approximately 100,000 years to travel from one end of the Milky Way to the other](https://imagine.gsfc.nasa.gov/features/cosmic/milkyway_info.html) is probably impressive to most people, as they would not know how such a distance can be measured. From this view, most of science knowledge is likely very impressive. Less obvious is how people infer (ii), the accuracy of the information, since as described earlier, people do not know much about science. Below, we describe how perceived consensus might be the main relevant cue for accuracy.

## People infer accuracy from consensus

In order to make the best of communicated information, individuals need to be able to evaluate it, i.e. being able to distinguish inaccurate and harmful from accurate and beneficial information [@maynard-smithAnimalSignals2003]. It has been argued that humans have evolved a suite of cognitive mechanisms to serve this function [@sperberEpistemicVigilance2010; @mercierNotBornYesterday2020]. In particular, we rely on cues of an informant's trustworthiness, and check the plausibility of an information against our background knowledge.

In the case of science, reliable cues and background information are scarce: people generally have little first-hand information to evaluate individual scientists' trustworthiness, because they don't know scientists personally. People also largely lack relevant background knowledge to evaluate the plausibility of scientific findings. Sometimes, to a certain extent, people might be able to judge the accuracy of scientific findings for themselves, for example when they are exposed to accessible and convincing explanations in school [@readExplanatoryCoherenceSocial1993; @lombrozoSimplicityProbabilityCausal2007; for a review, see @lombrozoStructureFunctionExplanations2006]. But for most scientific research, people cannot possibly evaluate the quality of the information for themselves, let alone make their own observations (e.g. quantum mechanics, genes).

An additional way to evaluate whether something is true or not is to aggregate opinions. It has been shown that, when no better information is available, people rely on majority heuristics: the more others agree on something, the more likely we are to believe them to be right [@mercierMajorityRulesHow2019]. It is a well established result in the literature on the wisdom of crowds that making this inference–to perceive convergent opinions as more accurate–is often appropriate [see e.g., @hastieRobustBeautyMajority2005]. However, this literature, in particular the Condorcet Jury Theorem, assumes that informants–the individuals providing answers–need to be at least minimally competent [i.e. better than chance, @decondorcetEssaiApplicationAnalyse2014]. This is a problem for the rational impression account, as it ultimately seeks to explain how people come to judge scientists as competent, and therefore cannot assume prior competence. However, recently, @pfanderHowWiseCrowd2025 have shown that it is often enough to assume that informants are unbiased to make justified inferences from their agreement to not only the accuracy of their answers, but also their competence. Participants made these inferences in abstract scenarios [@pfanderHowWiseCrowd2025], but other research suggests that they are justified across a wide range of real-world decision making scenarios [@kurversHowDetectHighperforming2019].

In the absence of other reliable cues and background knowledge, inferences from consensus are likely to be given considerable weight in a cognitive system of epistemic vigilance. In the case of science, this weight should play in favor of science's perceived trustworthiness: It has been argued that, by contrast with other intellectual enterprises, consensus is the defining trait of science [@collinsSociologyPhilosophiesGlobal2002]. Not only do scientists agree on things, but they agree on impressive things–things that would be impossible for any individual to ever uncover for themselves, such as the distance between the solar system and the center of the galaxy, or the atomic structure of DNA.

More indirect evidence suggests that people make this inference also in the context of science: @pfanderFrenchTrustMore2025 showed that in France, people trust scientists more when they work in disciplines that people perceive as more consensual. Yet more suggestive evidence comes from a popular psychological model, the "gateway model". The model suggests that informing people about the scientific consensus on specific issues acts as a gateway to change their beliefs on these issues [@vanderlindenGatewayBeliefModel2021]. Studies have demonstrated the effectiveness of consensus messaging in changing people's beliefs on contentious science topics such as climate change [@veckalov27countryTestCommunicating2024] or vaccination [@salmonVaccineHesitancyCauses2015; for an overview of results on vaccination, climate change, and genetically modified food, see @vanstekelenburgScientificConsensusCommunicationContested2022]. This evidence is only suggestive, however, because the fact that consensus changes people's beliefs or attitudes does not necessarily require enhanced trust. An alternative explanation, for example, is normative conformity-that is, when people follow the majority because of social pressure rather than a belief that the majority is correct [@mercierMajorityRulesHow2019].[^2]

[^2]: However, an accuracy inference seems to be the more plausible mechanism here: Studies on consensus messaging do not seem to be settings of high social pressure that we might expect to produce instances of normative conformity, compared to, for instance, the famous Asch experiments [@aschStudiesIndependenceConformity1956].

So far, we have argued that people are impressed by information that is difficult to acquire, given that it is true, and that to establish whether it is true, they use the degree of consensus as a cue. Applied to science, the prediction is that the more people are exposed to impressive science, the more they perceive scientists' at competent and, as a result, trust science more. This appears similar to the deficit model, as more exposure to science should lead to more science knowledge. This might be true to a limited extent, as suggests the weak correlation between science knowledge and attitudes towards science [@allumScienceKnowledgeAttitudes2008]. However, as we argue below, people likely forget most specific science content they had been exposed to, while an impression of trustworthiness persists.

## People forget specific knowledge while impressions persist

We commonly form impressions of the people around us while forgetting the details of how we formed these impressions: If a colleague fixes our computer, we might forget exactly how they fixed it, yet remember that they are good at fixing computers. Similarly, people might forget the specific content of science knowledge they have been exposed to, but retain an impression of scientists' trustworthiness. Several research strings suggest that abstract impressions can persist, while recall of specific information fades.

Memory research has suggested that implicit memory is more stable than explicit memory. In classic word completion paradigms, participants are first shown a vocabulary list. Next, they are given two tasks: First, they are given word fragments–some of which from words on the initial list ("primed" words) and some of which were not–and are asked to complete them. Second, participants are asked whether they have seen the word on the initial list or not. Participants were asked repeated these tasks after a certain time intervals. The researchers observed a priming effect: participants did considerably better in completing primed words. Importantly, this priming effect–the implicit memory–was considerably more persistent than the participants' ability of word recognition–explicit memory [@slomanForgettingPrimedFragment1988; @parkinDifferentialNatureImplicit1990].

Other research has argued that memory encodes information both as "verbatim" details–exact words or numbers–information and "gist" representations–the essence or bottom-line meaning [@reynaScientificTheoryGist2021], and that the verbatim memory tends to fade faster [@murphyForgettingVerbatimInformation1994].

More extreme examples supporting the idea that impressions can be detached from knowledge come from medical research: patients with severe amnesia, for instance, can continue to experience emotions linked to events they could not recall [@feinsteinSustainedExperienceEmotion2010]. Other research has shown that patients with profound episodic memory impairment due to dementia continue to show capacity for emotional learning [@evans-robertsRememberingRelationshipsPreserved2010].

Some research in the context of science, suggest that processes of impression formation and knowledge retention can be quite detached: it has been shown that while people find some science-related explanations more satisfying than others, this did not predict how well they could recall the explanations shortly after [@liquinMotivatedLearnAccount2022].

## Additional predictions of the rational impressions account

The rational impressions account makes several additional predictions. First, competence should be the main dimension of scientists' trustworthiness. In social psychology, a popular model suggests that people evaluate others along two fundamental dimensions: competence and warmth [@cuddyWarmthCompetenceUniversal2008]. Similarly, for trust in scientists, researchers have distinguished between an epistemological and an ethical dimension [@wilholtEpistemicTrustScience2013; @intemannScienceCommunicationPublic2023]. Sometimes, researchers make more fine-grained distinctions: For example, @hendriksMeasuringLaypeoplesTrust2015 have argued for three dimensions: expertise/competence, integrity, and benevolence. @besleyReassessingVariablesUsed2021a has suggested openness as an additional fourth dimension. Across these dimensions, anything but competence should be very hard to evaluate for people: there are relatively very few scientists in the world, and most people probably do not know any personally. The only other way they could judge scientists' character is through media coverage. But news on science–by contrast, for example, with news on politicians–mostly tend to concern the science, not the scientists. Besides, people consume very little news in general [@newmanDigitalNewsReport2023]. Competence, however, can be judged based on the mechanisms of the rational impression account. Accordingly, scientists should score higher in competence evaluations than in other dimensions of trustworthiness. In line with this prediction, it has been shown that people perceive scientists as very competent, but not so much as warm [@fiskeGainingTrustWell2014]. A recent Pew survey found that 89% of Americans viewed research scientists as intelligent, but only 65% viewed them as honest, and only 45% described research scientists as good communicators [@kennedyPublicTrustScientists2024; see also @fiskeGainingTrustWell2014]. Beyond the US, a recent study confirmed this tendency on a global scale [@colognaTrustScientistsTheir2025]: People perceived scientists as highly competent, with 78% tending to believe that scientists are qualified to conduct high-impact research. By contrast, people held scientists in lower esteem with regards to their integrity and benevolence: Only 57% of people tended to believe that most scientists are honest, and only 56% tended to believe that most scientists are concerned about people’s well-being.

Second, education, and more precisely science education, should be the main correlate of trust in science. Since most people consume very little news [@newmanDigitalNewsReport2023], the bulk of exposure to science can be assumed to happen during education. Education, and in particular science education, has been consistently identified as one of the strongest correlates of trust in science [@noyScienceGoodEffects2019; @wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020; but see @colognaTrustScientistsTheir2025 who only find a small positive relationship between tertiary education and trust in science]. This is compatible with the fact that people, even those who received a science education, do not know much about science: if we assume that education has some causal effect on trust in science, this effect does not need to be driven by a pure transmission of knowledge and understanding [for a similar argument, see @bakEducationPublicAttitudes2001]. The candidate mechanism proposed by the rational impression account is exposure to impressive scientific content. Students might not understand much of it, and potentially recall even less later on; but they might have been impressed by it, to the point that they come to perceive scientists as competent, and thus, everything else equal, as trustworthy. This impression might persist even when specific knowledge vanishes. In line with this, @mottaEnduringEffectScientific2018 found that, in the US, the more children were interested in science at age 12–14 years, the more they tended to trust in climate scientists in adulthood (mid thirties), irrespective of their political ideology.

Third, people with a basic science education should trust essentially all of basic science. These people should have had the opportunity to form impressions of trustworthiness of science. This should have built a solid baseline of trust in science. People might deviate from this default and distrust science on certain specific science topics for other reasons, but they should trust most of science. This is in line with the finding that in the US, almost everyone--even people who say they don't trust science in general or who hold specific beliefs blatantly violating scientific knowledge (e.g. that the earth is flat)--trusts almost all of basic science knowledge (e.g. that electrons are smaller than atoms) [@pfanderQuasiuniversalAcceptanceBasic2025].

# Discussion

It has long been a puzzle to the deficit model–which suggests that trust in science is primarily driven by science knowledge–that knowledge of science is at best weakly associated with science attitudes [@allumScienceKnowledgeAttitudes2008; @nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. The rational impression account can make sense of this: it lays out how trusting science without recalling specific knowledge can be the result of a sound inference process, rooted in basic cognitive mechanisms of information evaluation.

How is the rational impression account "rational", if it posits that trust is largely detached from recalling specific knowledge? It is rational in that the cognitive mechanisms it builds on lead to sound inferences in many contexts. If someone discovers something that is hard to know, such as the size of the Milky Way, and there appears to be a consensus, we should expect them to be competent, even without knowing the details of how they made this discovery. Even forgetting specific knowledge is not irrational: It has been argued that one of the main functions of episodic memory is to justify our beliefs in communication with others [@mahrWhyWeRemember2018]. As a result, we should be particularly good at remembering things we might need to convince others of. In this regard, incentives of remembering science seem to be weak: Most exposure to science happens at school, and there is little reason for young learners’ minds to anticipate having to convince others of the merits of specific scientific findings, which are typically of little practical relevance to them, and which appear to be consensually accepted.

The account is compatible with the finding that education, and in particular science education, has been repeatedly identified as one of the strongest correlates of trust in science [@bakEducationPublicAttitudes2001; @noyScienceGoodEffects2019; @wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020; but see @colognaTrustScientistsTheir2025]. By contrast with the deficit model, it suggests that the main causal role of education for public trust in science is not transmission of knowledge and understanding, but impression generation.

The rational impression account aligns with recent normative accounts of what makes science trustworthy. Instead of particular institutional features-certain methods, norms, or processes–these accounts argue that the trustworthiness of science lies in its diversity: @cartwrightTangleScienceReliability2022 make the case that scientific knowledge emerges from a "tangle" of results, relying on diverse research methods. @oreskesWhyTrustScience2019 makes a similar case: She argues that scientific practice takes place in different scientific communities who rely on a variety of different research methods. Through some shared practices, in particular peer-review, these communities engage in critical dialogue. What makes scientific knowledge trustworthy, according to Oreskes, is when from this diversity of actors and methods, a consensus emerges. According to this view, to infer trustworthiness, people should have a representation of the diversity of science. The rational impression account is, in a way, less strict: it does not require a representation of diversity. It does require, however, that people have a representation of science as an institution of independent thinkers.

The rational impressions account faces several limitations. First, it proposes a possible micro-level model of trust in science and should be seen as complementing, not competing with, macro-level processes that shape public trust in science. The rational impression account fits with a sociological literature investigating how "individual cognition and practice establish and maintain institutional fields and status hierarchies, especially in the face of imperfect knowledge" [@gauchatCulturalCognitiveMappingScientific2018, p.569]. However, sociological macro-level accounts have described how trust in science is entangled with broader cultural and political dynamics. These accounts, like the individual-level accounts reviewed above, tend to focus on explaining distrust in science. For example, @gauchatCulturalAuthorityScience2011 describes the 'alienation model', according to which the "public disassociation with science is a symptom of a general disenchantment with late modernity, mainly, the limitations associated with codified expertise, rational bureaucracy, and institutional authority” [@gauchatCulturalAuthorityScience2011, p.2]. This explanation builds on the work of social theorists [@habermasJurgenHabermasSociety1989; @beckRiskSocietyNew1992; @giddensModernitySelfidentitySelf1991; see @gauchatCulturalAuthorityScience2011 for an overview] who suggested that a modern, complex world increasingly requires expertise, and thus shapes institutions of knowledge elites. People who are not part of these institutions experience a lack of agency, resulting in a feeling of alienation. Similarly, @gauchatLegitimacyScience2023 argues that politicization of science in the US needs to be seen in its broader cultural context. Precisely, according to Gauchat, science has enabled the authority of the modern regulatory state. Consequently, conservative distrust of science reflects deeper structural tensions with the institutions and rational–legal authority of modern governance. At the micro-level, this is consistent with research showing that right-wing authoritarian ideology is associated with distrust towards science and scientists [@kerrRightwingAuthoritarianismSocial2021].

A second limitation of the rational impression account is that it assumes people have a representation of science as consensual. However, in practice–with perhaps some exceptions, such as during the Covid-19 pandemic–most people do not literally compare the opinions of different scientists for themselves and come to the conclusion that something is largely consensual. Where, then, could the representation of consensus possibly emerge? A plausible explanation, we believe, is that education fosters a representation of consensus: During education, in particular during early education, knowledge is typically presented as simply the result of science–a seemingly unanimous enterprise that produces knowledge. School books hardly teach about historical science controversies, suggest uncertainty around scientific findings, or cover cutting-edge research where disagreements are the norm. This could induce a default consensus assumption in people's perceptions of science. However, this argument is of course only speculative.

Third, the rational impression account cannot explain, for example, why people with no education, and thus presumably very little exposure to science, have some trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. A possible explanation could be two step effects, via some educated people who trust science whom they trust.

Fourth, conversely, the account also cannot explain why, in a context of the global north, where essentially everyone has been exposed to science through a basic science education, some people do not trust some aspects of science, or say they don't trust science in general [even if that is not really true, see @pfanderQuasiuniversalAcceptanceBasic2025]. Suggestions have already been made for a number of issues such as vaccination ([Miton and Mercier 2015](https://janpfander.github.io/phd_thesis/references.html#ref-mitonCognitiveObstaclesProVaccination2015)), GMOs ([Blancke et al. 2015](https://janpfander.github.io/phd_thesis/references.html#ref-blanckeFatalAttractionIntuitive2015)), or nuclear energy ([Hacquin et al. 2021](https://janpfander.github.io/phd_thesis/references.html#ref-hacquinDisgustSensitivityPublic2021)). However, research is still needed to better understand what motivates these rejections (see e.g., [Hornsey 2020](https://janpfander.github.io/phd_thesis/references.html#ref-hornseyWhyFactsAre2020)).

Beyond these theoretical and empirical limitations, the rational impression account is limited in its implications. First, we do not believe that flooding people with impressive consensual science knowledge is the key to overcoming all distrust of science. In the context of trust in political institutions, it has been argued that trust and distrust are not necessarily symmetrical: what causes the former might not help alleviate the latter [@bertsouRethinkingPoliticalDistrust2019]. We believe this is at least to some degree true for science, too. For example, consensus messaging has been shown to help convince people to trust science on particular issues, such as climate change or vaccines, but it is less clear whether it worked by fostering perceptions of trustworthiness. It could be the case that the people convinced by consensus messages already trusted science, but have not held strong opinions on the specific matter. This is not implausible, since it has been shown that on most matters, large segment of the public do not have opinions [@bourdieuPublicOpinionDoes1979; @zallerNatureOriginsMass1992]. For people who do not only lack trust, but who actively distrust, motivated reasoning accounts are likely better suited as a theoretical framework. Addressing relevant underlying motivations directly might be more fruitful to mitigate distrust in science than exposing people to consensual science more generally.

Second, and related, just because we propose an account by which trust in science can be the result of a rational cognitive process, this does not imply that, conversely, all distrust in science is irrational. Some groups of people do in fact have good reasons not to trust science. For example, some science has historically contributed to fostering racism [see e.g. @fuentesSystemicRacismScience2023; @noblesScienceMustOvercome2022], via instances such as the tragically famous Tuskegee syphilis study [@brandtRacismResearchCase1978; @scharffMoreTuskegeeUnderstanding2010].

Third, we do not think that science communication should stress consensus at all costs. In the rational impression account, consensus plays a central role for generating trust. However, this should not incentivize science communicators to neglect transparency about uncertainty. Acknowledging uncertainty in science communication has been argued to be crucial for fostering long term trust in science [@druckmanCommunicatingPolicyRelevantScience2015]. For example, in the context of Covid-19 vaccines, @petersenTransparentCommunicationNegative2021 have shown that communicating uncertainty is crucial for building long term trust in health authorities.

Fourth, science communication should not aim for impressiveness at all costs either. Research has shown that intellectual humility can increase trust in scientists [@koetkeEffectSeeingScientists2024]. Trying to oversell scientific results might therefore backfire. People appear to value transparency via open data practices in science [@songTrustingShouldersOpen2022], and trust science that replicates more [@hendriksReplicationCrisisTrust2020]. We should therefore expect that simply doing better, more transparent science, and being humble about it, is likely to be the most effective strategy to impress the public and elicit perceptions of trustworthiness.

Fifth, educators should not stop aiming at fostering a proper understanding of science. Most students might not understand all of the content, or recall much specific knowledge later on. However, for some students at least, some of that knowledge will be remembered, and will prove important in their lives. Second, to be impressive, a piece of information does not need to be confusingly complex. In fact, a proper understanding of research findings and their methods might even help in appreciating their complexity–even if, once again, that understanding is forgotten later.

Despite these limitations, we believe that the rational impressions account offers optimism for studies of science-society interfaces, and the field of science communication in particular: Exposure to science, especially one that leaves an impression, might be the foundation of public trust in science. This means that effective science communication is essential for fostering trust in science. Low scientific literacy levels should not discourage education and communication efforts, as they are not necessarily a good indicator of the value added in terms of fostering trust in science.

Taking a broader perspective, our account fits into a picture of humans as not gullible [@mercierHowGullibleAre2017; @mercierNotBornYesterday2020]. The "failure" of the knowledge account of trust in science–the fact that science knowledge appears to not be strongly associated with trust in science–might suggest that public trust in science is, to a large extent, irrational. The notion that trust in science is irrational or easily granted may amplify concerns about the impact of misinformation: if trust lacks a solid, rational foundation, then we would expect misinformation to easily lead people astray. There is much work to be done still to understand how misinformation impacts people's beliefs, and in particular elite-driven misinformation and more subtle forms of misinformation, such as one-sided reporting. But it has been shown that people are generally able to distinguish between true and false news and, if anything, tend to be generally skeptical of news [@pfanderSpottingFalseNews2025]. As a consequence, for a better informed public, fighting for (true) information seems at least as relevant as fighting against misinformation [@acerbiResearchNoteFighting2022]. Misinformation researchers increasingly acknowledge this: A recent report on science misinformation by the National Science Foundation [@nationalacademiesofsciencesUnderstandingAddressingMisinformation2024] dedicates considerable space on developing strategies to produce better information, for example by promoting high-quality science, health, and medical journalism.

The rational impression account stresses the role of fighting for information, when it comes to fostering trust in science. Well-placed trust in science does not require profound understanding or recall of specific knowledge; but it does require exposure to good science.

# References

::: {#refs}
:::
