\documentclass[
  jou,
  floatsintext,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[bidi=default]{babel}
\babelprovide[main,import]{english}


% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}

\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\usepackage{subcaption}


\renewcommand\thesubfigure{\Alph{subfigure}}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}




\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}





\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{The rational impression account of trust in science}


\shorttitle{The rational impression account of trust in science}


\usepackage{etoolbox}








\authorsnames{Jan Pfänder,Hugo Mercier}





\affiliation{
{Institut Jean Nicod, Département d'études cognitives, ENS, EHESS, PSL
University, CNRS, France}}




\leftheader{Pfänder and Mercier}



\abstract{Trust in science plays a crucial role in addressing major
societal challenges, from climate change to global health crises. Across
the globe most people tend to trust science, but this trust appears
broadly irrational, as people tend to know little about science. Here,
we argue that people need not possess detailed scientific knowledge or
profound understanding of science to rationally view it as trustworthy.
We propose a cognitive model of trust in science--the rational
impression account--according to which people come to trust science by
relying on a suite of basic cognitive mechanisms: First, people infer
competence from possessing rare knowledge; second, people infer accuracy
from consensus; third, people's impressions can persist they forget what
generated them. {[}TO BE COMPLETED AND EDITED{]} }

\keywords{trust in science, science literacy, deficit model}

\authornote{\par{\addORCIDlink{Jan
Pfänder}{0009-0009-4389-2807}}\par{\addORCIDlink{Hugo
Mercier}{0000-0002-0575-7913}} 
\par{ }
\par{   The authors have no conflicts of interest to disclose.    }
\par{Correspondence concerning this article should be addressed to Hugo
Mercier, Email: \href{mailto:hugo.mercier@gmail.com}{hugo.mercier@gmail.com}}
}

\usepackage{pbalance}
% \usepackage{float}
\makeatletter
\let\oldtpt\ThreePartTable
\let\endoldtpt\endThreePartTable
\def\ThreePartTable{\@ifnextchar[\ThreePartTable@i \ThreePartTable@ii}
\def\ThreePartTable@i[#1]{\begin{figure}[!htbp]
\onecolumn
\begin{minipage}{0.485\textwidth}
\oldtpt[#1]
}
\def\ThreePartTable@ii{\begin{figure}[!htbp]
\onecolumn
\begin{minipage}{0.48\textwidth}
\oldtpt
}
\def\endThreePartTable{
\endoldtpt
\end{minipage}
\twocolumn
\end{figure}}
\makeatother


\makeatletter
\let\endoldlt\endlongtable		
\def\endlongtable{
\hline
\endoldlt}
\makeatother

\newenvironment{twocolumntable}% environment name
{% begin code
\begin{table*}[!htbp]%
\onecolumn%
}%
{%
\twocolumn%
\end{table*}%
}% end code

\urlstyle{same}



\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{placeins}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

%% Disable longtable counter
%% https://tex.stackexchange.com/a/248395/211326

\usepackage{etoolbox}

\makeatletter
\patchcmd{\LT@caption}
  {\bgroup}
  {\bgroup\global\LTpatch@captiontrue}
  {}{}
\patchcmd{\longtable}
  {\par}
  {\par\global\LTpatch@captionfalse}
  {}{}
\apptocmd{\endlongtable}
  {\ifLTpatch@caption\else\addtocounter{table}{-1}\fi}
  {}{}
\newif\ifLTpatch@caption
\makeatother

\begin{document}

\maketitle


\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\setlength\LTleft{0pt}


\section{Introduction}\label{introduction}

Addressing important societal challenges, from fighting climate change
to managing pandemics, is greatly facilitated by trust in science.
Studies have demonstrated that individuals with higher levels of trust
in science are more likely to accept the scientific consensus on global
warming (\citeproc{ref-bogertEffectTrustScience2024}{Bogert et al.,
2024}), to engage in pro-environmental behavior, and to support climate
policies (\citeproc{ref-colognaRoleTrustClimate2020}{Cologna \&
Siegrist, 2020};
\citeproc{ref-hornseyMetaanalysesDeterminantsOutcomes2016}{Hornsey et
al., 2016}). Trust in science has also been shown to be positively
associated with willingness to get vaccinated
(\citeproc{ref-sturgisTrustScienceSocial2021}{Sturgis et al., 2021}; for
Covid-19 in particular,
\citeproc{ref-lindholtPublicAcceptanceCOVID192021}{Lindholt et al.,
2021}). During the Covid-19 pandemic, a panel study in 12 countries
found that trust in scientists was the strongest predictor of whether
people followed public health guidelines, such as mask-wearing or social
distancing (\citeproc{ref-alganTrustScientistsTimes2021}{Algan et al.,
2021}). Similar results have been found by other studies (e.g., positive
effects of trust in science on acceptance of social distancing in the
US, \citeproc{ref-koetkeTrustScienceIncreases2021}{Koetke et al.,
2021}).

Given the individual and social cost of a lack of trust in science, most
studies have focused on understanding why some people do not trust
science. However, it also important to understand why most people
\emph{do} trust science: it is important theoretically, as this trust
could stem from very different processes--from blind deference to a
rational assessment of scientific evidence; it is important practically,
as, depending on why people trust science, different interventions aimed
at increasing trust in science could be conceived. Here, we argue that,
even though people do not know much about science, their trust in
science can still be rational.

We start by reviewing work on explanations for variations in trust in
science, arguing that this work has not fully solved a basic puzzle: why
do most people tend to trust science, in spite of knowing so little
about it?

To solve this puzzle, we then develop a \emph{rational impression
account} of trust in science. According to this account, people do not
need a profound understanding or detailed knowledge of science to
rationally perceive it as trustworthy. Instead, by appealing to basic
cognitive mechanisms of information evaluation, science impresses
people, who then mostly forget what had impressed them.

\section{The puzzle of why people trust
science}\label{the-puzzle-of-why-people-trust-science}

A widely agreed-upon definition of trust is the willingness to be
vulnerable to another party--whether an individual, a group, or an
institution
(\citeproc{ref-mayerIntegrativeModelOrganizational1995a}{Mayer et al.,
1995}; \citeproc{ref-rousseauIntroductionSpecialTopic1998}{Rousseau et
al., 1998}). Building on this idea, trust in science has been defined as
``one's willingness to rely on science and scientists (as
representatives of the system) despite having a bounded understanding of
science'' (\citeproc{ref-wintterlinPredictingPublicTrust2022}{Wintterlin
et al., 2022, p. 2}). This definition implies that trust in science goes
beyond knowledge of science. Yet, the idea that knowledge of science is
the primary cause of trust in--and more generally positive attitudes
towards--science has long dominated research on public understanding of
science (\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007}). This
idea is widely known under the term ``deficit model'', because much of
the literature attested to the public ``depressingly low levels of
scientific knowledge'' that were assumed to be the principal cause of
negative attitudes towards science
(\citeproc{ref-sturgisScienceSocietyReEvaluating2004}{Sturgis \& Allum,
2004, p. 56}).

The deficit model has been criticized for idealizing science and viewing
the public as deficient and irrational
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011}).
Moreover, as reviewed below, the relationship between science knowledge
and trust in science is rather tenuous. As a result, the literature has
mostly moved beyond the idea of science knowledge as the principle
driver of trust in science. However, the focus on explaining a lack of
trust, rather than trust, persists.

Researchers have increasingly turned to how people's values, world
views, and identities shape their attitudes towards science
(\citeproc{ref-hornseyAttitudeRootsJiu2017a}{Hornsey \& Fielding, 2017};
\citeproc{ref-lewandowskyWorldviewmotivatedRejectionScience2021}{Lewandowsky
\& Oberauer, 2021}). The psychological literature has focused on
explaining negative attitudes towards science with motivated
reasoning--selecting and interpreting information to match one's
existing beliefs or behaviors
(\citeproc{ref-hornseyWhyFactsAre2020}{Hornsey, 2020};
\citeproc{ref-lewandowskyRoleConspiracistIdeation2013}{Lewandowsky et
al., 2013};
\citeproc{ref-lewandowskyMotivatedRejectionScience2016}{Lewandowsky \&
Oberauer, 2016}). This research mostly suggests that certain
psychological traits, such as a social dominance orientation, or a
tendency to engage in conspiracy thinking, lead people to reject
science. Arguments on a general conspiratory thinking style as one of
the root causes of science rejection shift the debate, to some extent,
from a knowledge deficit to a broader reasoning deficit
(\citeproc{ref-hornseyAttitudeRootsJiu2017a}{Hornsey \& Fielding, 2017};
\citeproc{ref-rutjensConspiracyBeliefsScience2022}{Rutjens \& Većkalov,
2022}).

Since the Covid-19 pandemic, the role of misinformation in fostering
distrust in science has received more attention
(\citeproc{ref-druckmanThreatsSciencePoliticization2022}{Druckman,
2022};
\citeproc{ref-nationalacademiesofsciencesUnderstandingAddressingMisinformation2024}{National
Academies of Sciences, 2024};
\citeproc{ref-scheufeleScienceAudiencesMisinformation2019}{Scheufele \&
Krause, 2019}). For this literature, by contrast with the deficit model,
the problem of trust in science is less a lack of information, and more
the abundance of harmful information.

By contrast with the deficit model and its stress on content, more
recent literature in science communication has shifted the focus on
source-based explanations of trust: from science knowledge to
perceptions of scientists. This work has established that people
evaluate scientists along different dimensions
(\citeproc{ref-intemannScienceCommunicationPublic2023}{Intemann, 2023}),
including competence, but also also integrity, benevolence, or openness
(\citeproc{ref-besleyReassessingVariablesUsed2021a}{Besley et al.,
2021}; \citeproc{ref-hendriksMeasuringLaypeoplesTrust2015}{Hendriks et
al., 2015}). This literature suggests that, for enhancing trust in
science, the latter, warmth-related dimensions could be particularly
relevant (\citeproc{ref-fiskeGainingTrustWell2014}{Fiske \& Dupree,
2014}): people would already perceive scientists as very competent, but
not as very warm, thus offering a greater margin for improvement.

Another explanation for distrust towards science is the alienation model
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011}).
According to this model, the ``public disassociation with science is a
symptom of a general disenchantment with late modernity, mainly, the
limitations associated with codified expertise, rational bureaucracy,
and institutional authority''
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011, p.
2}). This explanation builds on the work of social theorists
(\citeproc{ref-beckRiskSocietyNew1992}{Beck, 1992};
\citeproc{ref-giddensModernitySelfidentitySelf1991}{Giddens, 1991};
\citeproc{ref-habermasJurgenHabermasSociety1989}{Habermas, 1989}; see
\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011} for an
overview) who suggested that a modern, complex world increasingly
requires expertise, and thus shapes institutions of knowledge elites.
People who are not part of these institutions experience a lack of
agency, resulting in a feeling of alienation.

Overall, as reviews of science-society research have noted, the
literature continues to operate in ``deficit'' paradigms
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-scheufeleThirtyYearsScience2022}{Scheufele, 2022}) which
focus on why some people do not trust science, and do not attempt to
explain the elevated levels of trust in science observed in most places
in the world, as reviewed presently.

\subsection{People tend to trust
science}\label{people-tend-to-trust-science}

Across the globe, most people do trust science, at least to some extent.
A recent study in 68 countries found that, across the globe, trust in
scientists was ``moderately high'' (mean = 3.62; sd= 0.70; Scale: 1 =
very low, 2 = somewhat low, 3 = neither high nor low, 4 = somewhat high,
5 = very high), with not a single country below midpoint trust
(\citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et al., 2025}).
Long-term global data on trust in science across time is sparse, yet the
available data suggests, if anything, a recent increase of trust in
science: In 2018, the Wellcome Global Monitor (WGM) surveyed of over
140000 people in over 140 countries on trust in science
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018}). In 2020, during the first year of the Covid
pandemic and before vaccines were widely available, a follow-up survey
was conducted in 113 countries, involving 119000 participants
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{Wellcome
Global Monitor, 2020}). Between these two surveys, on average, trust in
science had risen
(\citeproc{ref-wellcomeglobalmonitorPublicTrustScientists2021}{Wellcome
Global Monitor, 2021}): In 2020, 41\% (32\% in 2018) of respondents said
they trust science a lot, 39\% (45\% in 2018) said they trust science to
some extent, 13\% (also 13\% in 2018) said they trust science ``not much
or not at all'' (with the rest answering``don't know'').

In the US, where long term data is available from the US General Social
Survey (GSS), this public trust appears to be both remarkably stable and
elevated relative to other institutions
(\citeproc{ref-funkScienceScientistsHeld2020}{Funk et al., 2020};
\citeproc{ref-funkPublicConfidenceScientists2020}{Funk \& Kennedy,
2020}; \citeproc{ref-smithTrendsPublicAttitudes2013}{Smith \& Son,
2013}): From the early 1970s to 2022, the currently the largest
available time span, on average 43\% of Americans say they have a great
deal of confidence in the scientific community. This is the second
highest score (just behind medicine, 45\%) among 13 institutions listed
in the GSS, surpassing e.g., the Supreme Court, organized religion, or
the military\footnote{Numbers are based on our own calculations using on
  the publicly available GSS cumulative data}. Note, however, that the
most recent polls suggest a small drop in trust in science in the US
(\citeproc{ref-lupiaTrendsUSPublic2024}{Lupia et al., 2024}).

\subsection{People do not know much about
science}\label{people-do-not-know-much-about-science}

As mentioned above, one of the main issues with the deficit model of
trust in science is that the relatively high levels of trust in science
do not seem to be matched by commensurate levels of science knowledge.

Early attempts of measuring science knowledge have developed what is
known as the ``Oxford scale'' (Table~\ref{tbl-oxford}) to measure
science knowledge--a set of specific true/false or multiple-choice
questions about basic science facts
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016}). Survey results
from the Oxford scale have been interpreted as revealing a science
knowledge deficit among the public. For example, Durant et al.
(\citeproc{ref-durantPublicUnderstandingScience1989}{1989}) (p.11)
initially reported that only ``34\% of Britons and 46\% of Americans
appeared to know that the Earth goes round the Sun once a year, and just
28\% of Britons and 25\% of Americans knew that antibiotics are
ineffective against viruses''. According to the National Academies of
Sciences, Engineering, and Medicine
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{2016}),
performance on the Oxford scale items in the US has been ``fairly stable
across 2 decades''
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016, p. 51}).

\begin{twocolumntable}

\begin{longtable}[t]{>{\raggedleft\arraybackslash}p{2em}>{\raggedright\arraybackslash}p{40em}}

\caption{\label{tbl-oxford}}

\tabularnewline

\toprule
 & \\
\midrule
1 & The center of the Earth is very hot. (True)\\
2 & The continents on which we live have been moving their locations for millions of years and will continue to move in the future. (True)\\
3 & Does the Earth go around the Sun, or does the Sun go around the Earth? (Earth around Sun)\\
4 & How long does it take for the Earth to go around the Sun? (One year)\textbackslash{}*\\
5 & All radioactivity is man-made. (False)\\
\addlinespace
6 & It is the father’s gene that decides whether the baby is a boy or a girl. (True)\\
7 & Antibiotics kill viruses as well as bacteria. (False)\\
8 & Electrons are smaller than atoms. (True)\\
9 & Lasers work by focusing sound waves. (False)\\
10 & Human beings, as we know them today, developed from earlier species of animals. (True)\\
\addlinespace
11 & The universe began with a huge explosion. (True)\\
\bottomrule
\multicolumn{2}{l}{\rule{0pt}{1em}*Only asked if previous question was answered correctly.}\\

\end{longtable}

An 11-item version of the Oxford-scale, as reported in a comprehensive
review of the literature on scientific literacy
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016})

\end{twocolumntable}

The Oxford scale has been criticized for only capturing factual recall
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-pardoCognitiveDimensionPublic2004}{Pardo \& Calvo, 2004}).
What should actually matter for trust, according to this critique, is a
different kind of knowledge, namely an institutional and methodological
understanding of how science works.

To circumvent these limitations, Durant et al.
(\citeproc{ref-durantPublicUnderstandingScience1989}{1989}) also
developed a scale of ``understanding of processes of scientific
inquiry''--several multiple-choice questions about the scientific method
and basic concepts of probability. Similarly, Miller
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{2004}) suggested
that a scientifically literate citizen was someone who had both a ``(1)
a basic vocabulary of scientific terms and constructs; and (2) a general
understanding of the nature of scientific inquiry.'' His measure of
science literacy included open-ended questions, for example on what
people understand as the meaning of scientific study
(\citeproc{ref-millerMeasurementCivicScientific1998a}{Miller, 1998}).
However, these measures have hardly drawn a more positive image of the
public's knowledge of science: Using an index of various understanding
questions, Miller concluded that ``approximately 10 percent of US adults
qualified as civic scientifically literate in the late 1980s and early
1990s, but this proportion increased to 17 percent in 1999''
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{Miller, 2004, p.
288}). Miller explained that, according to his measure, someone
qualifies as scientifically literate if they possess ``the level of
skill required to read most of the articles in the Tuesday science
section of The New York Times, watch and understand most episodes of
Nova, or read and understand many of the popular science books sold in
bookstores today''
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{Miller, 2004, p.
288}).

More recent data suggest that science literacy in the US may have
improved slightly since Miller's assessment during the early 2000s, but
that it remains low. Based on results from the 2018 US Science \&
Engineering Indicators, Scheufele and Krause
(\citeproc{ref-scheufeleScienceAudiencesMisinformation2019}{2019})
(p.~7663) report that ``one in three Americans (36\%) misunderstood the
concept of probability; half of the population (49\%) was unable to
provide a correct description of a scientific experiment; and three in
four (77\%) were unable to describe the idea of a scientific study.''
Similarly, the 2024 US Science \& Engineering Indicators, based on data
from the Pew Research Center's American Trends Panel (ATP) from 2020,
report that ``60\% of U.S. adults could correctly note that a control
group can be useful in making sense of study results'' and that ``only
half of U.S. adults (50\%) could correctly identify a scientific
hypothesis''
(\citeproc{ref-nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024}{National
Science Board, National Science Foundation, 2024, p. 24}).

Not only are levels of science knowledge and understanding low, but they
are only weakly correlated with trust in science. In a seminal
meta-analysis Allum et al.
(\citeproc{ref-allumScienceKnowledgeAttitudes2008}{2008}) found that
Oxford scale type science knowledge was only weakly associated with
attitudes towards science. More recently, Cologna et al.
(\citeproc{ref-colognaTrustScientistsTheir2025}{2025}) found no
statistically significant relationship between national science literacy
scores, based on the Program for International Student Assessment
(PISA), and national average trust in scientists for the 68 countries
included in their study.

To sum up, the literature shows that trust in science is relatively
high, but that knowledge and understanding of science do not seem to be
strong determinants of this trust. Does this mean that trust in science
is irrational?

From a sociological perspective, in particular in a Bourdieusian
framework, trust in science may be strongly influenced by
\emph{habitus}---a system of dispositions shaped by one's social class
and cultural background
(\citeproc{ref-bormannTrustTrustingPractices2019}{Bormann \& Thies,
2019}; \citeproc{ref-bourdieuOutlineTheoryPractice1977}{Bourdieu,
1977}). Rather than a reasoned appraisal of science's trustworthiness,
trust might result from internalized norms. In line with this
suggestion, Archer et al.
(\citeproc{ref-archerScienceCapitalConceptual2015}{2015}) show that
school children aged 11-15 years already differ considerably in their
``science capital''--an index of several questions pertaining to how
much they value and engage with science. These differences were
associated with differences in cultural capital (e.g.~parental
university attendance), gender, and ethnicity. If, for undetermined
sociological reasons, science acquires sufficient prestige among some
segments of the population, it could lead some people to look up to
science and trust it.

While sociological factors play a role in shaping people's attitudes
towards science, we presently introduce a model in which it might be
rational for people to trust science, even if they have little current
knowledge of it.

\section{The rational impression account of trust in
science}\label{the-rational-impression-account-of-trust-in-science}

In the rational impression account of trust in science, people trust
science because they have been impressed by it. This trust persists even
after the specific contents that gave rise to it have been forgotten.
The account builds on three basic cognitive mechanisms: First, we infer
competence from possessing rare knowledge: if someone states something
that is difficult to know, and we believe that they are right, we are
impressed, and deem that individual competent. Second, in many
situations, we infer accuracy from consensus: if something is highly
consensual, it is likely to be true. Third, impressions can persist
without recall of what generated them: while learning about science can
create lasting impressions, we are likely to forget about specific
science knowledge.

\subsection{People infer competence from rare
knowledge}\label{people-infer-competence-from-rare-knowledge}

Estimating other people's competence from communicated information is an
essential skill in a variety of social contexts, including communication
(\citeproc{ref-sperberEpistemicVigilance2010}{Sperber et al., 2010}),
cooperation (\citeproc{ref-cuddyBIASMapBehaviors2007}{Cuddy et al.,
2007}), and social learning
(\citeproc{ref-lalandSocialLearningStrategies}{Laland, n.d.}).

Humans use a variety of cues to estimate others' competence: from
superficial, generally unreliable cues such as facial features
(\citeproc{ref-todorovUnderstandingEvaluationFaces2008}{Todorov et al.,
2008}), to more reliable ones, such as providing good explanations
(\citeproc{ref-reimerUseHeuristicsPersuasion2004}{Reimer et al., 2004};
for children, see, e.g.,
\citeproc{ref-castelainEvidenceThatTwoYearOld2018a}{Castelain et al.,
2018}) or having made accurate predictions in the past
(\citeproc{ref-mellersIdentifyingCultivatingSuperforecasters2015}{Mellers
et al., 2015}; for children, see, e.g.,
\citeproc{ref-liuSelectiveTrustChildrens2013}{Liu et al., 2013}).

One reliable cue to competence is possessing specific pieces of
knowledge: people see others who share valuable ideas as more competent
(\citeproc{ref-altayItMyIdea2020}{Altay et al., 2020}). With trivia
questions, it has been shown that people have accurate perceptions of
whether something is hard to know or not, and that they use this
information to infer someone's competence
(\citeproc{ref-dubourgUsingNestedStructure2025}{Dubourg et al., 2025}):
knowing a rare piece of information indicates a high likelihood of
knowing more information in the same domain. In the case of science,
Pfänder, De Rouilhan, et al.
(\citeproc{ref-pfanderTrustingForgettingImpressive2025}{2025}) showed
that participants perceive some scientific findings as more impressive
than others. Reading about the more impressive scientific findings
increased participants' perceptions of both the scientists' competence
and the trustworthiness of their discipline.

For an information to be impressive, at least two criteria should be
met: (i) it is perceived as rare or hard to uncover, (ii) it is believed
to be true. Past research has shown that people have very accurate
intuitions regarding (i) (see, in particular,
\citeproc{ref-dubourgPeopleUseNested2024}{Dubourg et al., n.d.}), but
little is known about which features of an information exactly trigger
this intuition. For example, most people would probably only be mildly
impressed by someone telling them that a given tree has exactly 110201
leaves. Even though obtaining this information implies an exhausting
counting effort, everyone in principle knows how to do it. By contrast,
finding out that it takes light
\href{https://imagine.gsfc.nasa.gov/features/cosmic/milkyway_info.html}{approximately
100,000 years to travel from one end of the Milky Way to the other} is
probably impressive to most people, as they would not know how such a
distance can be measured. From this view, most of scientific knowledge
is likely to be deemed very impressive (and a survey in France showed
that people tended to trust more science if they deemed it more precise,
which is one way of being impressive,
\citeproc{ref-pfanderFrenchTrustMore2025}{Pfänder \& Mercier, 2025}).
Less obvious is how people infer (ii), i.e., that the information is
true, since, as a rule, people cannot evaluate for themselves scientific
discoveries. Below, we describe how perceived consensus might allow
people to infer that a piece of information is true, even if they do not
understand how it was acquired.

\subsection{People infer accuracy from
consensus}\label{people-infer-accuracy-from-consensus}

In order to make the best of communicated information, individuals need
to be able to evaluate it, i.e.~being able to distinguish inaccurate and
harmful from accurate and beneficial information
(\citeproc{ref-maynard-smithAnimalSignals2003}{Maynard-Smith \& Harper,
2003}). It has been argued that humans have evolved a suite of cognitive
mechanisms to serve this function, mechanisms which evaluate both the
source of a piece of information and its content
(\citeproc{ref-mercierNotBornYesterday2020}{Mercier, 2020};
\citeproc{ref-sperberEpistemicVigilance2010}{Sperber et al., 2010}).

In the case of science, if we do not already assume that people trust
scientists, it seems that we are left only with content. However,
scientific findings tend to violate our intuitions
(\citeproc{ref-cromerUncommonSenseHeretical1995}{Cromer, 1995};
\citeproc{ref-mccauleyWhyReligionNatural2011a}{McCauley, 2011};
\citeproc{ref-shtulmanScienceblindWhyOur2017}{Shtulman, 2017};
\citeproc{ref-wolpertUnnaturalNatureScience1994}{Wolpert, 1994}), and so
not to be intuitively plausible. In some contexts, people might be able
to judge the accuracy of scientific findings for themselves, for example
when they are exposed to accessible and convincing explanations in
school (\citeproc{ref-lombrozoSimplicityProbabilityCausal2007}{Lombrozo,
2007}; \citeproc{ref-readExplanatoryCoherenceSocial1993}{Read \&
Marcus-Newhall, 1993}; for a review, see
\citeproc{ref-lombrozoStructureFunctionExplanations2006}{Lombrozo,
2006}). But for most scientific research, people cannot possibly
evaluate the quality of the arguments and evidence for themselves, let
alone make their own observations (e.g.~few people understand complex
analysis or group symmetry, and even fewer have access to a particle
accelerator).

It is possible, however, to rationally believe that a piece of
information is true even if it is not intuitively plausible, and if we
don't already trust its source: if enough people agree with it. The
potential of the wisdom of crowds to lead to accurate answer has been
known for centuries
(\citeproc{ref-condorcetEssaiLapplicationLanalyse1785}{Condorcet, 1785};
\citeproc{ref-hastieRobustBeautyMajority2005}{Hastie \& Kameda, 2005})
and, on the whole, people make sound use of this heuristic, being more
likely to accept a piece of information when it is supported by a larger
majority (in relative and absolute terms, for review, see
\citeproc{ref-mercierMajorityRulesHow2019}{Mercier \& Morin, 2019}). The
wisdom of crowds literature, however, assumes that informants--the
individuals providing answers--need to be at least minimally competent
(i.e.~better than chance,
\citeproc{ref-condorcetEssaiLapplicationLanalyse1785}{Condorcet, 1785}).
This is a problem for the rational impression account, as it ultimately
seeks to explain how people come to judge scientists as competent, and
therefore cannot take it for granted that the informants are deemed
competent. However, recently, Pfänder, De Courson, et al.
(\citeproc{ref-pfanderHowWiseCrowd2025}{2025}) have shown that it is
enough to assume that informants are not all biased in exactly the same
way to make justified inferences from their agreement to not only the
accuracy of their answers, but also their competence. Participants who
had no prior beliefs about an answer's plausibility, or the competence
of those who provided it, deemed more convergent answers more plausible,
and those who made them more competent. This was true in abstract
scenarios (\citeproc{ref-pfanderHowWiseCrowd2025}{Pfänder, De Courson,
et al., 2025}), but other research suggests that these inferences are
justified across a wide range of real-world decision making scenarios
(\citeproc{ref-kurversHowDetectHighperforming2019}{Kurvers et al.,
2019}).

To the extent that people perceive a scientific finding as being largely
consensual within the research community, they should thus infer not
only that it is more likely to be correct, but also that the scientists
responsible for the finding are competent. Much evidence shows that, as
a rule, when people are told about the scientific consensus on a given
issue, they change their minds in the direction of the consensus
(\citeproc{ref-vanderlindenGatewayBeliefModel2021}{Linden, 2021};
\citeproc{ref-vanstekelenburgScientificConsensusCommunicationContested2022}{Van
Stekelenburg et al., 2022};
\citeproc{ref-veckalov27countryTestCommunicating2024}{Većkalov et al.,
2024}). Note, however, that participants start these experiments with a
fair degree of trust in science, so that they can rely on that to infer
that the scientists forming the consensus are competent, rather than
inferring their competence from the fact that they agree. Even if people
aren't explicitly told that a scientific consensus exists, they likely
assume that it is the case, at least for issues taken to be settled
science, such as those they are exposed to at school--and they would be
broadly right as the ability to reach a working consensus is a defining
trait of science
(\citeproc{ref-collinsSociologyPhilosophiesGlobal2002}{Collins, 2002}).
In line with this suggestion, people (in France) trust scientists more
when they work in disciplines that people perceive as more consensual
(\citeproc{ref-pfanderFrenchTrustMore2025}{Pfänder \& Mercier, 2025}).

So far, we have argued that (i) people are impressed by information that
is difficult to acquire, if they believe it is true and, (ii) that they
can come to believe it is true if they take it to be consensual. Applied
to science, the prediction is that the more people are exposed to
impressive science taken to be consensual, the more they perceive
scientists as competent and, as a result, trust science more. This might
appear similar to the deficit model, in that both models predict
exposure to science to lead to more science knowledge. However, as
pointed out above, the correlation between science knowledge and
attitudes towards science, if it is positive, is weak
(\citeproc{ref-allumScienceKnowledgeAttitudes2008}{Allum et al., 2008}).
To explain this and, more generally, the low levels of knowledge of
science by comparison with trust in science, we argue that people likely
forget most specific science content they had been exposed to, while an
impression of trustworthiness persists.

\subsection{People forget specific knowledge while impressions
persist}\label{people-forget-specific-knowledge-while-impressions-persist}

We commonly form impressions of the people around us while forgetting
the details of how we formed these impressions: If a colleague fixes our
computer, we might forget exactly how they fixed it, yet remember that
they are good at fixing computers. Similarly, people might forget the
specific content of science knowledge they have been exposed to, but
retain an impression of scientists' trustworthiness. Several research
strings suggest that abstract impressions can persist, while recall of
specific information fades.

Memory research suggests that implicit memory is more stable than
explicit memory
(\citeproc{ref-parkinDifferentialNatureImplicit1990}{Parkin et al.,
1990}; \citeproc{ref-slomanForgettingPrimedFragment1988}{Sloman et al.,
1988}). It has also been argued that memory encodes information both as
``verbatim'' details--exact words or numbers--information and ``gist''
representations--the essence or bottom-line meaning
(\citeproc{ref-reynaScientificTheoryGist2021}{Reyna, 2021}), and that
the verbatim memory tends to fade faster
(\citeproc{ref-murphyForgettingVerbatimInformation1994}{Murphy \&
Shapiro, 1994}).

More extreme examples supporting the idea that impressions can be
detached from the memory of specific events come from medical research:
patients with severe amnesia, for instance, can continue to experience
emotions linked to events they could not recall
(\citeproc{ref-feinsteinSustainedExperienceEmotion2010}{Feinstein et
al., 2010}). Other research has shown that patients with profound
episodic memory impairment due to dementia continue to show capacity for
emotional learning
(\citeproc{ref-evans-robertsRememberingRelationshipsPreserved2010}{Evans-Roberts
\& Turnbull, 2010}).

Some research in the context of science suggests that processes of
impression formation and knowledge retention can be quite detached: in
an experiment, participants found some science-related explanations more
satisfying than others, but this did not predict how well they could
recall the explanations shortly after
(\citeproc{ref-liquinMotivatedLearnAccount2022}{Liquin \& Lombrozo,
2022}). In the study mentioned above Pfänder, De Rouilhan, et al.
(\citeproc{ref-pfanderTrustingForgettingImpressive2025}{2025}), showing
that being exposed to impressive scientific content led to higher trust
in the relevant scientific discipline, another experiment showed that
participants immediately forgot most of the information which had
impressed them.

\subsection{Additional predictions of the rational impressions
account}\label{additional-predictions-of-the-rational-impressions-account}

The rational impression account of trust in science rests on three
already established cognitive mechanisms: (i) people deem competent
those who possess impressive knowledge; (ii) people deem opinions others
converge on true; (iii) people tend to forget how impressions are
formed, while the impressions are maintained. This account explains why
people trust science: when scientists agree on impressive findings,
people deem that to be a good cue that the scientists are right, and
that they are competent. The account also explains why people trust
science despite not understanding or knowing much of it: first, they
don't need to understand science to deem it true and to be impressed by
it, second, the impression of competence and trust can persist even if
they don't remember the scientific knowledge that gave rise to these
impressions. The argument we have made above suggests that people who
have been exposed to scientific content have good grounds for deeming
scientists competent. This requires that they believe the scientists
aren't conspiring to form a false consensus--but, in basic science, such
aspersions aren't very plausible (why would scientists conspire to make
us believe the Milky Way has such and such size?). However, the model
doesn't require that scientists be perceived as particularly benevolent.
Scientists' benevolence should be difficult for people to evaluate, as
few personally know any scientists, few even know \emph{of} any
individual living scientists
(\citeproc{ref-researchamericaMostAmericansCannot2021}{Research!America,
2021}). As a result, there are no obvious reasons why people should rate
scientists particularly high on benevolence or related dimensions. In
line with this prediction, it has been shown that people perceive
scientists as very competent, but not as particularly warm
(\citeproc{ref-fiskeGainingTrustWell2014}{Fiske \& Dupree, 2014}). A
recent Pew survey found that 89\% of Americans viewed research
scientists as intelligent, but only 65\% viewed them as honest
(\citeproc{ref-kennedyPublicTrustScientists2024}{Kennedy \& Brian,
2024}). Beyond the US, a recent study confirmed this tendency on a
global scale (\citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et
al., 2025}): People perceived scientists as highly competent, with 78\%
tending to believe that most scientists are qualified to conduct
high-impact research. By contrast, people held scientists in lower
esteem with regards to their integrity and benevolence: Only 57\% of
people tended to believe that most scientists are honest, and only 56\%
tended to believe that most scientists are concerned about people's
well-being.

A second prediction of the rational impression account is that
education, and more specifically science education, should be the main
predictor of trust in science. Since most people consume very little
news (\citeproc{ref-newmanDigitalNewsReport2023}{Newman et al., 2023}),
and even less scientific news
(\citeproc{ref-funkScienceNewsInformation2017}{Funk et al., 2017}), the
bulk of exposure to science can be assumed to happen during education.
Education, and in particular science education, has been consistently
identified as one of the strongest predictors of trust in science
(\citeproc{ref-noyScienceGoodEffects2019}{Noy \& O'Brien, 2019};
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018},
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{2020}; but
see \citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et al., 2025}
who only find a small positive relationship between tertiary education
and trust in science). This is compatible with the fact that people,
even those who received a science education, do not know much about
science: if we assume that education has some causal effect on trust in
science, this effect does not need to be driven by a pure transmission
of knowledge and understanding (for a similar argument, see
\citeproc{ref-bakEducationPublicAttitudes2001}{Bak, 2001}). The
candidate mechanism proposed by the rational impression account is
exposure to impressive scientific content. Students might not understand
all of it, and potentially recall even less later on; but they might
have been impressed by it, to the point that they come to perceive
scientists as competent, and thus as trustworthy, at least in that
respect. This impression might persist even when specific knowledge
vanishes. In line with this, Motta
(\citeproc{ref-mottaEnduringEffectScientific2018}{2018}) found that, in
the US, the more children were interested in science at age 12--14
years, the more they tended to trust in climate scientists in adulthood
(mid thirties), irrespective of their political ideology.

A third prediction is that people with a basic science education should
have had ample opportunities to form impressions of trustworthiness of
science, which should have built a solid baseline of trust in science.
These people should have had the opportunity to form impressions of
trustworthiness of science. This should have built a solid baseline of
trust in science. People might deviate from this default and distrust
science on certain specific science topics for various reasons, but they
should trust most of science. This is in line with the finding that in
the US, almost everyone--even people who say they don't trust science in
general or who hold specific beliefs blatantly violating scientific
knowledge (e.g.~that the earth is flat)--trusts almost all of basic
science knowledge (e.g.~that electrons are smaller than atoms)
(\citeproc{ref-pfanderQuasiuniversalAcceptanceBasic2025}{Pfänder,
Kerzreho, et al., 2025}).

\subsection{Counterarguments}\label{counterarguments}

There are several theoretical and empirical counterarguments that can be
made against the rational impression account. Here, we will try to
discuss some of them.

How is the rational impression account ``rational'', if it posits that
trust is largely detached from recalling specific knowledge? It is
rational in that the cognitive mechanisms it builds on lead to sound
inferences in many contexts. If someone discovers something that is hard
to know, such as the size of the Milky Way, and there appears to be a
consensus, we should expect them to be competent, even without knowing
the details of how they made this discovery. Even forgetting specific
knowledge is not irrational: It has been argued that one of the main
functions of episodic memory is to justify our beliefs in communication
with others (\citeproc{ref-mahrWhyWeRemember2018}{Mahr \& Csibra,
2018}). As a result, we should be particularly good at remembering
things we might need to convince others of. In this regard, incentives
of remembering science seem to be weak: Most exposure to science happens
at school, and there is little reason for young learners' minds to
anticipate having to convince others of the merits of specific
scientific findings, which are typically of little practical relevance
to them, and which appear to be consensually accepted.

How do people come to a representation of science as consensual? In
practice--with perhaps some exceptions, such as during the Covid-19
pandemic--most people do not literally compare the opinions of different
scientists for themselves and come to the conclusion that something is
largely consensual. Where, then, could the representation of consensus
possibly emerge? A plausible explanation, we believe, is that education
fosters a representation of consensus: During education, in particular
during early education, knowledge is typically presented as simply the
result of science--a seemingly unanimous enterprise that produces
knowledge. School books hardly teach about historical science
controversies, suggest uncertainty around scientific findings, or cover
cutting-edge research where disagreements are the norm. This could
induce a default consensus assumption in people's perceptions of
science. However, this argument is of course only speculative.

If the account relies on deference to consensus, why should it be
specific to science? Inferences from consensus to accuracy should happen
in all sorts of context--in fact, the experimental evidence has been
obtained in settings unrelated to science
(\citeproc{ref-pfanderHowWiseCrowd2025}{Pfänder, De Courson, et al.,
2025}). This inference is not necessarily always sound: There are
historic examples where there has been broad agreement on misbeliefs,
such as when Christian theologians had calculated that the Earth was
approximately six thousand years old. If people were aware of this broad
agreement, and believed the theologians to have reached it independently
of each other, this might have led them to believe their estimate to be
accurate, and the theologians to be competent. However, maintaining a
consensus around beliefs is hard, and, compared to other institutions,
science is exceptionally good at producing consensus
(\citeproc{ref-collinsSociologyPhilosophiesGlobal2002}{Collins, 2002}).

Why do some people trust science even if they haven't been to school?
Even people with no education, and thus presumably very little exposure
to science, have some trust in science
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018}). A possible explanation for this could be two
step effects: people with little or no education trust some people who
have an education (step one) and those people who have an education
trust science because of mechanisms described in the rational impression
account (step two).

Conversely, why do some people distrust science even though they have
been to school? In the global north, where essentially everyone has been
exposed to science through a basic science education, some people do not
trust some aspects of science, or say they don't trust science in
general (even if that is not really true, see
\citeproc{ref-pfanderQuasiuniversalAcceptanceBasic2025}{Pfänder,
Kerzreho, et al., 2025}). In these cases, trust in science is likely to
be a default state, but other cognitive mechanisms can lead people to
deviate from this default. Suggestions on such mechanisms have already
been made for specific issues such as vaccination
(\citeproc{ref-mitonCognitiveObstaclesProVaccination2015}{Miton \&
Mercier, 2015}), GMOs
(\citeproc{ref-blanckeFatalAttractionIntuitive2015}{Blancke et al.,
2015}), or nuclear energy
(\citeproc{ref-hacquinDisgustSensitivityPublic2021}{Hacquin et al.,
2021}), but more research is needed to better understand what motivates
distrust in science (\citeproc{ref-hornseyWhyFactsAre2020}{Hornsey,
2020}).

\section{Discussion}\label{discussion}

It has long been a puzzle to the deficit model--which suggests that
trust in science is primarily driven by science knowledge--that
knowledge of science is at best weakly associated with science attitudes
(\citeproc{ref-allumScienceKnowledgeAttitudes2008}{Allum et al., 2008};
\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016}). The rational
impression account can make sense of this: it lays out how trusting
science without recalling specific knowledge can be the result of a
sound inference process, rooted in basic cognitive mechanisms of
information evaluation.

The account is compatible with the finding that education, and in
particular science education, has been repeatedly identified as one of
the strongest correlates of trust in science
(\citeproc{ref-bakEducationPublicAttitudes2001}{Bak, 2001};
\citeproc{ref-noyScienceGoodEffects2019}{Noy \& O'Brien, 2019};
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018},
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{2020}; but
see \citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et al.,
2025}). By contrast with the deficit model, it suggests that the main
causal role of education for public trust in science is not transmission
of knowledge and understanding, but impression generation.

The rational impression account aligns with recent normative accounts of
what makes science trustworthy. Instead of particular institutional
features-certain methods, norms, or processes--these accounts argue that
the trustworthiness of science lies in its diversity: Cartwright et al.
(\citeproc{ref-cartwrightTangleScienceReliability2022}{2022}) make the
case that scientific knowledge emerges from a ``tangle'' of results,
relying on diverse research methods. Oreskes
(\citeproc{ref-oreskesWhyTrustScience2019}{2019}) makes a similar case:
She argues that scientific practice takes place in different scientific
communities who rely on a variety of different research methods. Through
some shared practices, in particular peer-review, these communities
engage in critical dialogue. What makes scientific knowledge
trustworthy, according to Oreskes, is when from this diversity of actors
and methods, a consensus emerges. According to this view, to infer
trustworthiness, people should have a representation of the diversity of
science. The rational impression account is, in a way, less strict: it
does not require a representation of diversity. It does require,
however, that people have a representation of science as an institution
of independent thinkers.

The rational impressions account faces two main theoretical limitations.
First, it proposes a possible micro-level model of trust in science and
should be seen as complementing, not competing with, macro-level
processes that shape public trust in science. The rational impression
account fits with a sociological literature investigating how
``individual cognition and practice establish and maintain institutional
fields and status hierarchies, especially in the face of imperfect
knowledge''
(\citeproc{ref-gauchatCulturalCognitiveMappingScientific2018}{Gauchat \&
Andrews, 2018, p. 569}). However, sociological macro-level accounts have
described how trust in science is entangled with broader cultural and
political dynamics. These accounts, like the individual-level accounts
reviewed above, tend to focus on explaining distrust in science. For
example, Gauchat
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{2011}) describes the
`alienation model', according to which the ``public disassociation with
science is a symptom of a general disenchantment with late modernity,
mainly, the limitations associated with codified expertise, rational
bureaucracy, and institutional authority''
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011, p.
2}). This explanation builds on the work of social theorists
(\citeproc{ref-beckRiskSocietyNew1992}{Beck, 1992};
\citeproc{ref-giddensModernitySelfidentitySelf1991}{Giddens, 1991};
\citeproc{ref-habermasJurgenHabermasSociety1989}{Habermas, 1989}; see
\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011} for an
overview) who suggested that a modern, complex world increasingly
requires expertise, and thus shapes institutions of knowledge elites.
People who are not part of these institutions experience a lack of
agency, resulting in a feeling of alienation. Similarly, Gauchat
(\citeproc{ref-gauchatLegitimacyScience2023}{2023}) argues that
politicization of science in the US needs to be seen in its broader
cultural context. Precisely, according to Gauchat, science has enabled
the authority of the modern regulatory state. Consequently, conservative
distrust of science reflects deeper structural tensions with the
institutions and rational--legal authority of modern governance. At the
micro-level, this is consistent with research showing that right-wing
authoritarian ideology is associated with distrust towards science and
scientists (\citeproc{ref-kerrRightwingAuthoritarianismSocial2021}{Kerr
\& Wilson, 2021}).

Second, it is rational impressions account is a model for explaining
trust, not for explaining distrust. In the context of trust in political
institutions, it has been argued that trust and distrust are not
necessarily symmetrical: what causes the former might not help alleviate
the latter
(\citeproc{ref-bertsouRethinkingPoliticalDistrust2019}{Bertsou, 2019}).
We believe this is, at least to some degree, true for science, too. As
we have argued above, trust in science is likely the default state for
people who have received a basic science education, but several other
cognitive mechanisms can explain why people would deviate from that
default. This distinction between trust and distrust matters also for
normative judgments about distrust in science: Just because we propose
an account by which trust in science can be the result of a rational
cognitive process, this does not imply that, conversely, all distrust in
science is irrational. Some groups of people do in fact have good
reasons not to trust science. For example, some science has historically
contributed to fostering racism (see e.g.
\citeproc{ref-fuentesSystemicRacismScience2023}{Fuentes, 2023};
\citeproc{ref-noblesScienceMustOvercome2022}{Nobles et al., 2022}), via
instances such as the tragically famous Tuskegee syphilis study
(\citeproc{ref-brandtRacismResearchCase1978}{Brandt, 1978};
\citeproc{ref-scharffMoreTuskegeeUnderstanding2010}{Scharff et al.,
2010}).

As a model of explaining existing trust in science, the rational
impression account is limited in its practical implications for science
communication and education. First, we do not think that science
communication should stress consensus at all costs. In the rational
impression account, consensus plays a central role for generating trust.
However, this should not incentivize science communicators to neglect
transparency about uncertainty. Acknowledging uncertainty in science
communication has been argued to be crucial for fostering long term
trust in science
(\citeproc{ref-druckmanCommunicatingPolicyRelevantScience2015}{Druckman,
2015}). For example, in the context of Covid-19 vaccines, Petersen et
al. (\citeproc{ref-petersenTransparentCommunicationNegative2021}{2021})
have shown that communicating uncertainty is crucial for building long
term trust in health authorities.

Second, science communication should not aim for impressiveness at all
costs either. Research has shown that intellectual humility can increase
trust in scientists
(\citeproc{ref-koetkeEffectSeeingScientists2024}{Koetke et al., 2024}).
Trying to oversell scientific results might therefore backfire. People
appear to value transparency via open data practices in science
(\citeproc{ref-songTrustingShouldersOpen2022}{Song et al., 2022}), and
trust science that replicates more
(\citeproc{ref-hendriksReplicationCrisisTrust2020}{Hendriks et al.,
2020}). We should therefore expect that simply doing better, more
transparent science, and being humble about it, is likely to be the most
effective strategy to impress the public and elicit perceptions of
trustworthiness.

Third, educators should not stop aiming at fostering a proper
understanding of science. Most students might not understand all of the
content, or recall much specific knowledge later on. However, for some
students at least, some of that knowledge will be remembered, and will
prove important in their lives. Second, to be impressive, a piece of
information does not need to be confusingly complex. In fact, a proper
understanding of research findings and their methods might even help in
appreciating their complexity--even if, once again, that understanding
is forgotten later.

Despite these limitations, we believe that the rational impressions
account offers optimism for studies of science-society interfaces, and
the field of science communication in particular: Exposure to science,
especially one that leaves an impression, might be the foundation of
public trust in science. This means that effective science communication
is essential for fostering trust in science. Low scientific literacy
levels should not discourage education and communication efforts, as
they are not necessarily a good indicator of the value added in terms of
fostering trust in science.

Taking a broader perspective, our account fits into a picture of humans
as not gullible (\citeproc{ref-mercierHowGullibleAre2017}{Mercier,
2017}, \citeproc{ref-mercierNotBornYesterday2020}{2020}). The
``failure'' of the deficit model--the fact that science knowledge
appears to not be strongly associated with trust in science--might
suggest that public trust in science is, to a large extent, irrational.
The notion that trust in science is irrational or easily granted may
amplify concerns about the impact of misinformation: if trust lacks a
solid, rational foundation, then we would expect misinformation to
easily lead people astray. There is much work to be done still to
understand how misinformation impacts people's beliefs, and in
particular elite-driven misinformation and more subtle forms of
misinformation, such as one-sided reporting. But it has been shown that
people are generally able to distinguish between true and false news
and, if anything, tend to be generally skeptical of news
(\citeproc{ref-pfanderSpottingFalseNews2025}{Pfänder \& Altay, 2025}).
As a consequence, for a better informed public, fighting for (true)
information seems at least as relevant as fighting against
misinformation (\citeproc{ref-acerbiResearchNoteFighting2022}{Acerbi et
al., 2022}). Misinformation researchers increasingly acknowledge this: A
recent report on science misinformation by the National Science
Foundation
(\citeproc{ref-nationalacademiesofsciencesUnderstandingAddressingMisinformation2024}{National
Academies of Sciences, 2024}) dedicates considerable space on developing
strategies to produce better information, for example by promoting
high-quality science, health, and medical journalism.

The rational impression account stresses the role of fighting for
information, when it comes to fostering trust in science. Well-placed
trust in science does not require profound understanding or recall of
specific knowledge; but it does require exposure to good science.

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-acerbiResearchNoteFighting2022}
Acerbi, A., Altay, S., \& Mercier, H. (2022). Research note: Fighting
misinformation or fighting for information? \emph{Harvard Kennedy School
Misinformation Review}. \url{https://doi.org/10.37016/mr-2020-87}

\bibitem[\citeproctext]{ref-alganTrustScientistsTimes2021}
Algan, Y., Cohen, D., Davoine, E., Foucault, M., \& Stantcheva, S.
(2021). Trust in scientists in times of pandemic: Panel evidence from 12
countries. \emph{Proceedings of the National Academy of Sciences},
\emph{118}(40), e2108576118.
\url{https://doi.org/10.1073/pnas.2108576118}

\bibitem[\citeproctext]{ref-allumScienceKnowledgeAttitudes2008}
Allum, N., Sturgis, P., Tabourazi, D., \& Brunton-Smith, I. (2008).
Science knowledge and attitudes across cultures: a meta-analysis.
\emph{Public Understanding of Science}, \emph{17}(1), 35--54.
\url{https://doi.org/10.1177/0963662506070159}

\bibitem[\citeproctext]{ref-altayItMyIdea2020}
Altay, S., Majima, Y., \& Mercier, H. (2020). It's my idea! Reputation
management and idea appropriation. \emph{Evolution and Human Behavior},
\emph{41}(3), 235--243.
\url{https://doi.org/10.1016/j.evolhumbehav.2020.03.004}

\bibitem[\citeproctext]{ref-archerScienceCapitalConceptual2015}
Archer, L., Dawson, E., DeWitt, J., Seakins, A., \& Wong, B. (2015).
{``}Science capital{''}: A conceptual, methodological, and empirical
argument for extending bourdieusian notions of capital beyond the arts.
\emph{Journal of Research in Science Teaching}, \emph{52}(7), 922--948.
\url{https://doi.org/10.1002/tea.21227}

\bibitem[\citeproctext]{ref-bakEducationPublicAttitudes2001}
Bak, H.-J. (2001). Education and Public Attitudes toward Science:
Implications for the {``}Deficit Model{''} of Education and Support for
Science and Technology. \emph{Social Science Quarterly}, \emph{82}(4),
779--795. \url{https://doi.org/10.1111/0038-4941.00059}

\bibitem[\citeproctext]{ref-bauerWhatCanWe2007}
Bauer, M. W., Allum, N., \& Miller, S. (2007). What can we learn from 25
years of PUS survey research? Liberating and expanding the agenda.
\emph{Public Understanding of Science}, \emph{16}(1), 79--95.
\url{https://doi.org/10.1177/0963662506071287}

\bibitem[\citeproctext]{ref-beckRiskSocietyNew1992}
Beck, U. (1992). \emph{Risk society: towards a new modernity} (Repr).
Sage.

\bibitem[\citeproctext]{ref-bertsouRethinkingPoliticalDistrust2019}
Bertsou, E. (2019). Rethinking political distrust. \emph{European
Political Science Review}, \emph{11}(2), 213--230.
\url{https://doi.org/10.1017/S1755773919000080}

\bibitem[\citeproctext]{ref-besleyReassessingVariablesUsed2021a}
Besley, J. C., Lee, N. M., \& Pressgrove, G. (2021). Reassessing the
Variables Used to Measure Public Perceptions of Scientists.
\emph{Science Communication}, \emph{43}(1), 3--32.
\url{https://doi.org/10.1177/1075547020949547}

\bibitem[\citeproctext]{ref-blanckeFatalAttractionIntuitive2015}
Blancke, S., Van Breusegem, F., De Jaeger, G., Braeckman, J., \& Van
Montagu, M. (2015). Fatal attraction: the intuitive appeal of GMO
opposition. \emph{Trends in Plant Science}, \emph{20}(7), 414--418.
\url{https://doi.org/10.1016/j.tplants.2015.03.011}

\bibitem[\citeproctext]{ref-bogertEffectTrustScience2024}
Bogert, J. M., Buczny, Harvey, \& Ellers, J. and. (2024). The effect of
trust in science and media use on public belief in anthropogenic climate
change: A meta-analysis. \emph{Environmental Communication},
\emph{18}(4), 484--509.
\url{https://doi.org/10.1080/17524032.2023.2280749}

\bibitem[\citeproctext]{ref-bormannTrustTrustingPractices2019}
Bormann, I., \& Thies, B. (2019). Trust and trusting practices during
transition to higher education: Introducing a framework of habitual
trust. \emph{Educational Research}, \emph{61}(2), 161--180.
\url{https://doi.org/10.1080/00131881.2019.1596036}

\bibitem[\citeproctext]{ref-bourdieuOutlineTheoryPractice1977}
Bourdieu, P. (1977). \emph{Outline of a theory of practice} (1st ed.).
Cambridge University Press.
\url{https://doi.org/10.1017/CBO9780511812507}

\bibitem[\citeproctext]{ref-brandtRacismResearchCase1978}
Brandt, A. M. (1978). Racism and Research: The Case of the Tuskegee
Syphilis Study. \emph{The Hastings Center Report}, \emph{8}(6), 21.
\url{https://doi.org/10.2307/3561468}

\bibitem[\citeproctext]{ref-cartwrightTangleScienceReliability2022}
Cartwright, N., Hardie, J., Montuschi, E., Soleiman, M., \& Thresher, A.
C. (2022). \emph{The tangle of science: Reliability beyond method,
rigour, and objectivity}. Oxford University Press.

\bibitem[\citeproctext]{ref-castelainEvidenceThatTwoYearOld2018a}
Castelain, T., Bernard, S., \& Mercier, H. (2018). Evidence that
Two{-}Year{-}Old Children are Sensitive to Information Presented in
Arguments. \emph{Infancy}, \emph{23}(1), 124--135.
\url{https://doi.org/10.1111/infa.12202}

\bibitem[\citeproctext]{ref-collinsSociologyPhilosophiesGlobal2002}
Collins, R. (2002). \emph{The sociology of philosophies: a global theory
of intellectual change} (4. print., 1. Harvard Univ. Pr. paperback ed.,
2000). Belknap Press of Harvard Univ. Press.

\bibitem[\citeproctext]{ref-colognaTrustScientistsTheir2025}
Cologna, V., Mede, N. G., Berger, S., Besley, J., Brick, C., Joubert,
M., Maibach, E. W., Mihelj, S., Oreskes, N., Schäfer, M. S., Linden, S.
van der, Abdul Aziz, N. I., Abdulsalam, S., Shamsi, N. A., Aczel, B.,
Adinugroho, I., Alabrese, E., Aldoh, A., Alfano, M., \ldots{} Zwaan, R.
A. (2025). Trust in scientists and their role in society across 68
countries. \emph{Nature Human Behaviour}, 1--18.
\url{https://doi.org/10.1038/s41562-024-02090-5}

\bibitem[\citeproctext]{ref-colognaRoleTrustClimate2020}
Cologna, V., \& Siegrist, M. (2020). The role of trust for climate
change mitigation and adaptation behaviour: A meta-analysis.
\emph{Journal of Environmental Psychology}, \emph{69}, 101428.
\url{https://doi.org/10.1016/j.jenvp.2020.101428}

\bibitem[\citeproctext]{ref-condorcetEssaiLapplicationLanalyse1785}
Condorcet, N. (1785). \emph{Essai sur l'application de l'analyse à la
probabilité des décisions rendues à la pluralité des voix}. Paris:
L{'}imprimerie royale.

\bibitem[\citeproctext]{ref-cromerUncommonSenseHeretical1995}
Cromer, A. H. (1995). \emph{Uncommon Sense: The Heretical Nature of
Science} (1st ed). Oxford University Press, Incorporated.

\bibitem[\citeproctext]{ref-cuddyBIASMapBehaviors2007}
Cuddy, A. J. C., Fiske, S. T., \& Glick, P. (2007). The BIAS map:
Behaviors from intergroup affect and stereotypes. \emph{Journal of
Personality and Social Psychology}, \emph{92}(4), 631--648.
\url{https://doi.org/10.1037/0022-3514.92.4.631}

\bibitem[\citeproctext]{ref-druckmanCommunicatingPolicyRelevantScience2015}
Druckman, J. N. (2015). Communicating Policy-Relevant Science. \emph{PS:
Political Science \& Politics}, \emph{48}(S1), 58--69.
\url{https://doi.org/10.1017/S1049096515000438}

\bibitem[\citeproctext]{ref-druckmanThreatsSciencePoliticization2022}
Druckman, J. N. (2022). Threats to Science: Politicization,
Misinformation, and Inequalities. \emph{The ANNALS of the American
Academy of Political and Social Science}, \emph{700}(1), 8--24.
\url{https://doi.org/10.1177/00027162221095431}

\bibitem[\citeproctext]{ref-dubourgPeopleUseNested2024}
Dubourg, E., Dheilly, T., Mercier, H., \& Morin, O. (n.d.). \emph{People
use the nested structure of knowledge to infer what others know.}

\bibitem[\citeproctext]{ref-dubourgUsingNestedStructure2025}
Dubourg, E., Dheilly, T., Mercier, H., \& Morin, O. (2025). Using the
Nested Structure of Knowledge to Infer What Others Know.
\emph{Psychological Science}, \emph{36}(6), 443--450.
\url{https://doi.org/10.1177/09567976251339633}

\bibitem[\citeproctext]{ref-durantPublicUnderstandingScience1989}
Durant, J. R., Evans, G. A., \& Thomas, G. P. (1989). The public
understanding of science. \emph{Nature}, \emph{340}(6228), 11--14.
\url{https://doi.org/10.1038/340011a0}

\bibitem[\citeproctext]{ref-evans-robertsRememberingRelationshipsPreserved2010}
Evans-Roberts, C. E. Y., \& Turnbull, O. H. (2010). Remembering
Relationships: Preserved Emotion-Based Learning in Alzheimer's Disease.
\emph{Experimental Aging Research}, \emph{37}(1), 1--16.
\url{https://doi.org/10.1080/0361073X.2011.536750}

\bibitem[\citeproctext]{ref-feinsteinSustainedExperienceEmotion2010}
Feinstein, J. S., Duff, M. C., \& Tranel, D. (2010). Sustained
experience of emotion after loss of memory in patients with amnesia.
\emph{Proceedings of the National Academy of Sciences}, \emph{107}(17),
7674--7679. \url{https://doi.org/10.1073/pnas.0914054107}

\bibitem[\citeproctext]{ref-fiskeGainingTrustWell2014}
Fiske, S. T., \& Dupree, C. (2014). Gaining trust as well as respect in
communicating to motivated audiences about science topics.
\emph{Proceedings of the National Academy of Sciences},
\emph{111}(Supplement{\_}4), 13593--13597.
\url{https://doi.org/10.1073/pnas.1317505111}

\bibitem[\citeproctext]{ref-fuentesSystemicRacismScience2023}
Fuentes, A. (2023). Systemic racism in science: Reactions matter.
\emph{Science}, \emph{381}(6655), eadj7675.
\url{https://doi.org/10.1126/science.adj7675}

\bibitem[\citeproctext]{ref-funkScienceNewsInformation2017}
Funk, C., Gottfried, J., \& Mitchell, A. (2017). \emph{Science news and
information today}.
\url{https://www.pewresearch.org/science/2017/09/20/science-news-and-information-today/}

\bibitem[\citeproctext]{ref-funkPublicConfidenceScientists2020}
Funk, C., \& Kennedy, B. (2020). \emph{Public confidence in scientists
has remained stable for decades}.
\url{https://www.pewresearch.org/short-reads/2020/08/27/public-confidence-in-scientists-has-remained-stable-for-decades/}

\bibitem[\citeproctext]{ref-funkScienceScientistsHeld2020}
Funk, C., Tyson, A., Kennedy, B., \& Johnson, C. (2020). \emph{Science
and scientists held in high esteem across global publics}.
\url{https://www.pewresearch.org/science/2020/09/29/science-and-scientists-held-in-high-esteem-across-global-publics/}

\bibitem[\citeproctext]{ref-gauchatCulturalAuthorityScience2011}
Gauchat, G. (2011). The cultural authority of science: Public trust and
acceptance of organized science. \emph{Public Understanding of Science},
\emph{20}(6), 751--770. \url{https://doi.org/10.1177/0963662510365246}

\bibitem[\citeproctext]{ref-gauchatLegitimacyScience2023}
Gauchat, G. (2023). The Legitimacy of Science. \emph{Annual Review of
Sociology}, \emph{49}(1), 263--279.
\url{https://doi.org/10.1146/annurev-soc-030320-035037}

\bibitem[\citeproctext]{ref-gauchatCulturalCognitiveMappingScientific2018}
Gauchat, G., \& Andrews, K. T. (2018). The Cultural-Cognitive Mapping of
Scientific Professions. \emph{American Sociological Review},
\emph{83}(3), 567--595. \url{https://doi.org/10.1177/0003122418773353}

\bibitem[\citeproctext]{ref-giddensModernitySelfidentitySelf1991}
Giddens, A. (1991). \emph{Modernity and self-identity: Self and society
in the late modern age}. Stanford University Press.

\bibitem[\citeproctext]{ref-habermasJurgenHabermasSociety1989}
Habermas, J. (1989). \emph{Jürgen Habermas on society and politics: a
reader} (S. Seidman, Ed.; 1. {[}print.{]}). Beacon Press.

\bibitem[\citeproctext]{ref-hacquinDisgustSensitivityPublic2021}
Hacquin, A.-S., Altay, S., Aarøe, L., \& Mercier, H. (2021). Disgust
sensitivity and public opinion on nuclear energy. \emph{Journal of
Environmental Psychology}, 101749.

\bibitem[\citeproctext]{ref-hastieRobustBeautyMajority2005}
Hastie, R., \& Kameda, T. (2005). The Robust Beauty of Majority Rules in
Group Decisions. \emph{Psychological Review}, \emph{112}(2), 494--508.
\url{https://doi.org/10.1037/0033-295X.112.2.494}

\bibitem[\citeproctext]{ref-hendriksMeasuringLaypeoplesTrust2015}
Hendriks, F., Kienhues, D., \& Bromme, R. (2015). Measuring
Laypeople{'}s Trust in Experts in a Digital Age: The Muenster Epistemic
Trustworthiness Inventory (METI). \emph{PLOS ONE}, \emph{10}(10),
e0139309. \url{https://doi.org/10.1371/journal.pone.0139309}

\bibitem[\citeproctext]{ref-hendriksReplicationCrisisTrust2020}
Hendriks, F., Kienhues, D., \& Bromme, R. (2020). Replication crisis =
trust crisis? The effect of successful vs failed replications on
laypeople{'}s trust in researchers and research. \emph{Public
Understanding of Science}, \emph{29}(3), 270--288.
\url{https://doi.org/10.1177/0963662520902383}

\bibitem[\citeproctext]{ref-hornseyWhyFactsAre2020}
Hornsey, M. J. (2020). Why Facts Are Not Enough: Understanding and
Managing the Motivated Rejection of Science. \emph{Current Directions in
Psychological Science}, \emph{29}(6), 583--591.
\url{https://doi.org/10.1177/0963721420969364}

\bibitem[\citeproctext]{ref-hornseyAttitudeRootsJiu2017a}
Hornsey, M. J., \& Fielding, K. S. (2017). Attitude roots and Jiu Jitsu
persuasion: Understanding and overcoming the motivated rejection of
science. \emph{American Psychologist}, \emph{72}(5), 459--473.
\url{https://doi.org/10.1037/a0040437}

\bibitem[\citeproctext]{ref-hornseyMetaanalysesDeterminantsOutcomes2016}
Hornsey, M. J., Harris, E. A., Bain, P. G., \& Fielding, K. S. (2016).
Meta-analyses of the determinants and outcomes of belief in climate
change. \emph{Nature Climate Change}, \emph{6}(6), 622--626.
\url{https://doi.org/10.1038/nclimate2943}

\bibitem[\citeproctext]{ref-intemannScienceCommunicationPublic2023}
Intemann, K. (2023). Science communication and public trust in science.
\emph{Interdisciplinary Science Reviews}, \emph{48}(2), 350--365.
\url{https://doi.org/10.1080/03080188.2022.2152244}

\bibitem[\citeproctext]{ref-kennedyPublicTrustScientists2024}
Kennedy, A. T., \& Brian. (2024). \emph{Public trust in scientists and
views on their role in policymaking}.
\url{https://www.pewresearch.org/science/2024/11/14/public-trust-in-scientists-and-views-on-their-role-in-policymaking/}

\bibitem[\citeproctext]{ref-kerrRightwingAuthoritarianismSocial2021}
Kerr, J. R., \& Wilson, M. S. (2021). Right-wing authoritarianism and
social dominance orientation predict rejection of science and
scientists. \emph{Group Processes \& Intergroup Relations},
\emph{24}(4), 550--567. \url{https://doi.org/10.1177/1368430221992126}

\bibitem[\citeproctext]{ref-koetkeEffectSeeingScientists2024}
Koetke, J., Schumann, K., Bowes, S. M., \& Vaupotič, N. (2024). The
effect of seeing scientists as intellectually humble on trust in
scientists and their research. \emph{Nature Human Behaviour}, 1--14.
\url{https://doi.org/10.1038/s41562-024-02060-x}

\bibitem[\citeproctext]{ref-koetkeTrustScienceIncreases2021}
Koetke, J., Schumann, K., \& Porter, T. (2021). Trust in science
increases conservative support for social distancing. \emph{Group
Processes \& Intergroup Relations}, \emph{24}(4), 680--697.
\url{https://doi.org/10.1177/1368430220985918}

\bibitem[\citeproctext]{ref-kurversHowDetectHighperforming2019}
Kurvers, R. H., Herzog, S. M., Hertwig, R., Krause, J., Moussaid, M.,
Argenziano, G., Zalaudek, I., Carney, P. A., \& Wolf, M. (2019). How to
detect high-performing individuals and groups: Decision similarity
predicts accuracy. \emph{Science Advances}, \emph{5}(11), eaaw9011.

\bibitem[\citeproctext]{ref-lalandSocialLearningStrategies}
Laland, K. N. (n.d.). \emph{Social learning strategies}.

\bibitem[\citeproctext]{ref-lewandowskyRoleConspiracistIdeation2013}
Lewandowsky, S., Gignac, G. E., \& Oberauer, K. (2013). The Role of
Conspiracist Ideation and Worldviews in Predicting Rejection of Science.
\emph{PLOS ONE}, \emph{8}(10), e75637.
\url{https://doi.org/10.1371/journal.pone.0075637}

\bibitem[\citeproctext]{ref-lewandowskyMotivatedRejectionScience2016}
Lewandowsky, S., \& Oberauer, K. (2016). Motivated Rejection of Science.
\emph{Current Directions in Psychological Science}, \emph{25}(4),
217--222. \url{https://doi.org/10.1177/0963721416654436}

\bibitem[\citeproctext]{ref-lewandowskyWorldviewmotivatedRejectionScience2021}
Lewandowsky, S., \& Oberauer, K. (2021). Worldview-motivated rejection
of science and the norms of science. \emph{Cognition}, \emph{215},
104820. \url{https://doi.org/10.1016/j.cognition.2021.104820}

\bibitem[\citeproctext]{ref-vanderlindenGatewayBeliefModel2021}
Linden, S. van der. (2021). The Gateway Belief Model (GBM): A review and
research agenda for communicating the scientific consensus on climate
change. \emph{Current Opinion in Psychology}, \emph{42}, 7--12.
\url{https://doi.org/10.1016/j.copsyc.2021.01.005}

\bibitem[\citeproctext]{ref-lindholtPublicAcceptanceCOVID192021}
Lindholt, M. F., Jørgensen, F., Bor, A., \& Petersen, M. B. (2021).
Public acceptance of COVID-19 vaccines: cross-national evidence on
levels and individual-level predictors using observational data.
\emph{BMJ Open}, \emph{11}(6), e048172.
\url{https://doi.org/10.1136/bmjopen-2020-048172}

\bibitem[\citeproctext]{ref-liquinMotivatedLearnAccount2022}
Liquin, E. G., \& Lombrozo, T. (2022). Motivated to learn: An account of
explanatory satisfaction. \emph{Cognitive Psychology}, \emph{132},
101453. \url{https://doi.org/10.1016/j.cogpsych.2021.101453}

\bibitem[\citeproctext]{ref-liuSelectiveTrustChildrens2013}
Liu, D., Vanderbilt, K. E., \& Heyman, G. D. (2013). Selective trust:
Children's use of intention and outcome of past testimony.
\emph{Developmental Psychology}, \emph{49}(3), 439--445.
\url{https://doi.org/10.1037/a0031615}

\bibitem[\citeproctext]{ref-lombrozoStructureFunctionExplanations2006}
Lombrozo, T. (2006). The structure and function of explanations.
\emph{Trends in Cognitive Sciences}, \emph{10}(10), 464--470.
\url{https://doi.org/10.1016/j.tics.2006.08.004}

\bibitem[\citeproctext]{ref-lombrozoSimplicityProbabilityCausal2007}
Lombrozo, T. (2007). Simplicity and probability in causal explanation.
\emph{Cognitive Psychology}, \emph{55}(3), 232--257.
\url{https://doi.org/10.1016/j.cogpsych.2006.09.006}

\bibitem[\citeproctext]{ref-lupiaTrendsUSPublic2024}
Lupia, A., Allison, D. B., Jamieson, K. H., Heimberg, J., Skipper, M.,
\& Wolf, S. M. (2024). Trends in US public confidence in science and
opportunities for progress. \emph{Proceedings of the National Academy of
Sciences}, \emph{121}(11), e2319488121.
\url{https://doi.org/10.1073/pnas.2319488121}

\bibitem[\citeproctext]{ref-mahrWhyWeRemember2018}
Mahr, J. B., \& Csibra, G. (2018). Why do we remember? The communicative
function of episodic memory. \emph{Behavioral and Brain Sciences},
\emph{41}, e1. \url{https://doi.org/10.1017/S0140525X17000012}

\bibitem[\citeproctext]{ref-mayerIntegrativeModelOrganizational1995a}
Mayer, R. C., Davis, J. H., \& Schoorman, F. D. (1995). An Integrative
Model of Organizational Trust. \emph{The Academy of Management Review},
\emph{20}(3), 709. \url{https://doi.org/10.2307/258792}

\bibitem[\citeproctext]{ref-maynard-smithAnimalSignals2003}
Maynard-Smith, J., \& Harper, D. (2003). \emph{Animal signals}. Oxford
University Press.

\bibitem[\citeproctext]{ref-mccauleyWhyReligionNatural2011a}
McCauley, R. N. (2011). \emph{Why religion is natural and science is
not}. Oxford University Press.

\bibitem[\citeproctext]{ref-mellersIdentifyingCultivatingSuperforecasters2015}
Mellers, B., Stone, E., Murray, T., Minster, A., Rohrbaugh, N., Bishop,
M., Chen, E., Baker, J., Hou, Y., Horowitz, M., Ungar, L., \& Tetlock,
P. (2015). Identifying and Cultivating Superforecasters as a Method of
Improving Probabilistic Predictions. \emph{Perspectives on Psychological
Science}, \emph{10}(3), 267--281.
\url{https://doi.org/10.1177/1745691615577794}

\bibitem[\citeproctext]{ref-mercierHowGullibleAre2017}
Mercier, H. (2017). How Gullible are We? A Review of the Evidence from
Psychology and Social Science. \emph{Review of General Psychology},
\emph{21}(2), 103--122. \url{https://doi.org/10.1037/gpr0000111}

\bibitem[\citeproctext]{ref-mercierNotBornYesterday2020}
Mercier, H. (2020). \emph{Not born yesterday: the science of who we
trust and what we believe}. \url{https://doi.org/10.1515/9780691198842}

\bibitem[\citeproctext]{ref-mercierMajorityRulesHow2019}
Mercier, H., \& Morin, O. (2019). Majority rules: how good are we at
aggregating convergent opinions? \emph{Evolutionary Human Sciences},
\emph{1}, e6. \url{https://doi.org/10.1017/ehs.2019.6}

\bibitem[\citeproctext]{ref-millerMeasurementCivicScientific1998a}
Miller, J. D. (1998). The measurement of civic scientific literacy.
\emph{Public Understanding of Science}, \emph{7}(3), 203--223.
\url{https://doi.org/10.1088/0963-6625/7/3/001}

\bibitem[\citeproctext]{ref-millerPublicUnderstandingAttitudes2004}
Miller, J. D. (2004). Public Understanding of, and Attitudes toward,
Scientific Research: What We Know and What We Need to Know. \emph{Public
Understanding of Science}, \emph{13}(3), 273--294.
\url{https://doi.org/10.1177/0963662504044908}

\bibitem[\citeproctext]{ref-mitonCognitiveObstaclesProVaccination2015}
Miton, H., \& Mercier, H. (2015). Cognitive Obstacles to Pro-Vaccination
Beliefs. \emph{Trends in Cognitive Sciences}, \emph{19}(11), 633--636.
\url{https://doi.org/10.1016/j.tics.2015.08.007}

\bibitem[\citeproctext]{ref-mottaEnduringEffectScientific2018}
Motta, M. (2018). The enduring effect of scientific interest on trust in
climate scientists in the United States. \emph{Nature Climate Change},
\emph{8}(6), 485--488. \url{https://doi.org/10.1038/s41558-018-0126-9}

\bibitem[\citeproctext]{ref-murphyForgettingVerbatimInformation1994}
Murphy, G. L., \& Shapiro, A. M. (1994). Forgetting of verbatim
information in discourse. \emph{Memory \& Cognition}, \emph{22}(1),
85--94. \url{https://doi.org/10.3758/BF03202764}

\bibitem[\citeproctext]{ref-nationalacademiesofsciencesUnderstandingAddressingMisinformation2024}
National Academies of Sciences, E. (2024). \emph{Understanding and
addressing misinformation about science}.

\bibitem[\citeproctext]{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}
National Academies of Sciences, Engineering, and Medicine. (2016).
\emph{Science Literacy: Concepts, Contexts, and Consequences} (C. E.
Snow \& K. A. Dibner, Eds.). National Academies Press.
\url{https://doi.org/10.17226/23595}

\bibitem[\citeproctext]{ref-nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024}
National Science Board, National Science Foundation. (2024).
\emph{Science and technology: Public perceptions, awareness, and
information sources.} \url{https://ncses.nsf.gov/pubs/nsb20244}

\bibitem[\citeproctext]{ref-newmanDigitalNewsReport2023}
Newman, N., Fletcher, R., Eddy, K., Robertson, C. T., \& Nielsen, R. K.
(2023). \emph{Digital news report 2023}.

\bibitem[\citeproctext]{ref-noblesScienceMustOvercome2022}
Nobles, M., Womack, C., Wonkam, A., \& Wathuti, E. (2022). Science must
overcome its racist legacy: Nature{'}s guest editors speak.
\emph{Nature}, \emph{606}(7913), 225--227.
\url{https://doi.org/10.1038/d41586-022-01527-z}

\bibitem[\citeproctext]{ref-noyScienceGoodEffects2019}
Noy, S., \& O'Brien, T. L. (2019). Science for good? The effects of
education and national context on perceptions of science. \emph{Public
Understanding of Science}, \emph{28}(8), 897--916.
\url{https://doi.org/10.1177/0963662519863575}

\bibitem[\citeproctext]{ref-oreskesWhyTrustScience2019}
Oreskes, N. (2019). \emph{Why trust science?} Princeton University
Press.

\bibitem[\citeproctext]{ref-pardoCognitiveDimensionPublic2004}
Pardo, R., \& Calvo, F. (2004). The Cognitive Dimension of Public
Perceptions of Science: Methodological Issues. \emph{Public
Understanding of Science}, \emph{13}(3), 203--227.
\url{https://doi.org/10.1177/0963662504045002}

\bibitem[\citeproctext]{ref-parkinDifferentialNatureImplicit1990}
Parkin, A. J., Reid, T. K., \& Russo, R. (1990). On the differential
nature of implicit and explicit memory. \emph{Memory \& Cognition},
\emph{18}(5), 507--514. \url{https://doi.org/10.3758/BF03198483}

\bibitem[\citeproctext]{ref-petersenTransparentCommunicationNegative2021}
Petersen, M. B., Bor, A., Jørgensen, F., \& Lindholt, M. F. (2021).
Transparent communication about negative features of COVID-19 vaccines
decreases acceptance but increases trust. \emph{Proceedings of the
National Academy of Sciences}, \emph{118}(29), e2024597118.
\url{https://doi.org/10.1073/pnas.2024597118}

\bibitem[\citeproctext]{ref-pfanderSpottingFalseNews2025}
Pfänder, J., \& Altay, S. (2025). Spotting false news and doubting true
news: a systematic review and meta-analysis of news judgements.
\emph{Nature Human Behaviour}, 1--12.
\url{https://doi.org/10.1038/s41562-024-02086-1}

\bibitem[\citeproctext]{ref-pfanderHowWiseCrowd2025}
Pfänder, J., De Courson, B., \& Mercier, H. (2025). How wise is the
crowd: Can we infer people are accurate and competent merely because
they agree with each other? \emph{Cognition}, \emph{255}, 106005.
\url{https://doi.org/10.1016/j.cognition.2024.106005}

\bibitem[\citeproctext]{ref-pfanderTrustingForgettingImpressive2025}
Pfänder, J., De Rouilhan, S., \& Mercier, H. (2025). \emph{Trusting but
forgetting impressive science}.
\url{https://doi.org/10.31219/osf.io/argq5_v1}

\bibitem[\citeproctext]{ref-pfanderQuasiuniversalAcceptanceBasic2025}
Pfänder, J., Kerzreho, L., \& Mercier, H. (2025). \emph{Quasi-universal
acceptance of basic science in the US}.
\url{https://doi.org/10.31219/osf.io/qc43v_v2}

\bibitem[\citeproctext]{ref-pfanderFrenchTrustMore2025}
Pfänder, J., \& Mercier, H. (2025). \emph{The french trust more the
sciences they perceive as precise and consensual}.
\url{https://doi.org/10.31219/osf.io/k9m6e_v1}

\bibitem[\citeproctext]{ref-readExplanatoryCoherenceSocial1993}
Read, S. J., \& Marcus-Newhall, A. (1993). Explanatory coherence in
social explanations: A parallel distributed processing account.
\emph{Journal of Personality and Social Psychology}, \emph{65}(3),
429--447. \url{https://doi.org/10.1037/0022-3514.65.3.429}

\bibitem[\citeproctext]{ref-reimerUseHeuristicsPersuasion2004}
Reimer, T., Mata, R., \& Stoecklin, M. (2004). The use of heuristics in
persuasion: Deriving cues on source expertise from argument quality.
\emph{Current Research in Social Psychology}, \emph{10}(6), 69--84.

\bibitem[\citeproctext]{ref-researchamericaMostAmericansCannot2021}
Research!America. (2021). \emph{Most americans cannot name a living
scientist or a research institution}.
\url{https://www.researchamerica.org/blog/survey-most-americans-cannot-name-a-living-scientist-or-a-research-institution/}

\bibitem[\citeproctext]{ref-reynaScientificTheoryGist2021}
Reyna, V. F. (2021). A scientific theory of gist communication and
misinformation resistance, with implications for health, education, and
policy. \emph{Proceedings of the National Academy of Sciences},
\emph{118}(15), e1912441117.
\url{https://doi.org/10.1073/pnas.1912441117}

\bibitem[\citeproctext]{ref-rousseauIntroductionSpecialTopic1998}
Rousseau, D. M., Sitkin, S. B., Burt, R. S., \& Camerer, C. (1998).
Introduction to Special Topic Forum: Not so Different after All: A
Cross-Discipline View of Trust. \emph{The Academy of Management Review},
\emph{23}(3), 393--404. \url{http://www.jstor.org/stable/259285}

\bibitem[\citeproctext]{ref-rutjensConspiracyBeliefsScience2022}
Rutjens, B. T., \& Većkalov, B. (2022). Conspiracy beliefs and science
rejection. \emph{Current Opinion in Psychology}, \emph{46}, 101392.
\url{https://doi.org/10.1016/j.copsyc.2022.101392}

\bibitem[\citeproctext]{ref-scharffMoreTuskegeeUnderstanding2010}
Scharff, D. P., Mathews, K. J., Jackson, P., Hoffsuemmer, J., Martin,
E., \& Edwards, D. (2010). More than Tuskegee: Understanding Mistrust
about Research Participation. \emph{Journal of Health Care for the Poor
and Underserved}, \emph{21}(3), 879--897.
\url{https://doi.org/10.1353/hpu.0.0323}

\bibitem[\citeproctext]{ref-scheufeleThirtyYearsScience2022}
Scheufele, D. A. (2022). Thirty years of science{\textendash}society
interfaces: What{'}s next? \emph{Public Understanding of Science},
\emph{31}(3), 297--304. \url{https://doi.org/10.1177/09636625221075947}

\bibitem[\citeproctext]{ref-scheufeleScienceAudiencesMisinformation2019}
Scheufele, D. A., \& Krause, N. M. (2019). Science audiences,
misinformation, and fake news. \emph{Proceedings of the National Academy
of Sciences}, \emph{116}(16), 7662--7669.
\url{https://doi.org/10.1073/pnas.1805871115}

\bibitem[\citeproctext]{ref-shtulmanScienceblindWhyOur2017}
Shtulman, A. (2017). \emph{Scienceblind: why our intuitive theories
about the world are so often wrong}. Basic Books.

\bibitem[\citeproctext]{ref-slomanForgettingPrimedFragment1988}
Sloman, S. A., Hayman, C. A., Ohta, N., Law, J., \& Tulving, E. (1988).
Forgetting in primed fragment completion. \emph{Journal of Experimental
Psychology: Learning, Memory, and Cognition}, \emph{14}(2), 223.

\bibitem[\citeproctext]{ref-smithTrendsPublicAttitudes2013}
Smith, T. W., \& Son, J. (2013). Trends in public attitudes about
confidence in institutions. \emph{General Social Survey Final Report}.

\bibitem[\citeproctext]{ref-songTrustingShouldersOpen2022}
Song, H., Markowitz, D. M., \& Taylor, S. H. (2022). Trusting on the
shoulders of open giants? Open science increases trust in science for
the public and academics. \emph{Journal of Communication}, \emph{72}(4),
497--510. \url{https://doi.org/10.1093/joc/jqac017}

\bibitem[\citeproctext]{ref-sperberEpistemicVigilance2010}
Sperber, D., Clément, F., Heintz, C., Mascaro, O., Mercier, H., Origgi,
G., \& Wilson, D. (2010). Epistemic vigilance. \emph{Mind \& Language},
\emph{25}(4), 359--393.

\bibitem[\citeproctext]{ref-sturgisScienceSocietyReEvaluating2004}
Sturgis, P., \& Allum, N. (2004). Science in Society: Re-Evaluating the
Deficit Model of Public Attitudes. \emph{Public Understanding of
Science}, \emph{13}(1), 55--74.
\url{https://doi.org/10.1177/0963662504042690}

\bibitem[\citeproctext]{ref-sturgisTrustScienceSocial2021}
Sturgis, P., Brunton-Smith, I., \& Jackson, J. (2021). Trust in science,
social consensus and vaccine confidence. \emph{Nature Human Behaviour},
\emph{5}(11), 1528--1534.
\url{https://doi.org/10.1038/s41562-021-01115-7}

\bibitem[\citeproctext]{ref-todorovUnderstandingEvaluationFaces2008}
Todorov, A., Said, C. P., Engell, A. D., \& Oosterhof, N. N. (2008).
Understanding evaluation of faces on social dimensions. \emph{Trends in
Cognitive Sciences}, \emph{12}(12), 455--460.
\url{https://doi.org/10.1016/j.tics.2008.10.001}

\bibitem[\citeproctext]{ref-vanstekelenburgScientificConsensusCommunicationContested2022}
Van Stekelenburg, A., Schaap, G., Veling, H., Van 'T Riet, J., \&
Buijzen, M. (2022). Scientific-Consensus Communication About Contested
Science: A Preregistered Meta-Analysis. \emph{Psychological Science},
\emph{33}(12), 1989--2008.
\url{https://doi.org/10.1177/09567976221083219}

\bibitem[\citeproctext]{ref-veckalov27countryTestCommunicating2024}
Većkalov, B., Geiger, S. J., Bartoš, F., White, M. P., Rutjens, B. T.,
Harreveld, F. van, Stablum, F., Akın, B., Aldoh, A., Bai, J., Berglund,
F., Bratina Zimic, A., Broyles, M., Catania, A., Chen, A., Chorzępa, M.,
Farahat, E., Götz, J., Hoter-Ishay, B., \ldots{} Linden, S. van der.
(2024). A 27-country test of communicating the scientific consensus on
climate change. \emph{Nature Human Behaviour}, 1--14.
\url{https://doi.org/10.1038/s41562-024-01928-2}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}
Wellcome Global Monitor. (2018). \emph{Wellcome Global Monitor 2018}.
\url{https://wellcome.org/reports/wellcome-global-monitor/2018}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}
Wellcome Global Monitor. (2020). \emph{Wellcome Global Monitor 2020:
Covid-19}.
\url{https://wellcome.org/reports/wellcome-global-monitor-covid-19/2020}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorPublicTrustScientists2021}
Wellcome Global Monitor. (2021). \emph{Public trust in scientists rose
during the Covid-19 pandemic \textbar{} News}.
\url{https://wellcome.org/news/public-trust-scientists-rose-during-covid-19-pandemic}

\bibitem[\citeproctext]{ref-wintterlinPredictingPublicTrust2022}
Wintterlin, F., Hendriks, F., Mede, N. G., Bromme, R., Metag, J., \&
Schäfer, M. S. (2022). Predicting public trust in science: The role of
basic orientations toward science, perceived trustworthiness of
scientists, and experiences with science. \emph{Frontiers in
Communication}, \emph{6}.
\url{https://doi.org/10.3389/fcomm.2021.822757}

\bibitem[\citeproctext]{ref-wolpertUnnaturalNatureScience1994}
Wolpert, L. (1994). \emph{The unnatural nature of science} (1st pbk ed.,
2nd print). Harvard University Press.

\end{CSLReferences}






\end{document}
