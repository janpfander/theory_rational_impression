\documentclass[
  jou,
  floatsintext,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[bidi=default]{babel}
\babelprovide[main,import]{english}


% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}

\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\usepackage{subcaption}


\renewcommand\thesubfigure{\Alph{subfigure}}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}




\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}





\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{The rational impression account of trust in science}


\shorttitle{The rational impression account of trust in science}


\usepackage{etoolbox}









\authorsnames[{1},{2}]{Jan Pfänder,Hugo Mercier}







\authorsaffiliations{
{Department of Environmental Social Sciences, Swiss Federal Institute of
Aquatic Science and Technology (Eawag)},{Institut Jean Nicod,
Département d'études cognitives, ENS, EHESS, PSL University, CNRS,
France}}




\leftheader{Pfänder and Mercier}



\abstract{Trust in science plays a crucial role in addressing major
societal challenges, from climate change to global health. In a wide
range of countries, most people tend to trust science. However this
trust might seem irrational, since people tend to know little about
science. Here, we argue that people need not possess much knowledge or
understanding of science to rationally trust it. We propose a cognitive
model of trust in science---the rational impression account---according
to which people come to trust science by relying on a suite of basic
cognitive mechanisms: First, people infer competence from possessing
rare knowledge; Second, people infer accuracy from consensus; Third,
people's impressions can persist after they forget what generated them.
The rational impression account stresses the importance of science
education and communication in fostering public trust in science. }

\keywords{trust in science, science literacy, deficit model, epistemic
vigilance, consensus}

\authornote{\par{\addORCIDlink{Jan
Pfänder}{0009-0009-4389-2807}}\par{\addORCIDlink{Hugo
Mercier}{0000-0002-0575-7913}} 
\par{ }
\par{   The authors have no conflicts of interest to disclose.    }
\par{Correspondence concerning this article should be addressed to Jan
Pfänder, Email: \href{mailto:janlukas.pfaender@gmail.com}{janlukas.pfaender@gmail.com}}
}

\usepackage{pbalance}
% \usepackage{float}
\makeatletter
\let\oldtpt\ThreePartTable
\let\endoldtpt\endThreePartTable
\def\ThreePartTable{\@ifnextchar[\ThreePartTable@i \ThreePartTable@ii}
\def\ThreePartTable@i[#1]{\begin{figure}[!htbp]
\onecolumn
\begin{minipage}{0.485\textwidth}
\oldtpt[#1]
}
\def\ThreePartTable@ii{\begin{figure}[!htbp]
\onecolumn
\begin{minipage}{0.48\textwidth}
\oldtpt
}
\def\endThreePartTable{
\endoldtpt
\end{minipage}
\twocolumn
\end{figure}}
\makeatother


\makeatletter
\let\endoldlt\endlongtable		
\def\endlongtable{
\hline
\endoldlt}
\makeatother

\newenvironment{twocolumntable}% environment name
{% begin code
\begin{table*}[!htbp]%
\onecolumn%
}%
{%
\twocolumn%
\end{table*}%
}% end code

\urlstyle{same}



\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{placeins}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

%% Disable longtable counter
%% https://tex.stackexchange.com/a/248395/211326

\usepackage{etoolbox}

\makeatletter
\patchcmd{\LT@caption}
  {\bgroup}
  {\bgroup\global\LTpatch@captiontrue}
  {}{}
\patchcmd{\longtable}
  {\par}
  {\par\global\LTpatch@captionfalse}
  {}{}
\apptocmd{\endlongtable}
  {\ifLTpatch@caption\else\addtocounter{table}{-1}\fi}
  {}{}
\newif\ifLTpatch@caption
\makeatother

\begin{document}

\maketitle


\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\setlength\LTleft{0pt}


\section{Introduction}\label{introduction}

Addressing important societal challenges, from fighting climate change
to managing pandemics, is greatly facilitated by trust in science.
Studies have demonstrated that individuals with higher levels of trust
in science are more likely to accept the scientific consensus on global
warming (\citeproc{ref-bogertEffectTrustScience2024}{Bogert et al.,
2024}), as well as to engage in pro-environmental behavior and support
climate policies (\citeproc{ref-colognaRoleTrustClimate2020}{Cologna \&
Siegrist, 2020};
\citeproc{ref-hornseyMetaanalysesDeterminantsOutcomes2016}{Hornsey et
al., 2016}). Trust in science has also been shown to be positively
associated with willingness to get vaccinated
(\citeproc{ref-sturgisTrustScienceSocial2021}{Sturgis et al., 2021}; for
Covid-19 in particular,
\citeproc{ref-lindholtPublicAcceptanceCOVID192021}{Lindholt et al.,
2021}). During the Covid-19 pandemic, a panel study in 12 countries
found that trust in scientists was the strongest predictor of whether
people followed public health guidelines, such as mask-wearing or social
distancing Koetke et al.
(\citeproc{ref-koetkeTrustScienceIncreases2021}{2021}).

Given the individual and social cost of a lack of trust in science, most
studies have focused on understanding why some people do not trust
science. However, it also important to understand why most people
\emph{do} trust science: it is important theoretically, as this trust
could stem from very different processes--from blind deference to a
rational assessment of scientific evidence; it is important practically,
as, depending on why people trust science, different interventions aimed
at increasing trust in science could be conceived. Here, we argue that,
even though people do not know much about science, their trust in
science can still be rational.

We start by reviewing existing explanations for why people trust or do
not trust science, arguing that this work has not fully solved a basic
puzzle: Why do most people tend to trust science, in spite of knowing so
little about it?

To solve this puzzle, we then develop a \emph{rational impression
account} of trust in science. According to this account, people do not
need a profound understanding or detailed knowledge of science to
rationally trust it. Instead, by appealing to basic cognitive mechanisms
of information evaluation, science impresses people, who then mostly
forget the information that had impressed them.

\section{The puzzle of why people trust
science}\label{the-puzzle-of-why-people-trust-science}

A widely agreed-upon definition of trust is the willingness to be
vulnerable to another party--whether an individual, a group, or an
institution
(\citeproc{ref-mayerIntegrativeModelOrganizational1995a}{Mayer et al.,
1995}; \citeproc{ref-rousseauIntroductionSpecialTopic1998}{Rousseau et
al., 1998}). Building on this idea, trust in science has been defined as
``one's willingness to rely on science and scientists (as
representatives of the system) despite having a bounded understanding of
science'' (\citeproc{ref-wintterlinPredictingPublicTrust2022}{Wintterlin
et al., 2022, p. 2}). This definition implies that trust in science goes
beyond knowledge of science. Yet, the idea that knowledge of science is
the primary cause of trust in--and more generally positive attitudes
towards--science has long dominated research on public understanding of
science (\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007}). This
idea is widely known under the term ``deficit model,'' because much of
the literature attested to the public's ``depressingly low levels of
scientific knowledge'' that were assumed to be the principal cause of
negative attitudes towards science
(\citeproc{ref-sturgisScienceSocietyReEvaluating2004}{Sturgis \& Allum,
2004, p. 56}).

The deficit model has been criticized for idealizing science and viewing
the public as deficient and irrational
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011}).
Moreover, as reviewed below, the relationship between science knowledge
and trust in science is rather tenuous. As a result, the literature has
mostly moved beyond the idea of science knowledge as the main driver of
trust in science. However, the focus on explaining a lack of trust,
rather than trust, persists.

Researchers have increasingly turned to how people's values, world
views, and identities shape their attitudes towards science
(\citeproc{ref-hornseyAttitudeRootsJiu2017a}{Hornsey \& Fielding, 2017};
\citeproc{ref-lewandowskyWorldviewmotivatedRejectionScience2021}{Lewandowsky
\& Oberauer, 2021}). The psychological literature has focused on
explaining negative attitudes towards science with motivated
reasoning--selecting and interpreting information to match one's
existing beliefs or behaviors
(\citeproc{ref-hornseyWhyFactsAre2020}{Hornsey, 2020};
\citeproc{ref-lewandowskyRoleConspiracistIdeation2013}{Lewandowsky et
al., 2013};
\citeproc{ref-lewandowskyMotivatedRejectionScience2016}{Lewandowsky \&
Oberauer, 2016}). This research mostly suggests that certain
psychological traits, such as a social dominance orientation, or a
tendency to engage in conspiratorial thinking, lead people to reject
science. Arguments on a general conspiratorial thinking style as one of
the root causes of science rejection shift the debate, to some extent,
from a knowledge deficit to a broader reasoning deficit
(\citeproc{ref-hornseyAttitudeRootsJiu2017a}{Hornsey \& Fielding, 2017};
\citeproc{ref-rutjensConspiracyBeliefsScience2022}{Rutjens \& Većkalov,
2022}).

Since the Covid-19 pandemic, the role of misinformation in fostering
distrust in science has received more attention
(\citeproc{ref-druckmanThreatsSciencePoliticization2022}{Druckman,
2022};
\citeproc{ref-nationalacademiesofsciencesUnderstandingAddressingMisinformation2024}{National
Academies of Sciences, 2024};
\citeproc{ref-scheufeleScienceAudiencesMisinformation2019}{Scheufele \&
Krause, 2019}). For this literature, by contrast with the deficit model,
the problem of trust in science is less a lack of information, and more
the abundance of harmful information.

Another explanation for distrust towards science is the alienation model
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011}).
According to this model, the ``public disassociation with science is a
symptom of a general disenchantment with late modernity, mainly, the
limitations associated with codified expertise, rational bureaucracy,
and institutional authority''
(\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011, p.
2}). This explanation builds on the work of social theorists
(\citeproc{ref-beckRiskSocietyNew1992}{Beck, 1992};
\citeproc{ref-giddensModernitySelfidentitySelf1991}{Giddens, 1991};
\citeproc{ref-habermasJurgenHabermasSociety1989}{Habermas, 1989}; see
\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat, 2011} for an
overview) who suggested that a modern, complex world increasingly
requires expertise, and thus shapes institutions run by knowledge
elites. People who are not part of these institutions would experience a
lack of agency, resulting in a feeling of alienation.

Besides enquiring into the causes of (mis)trust in science, scholars
have also sought to better understand what that (mis)trust consist in,
turning for instance to the different dimensions along which scientists
are perceived
(\citeproc{ref-intemannScienceCommunicationPublic2023}{Intemann, 2023}),
including competence, integrity, benevolence, and openness
(\citeproc{ref-besleyReassessingVariablesUsed2021a}{Besley et al.,
2021}; \citeproc{ref-hendriksMeasuringLaypeoplesTrust2015}{Hendriks et
al., 2015}). This literature suggests that, for improving trust in
science, the latter, warmth-related dimensions could be particularly
relevant (\citeproc{ref-fiskeGainingTrustWell2014}{Fiske \& Dupree,
2014}): people would already perceive scientists as very competent, but
not as very warm, thus offering a greater margin for improvement.

Overall, as reviews of science-society research have noted, the
literature continues to focus on why some people do not trust science
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-scheufeleThirtyYearsScience2022}{Scheufele, 2022}), and do
not attempt to explain the elevated levels of trust in science observed
in most places in the world, as reviewed presently.

\subsection{People tend to trust
science}\label{people-tend-to-trust-science}

Across the globe, most people trust science, at least to some extent. A
recent study in 68 countries found that, across the globe, trust in
scientists was ``moderately high'' (mean = 3.62; sd= 0.70; Scale: 1 =
very low, 2 = somewhat low, 3 = neither high nor low, 4 = somewhat high,
5 = very high), with not a single country below midpoint trust
(\citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et al., 2025}).
Long-term global data on trust in science across time is sparse, yet the
available data suggests, if anything, a recent increase of trust in
science: In 2018, the Wellcome Global Monitor (WGM) surveyed more than
140,000 people in over 140 countries on trust in science
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018}). In 2020, during the first year of the Covid
pandemic and before vaccines were widely available, a follow-up survey
was conducted in 113 countries, with 119,000 participants
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{Wellcome
Global Monitor, 2020}). Between these two surveys, on average, trust in
science had risen
(\citeproc{ref-wellcomeglobalmonitorPublicTrustScientists2021}{Wellcome
Global Monitor, 2021}): In 2020, 41\% (32\% in 2018) of respondents said
they trust science a lot, 39\% (45\% in 2018) said they trust science to
some extent, 13\% (also 13\% in 2018) said they trust science ``not much
or not at all'' (with the rest answering``I don't know'').

In the US, where long term data is available from the US General Social
Survey (GSS), trust in science appears to be both remarkably stable and
elevated relative to trust in other institutions
(\citeproc{ref-funkScienceScientistsHeld2020}{Funk et al., 2020};
\citeproc{ref-funkPublicConfidenceScientists2020}{Funk \& Kennedy,
2020}; \citeproc{ref-smithTrendsPublicAttitudes2013}{Smith \& Son,
2013}): From the early 1970s to 2022, on average 43\% (average yearly
deviation = 2.3 percentage points) of Americans say they have a great
deal of confidence in the scientific community. This is the second
highest score (just behind medicine, 45\%, average yearly deviation =
5.8 percentage points) among 13 institutions listed in the GSS,
surpassing e.g., the Supreme Court, organized religion, or the
military\footnote{Numbers are based on our own calculations using on the
  publicly available GSS cumulative data}. Note, however, that the most
recent polls suggest a small drop in trust in science in the US
(\citeproc{ref-lupiaTrendsUSPublic2024}{Lupia et al., 2024}).

\subsection{People do not know much about
science}\label{people-do-not-know-much-about-science}

As mentioned above, one of the main issues with the deficit model of
trust in science is that the relatively high levels of trust in science
do not seem to be matched by commensurate levels of science knowledge.

Early attempts at measuring science knowledge have developed what is
known as the ``Oxford scale'' (Table~\ref{tbl-oxford}) to measure
science knowledge---a set of specific true/false or multiple-choice
questions about basic science facts
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016}). Survey results
from the Oxford scale have been interpreted as revealing a science
knowledge deficit among the public. For example, Durant et al.
(\citeproc{ref-durantPublicUnderstandingScience1989}{1989, p. 11})
initially reported that only ``34\% of Britons and 46\% of Americans
appeared to know that the Earth goes round the Sun once a year, and just
28\% of Britons and 25\% of Americans knew that antibiotics are
ineffective against viruses.'' According to the National Academies of
Sciences, Engineering, and Medicine
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{2016,
p. 51}), performance on the Oxford scale items in the US has been
``fairly stable across 2 decades.''

\begin{twocolumntable}

\begin{longtable}[t]{>{\raggedleft\arraybackslash}p{2em}>{\raggedright\arraybackslash}p{40em}}

\caption{\label{tbl-oxford}}

\tabularnewline

\toprule
 & \\
\midrule
1 & The center of the Earth is very hot. (True)\\
2 & The continents on which we live have been moving their locations for millions of years and will continue to move in the future. (True)\\
3 & Does the Earth go around the Sun, or does the Sun go around the Earth? (Earth around Sun)\\
4 & How long does it take for the Earth to go around the Sun? (One year)\textbackslash{}*\\
5 & All radioactivity is man-made. (False)\\
\addlinespace
6 & It is the father’s gene that decides whether the baby is a boy or a girl. (True)\\
7 & Antibiotics kill viruses as well as bacteria. (False)\\
8 & Electrons are smaller than atoms. (True)\\
9 & Lasers work by focusing sound waves. (False)\\
10 & Human beings, as we know them today, developed from earlier species of animals. (True)\\
\addlinespace
11 & The universe began with a huge explosion. (True)\\
\bottomrule
\multicolumn{2}{l}{\rule{0pt}{1em}*Only asked if previous question was answered correctly.}\\

\end{longtable}

An 11-item version of the Oxford-scale, as reported in a comprehensive
review of the literature on scientific literacy
(\citeproc{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}{National
Academies of Sciences, Engineering, and Medicine, 2016})

\end{twocolumntable}

The Oxford scale has been criticized for only capturing factual recall
(\citeproc{ref-bauerWhatCanWe2007}{Bauer et al., 2007};
\citeproc{ref-pardoCognitiveDimensionPublic2004}{Pardo \& Calvo, 2004}).
What should actually matter for trust, according to this critique, is a
different kind of knowledge, namely an institutional and methodological
understanding of how science works.

To circumvent these limitations, Durant et al.
(\citeproc{ref-durantPublicUnderstandingScience1989}{1989}) also
developed a scale of ``understanding of processes of scientific
inquiry''--several multiple-choice questions about the scientific method
and basic concepts of probability. Similarly, Miller
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{2004, p. 273})
suggested that a scientifically literate citizen was someone who had
both a ``(1) a basic vocabulary of scientific terms and constructs; and
(2) a general understanding of the nature of scientific inquiry.'' His
measure of science literacy included open-ended questions, for example
on what people understand as the meaning of scientific study
(\citeproc{ref-millerMeasurementCivicScientific1998a}{Miller, 1998}).
However, these measures have hardly drawn a more positive picture of the
public's knowledge of science: Using an index of various understanding
questions, Miller concluded that ``approximately 10 percent of US adults
qualified as civic scientifically literate in the late 1980s and early
1990s, but this proportion increased to 17 percent in 1999''
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{Miller, 2004, p.
288}). Miller explained that, according to his measure, someone
qualifies as scientifically literate if they possess ``the level of
skill required to read most of the articles in the Tuesday science
section of The New York Times, watch and understand most episodes of
Nova, or read and understand many of the popular science books sold in
bookstores today''
(\citeproc{ref-millerPublicUnderstandingAttitudes2004}{Miller, 2004, p.
288}).

More recent data suggest that science literacy in the US may have
improved slightly since Miller's assessment during the early 2000s, but
that it remains low. Based on results from the 2018 US Science \&
Engineering Indicators, Scheufele and Krause
(\citeproc{ref-scheufeleScienceAudiencesMisinformation2019}{2019, p.
7663}) report that ``one in three Americans (36\%) misunderstood the
concept of probability; half of the population (49\%) was unable to
provide a correct description of a scientific experiment; and three in
four (77\%) were unable to describe the idea of a scientific study.''
Similarly, the 2024 US Science \& Engineering Indicators, based on data
from the Pew Research Center's American Trends Panel (ATP) from 2020,
report that only ``60\% of U.S. adults could correctly note that a
control group can be useful in making sense of study results'' and that
``only half of U.S. adults (50\%) could correctly identify a scientific
hypothesis''
(\citeproc{ref-nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024}{National
Science Board, National Science Foundation, 2024, p. 24}).

Not only are levels of science knowledge and understanding low, but they
are only weakly correlated with trust in science. In a meta-analysis,
Allum et al. (\citeproc{ref-allumScienceKnowledgeAttitudes2008}{2008})
found that science knowledge measured by the Oxford scale was only
weakly associated with attitudes towards science. More recently, Cologna
et al. (\citeproc{ref-colognaTrustScientistsTheir2025}{2025}) found no
statistically significant relationship between national science literacy
scores, based on the Program for International Student Assessment
(PISA), and national average trust in scientists for the 68 countries
included in their study.

To sum up, the literature shows that trust in science is relatively
high, but that knowledge and understanding of science do not seem to be
strong determinants of this trust. Does this mean that trust in science
is irrational?

From a sociological perspective, in particular in a Bourdieusian
framework, trust in science may be strongly influenced by
\emph{habitus}---a system of dispositions shaped by one's social class
and cultural background
(\citeproc{ref-bormannTrustTrustingPractices2019}{Bormann \& Thies,
2019}; \citeproc{ref-bourdieuOutlineTheoryPractice1977}{Bourdieu,
1977}). Rather than a reasoned appraisal of science's reliability, trust
might result from internalized norms. In line with this suggestion,
Archer et al. (\citeproc{ref-archerScienceCapitalConceptual2015}{2015})
show that school children aged 11-15 years already differ considerably
in their ``science capital''---an index of several questions pertaining
to how much they value and engage with science. These differences were
associated with differences in cultural capital (e.g.~parental
university attendance), gender, and ethnicity. If, for undetermined
sociological reasons, science acquires sufficient prestige among some
segments of the population, it could lead other people to look up to
science and trust it.

While sociological factors also play a role in shaping people's
attitudes towards science, we presently introduce a model in which it
might be rational for people to trust science, even if they have little
current knowledge of it.

\section{The rational impression account of trust in
science}\label{the-rational-impression-account-of-trust-in-science}

In the rational impression account of trust in science, people trust
science because they have been impressed by it. This trust persists even
after the specific contents that gave rise to the trust have been
forgotten. The account builds on three basic cognitive mechanisms.
First, we infer that people who possess rare knowledge are more broadly
knowledgeable: If someone states something that is difficult to know,
and we believe that they are right, we are impressed, and deem that
individual competent. Second, in many situations, we infer accuracy from
consensus: If something is highly consensual, it is likely to be true.
Third, impressions can persist without recall of what generated them:
While learning about science can create lasting impressions, we are
likely to forget about specific science knowledge.

\subsection{People infer competence from rare
knowledge}\label{people-infer-competence-from-rare-knowledge}

Estimating other people's competence from communicated information is an
essential skill in a variety of social contexts, including communication
(\citeproc{ref-sperberEpistemicVigilance2010}{Sperber et al., 2010}),
cooperation (\citeproc{ref-cuddyBIASMapBehaviors2007}{Cuddy et al.,
2007}), and social learning
(\citeproc{ref-kendalSocialLearningStrategies2018}{Kendal et al.,
2018}).

Humans use a variety of cues to estimate others' competence: from
superficial, generally unreliable cues such as facial features
(\citeproc{ref-todorovUnderstandingEvaluationFaces2008}{Todorov et al.,
2008}), to more reliable cues, such as providing good explanations
(\citeproc{ref-reimerUseHeuristicsPersuasion2004}{Reimer et al., 2004};
for children, see, e.g.,
\citeproc{ref-castelainEvidenceThatTwoYearOld2018a}{Castelain et al.,
2018}) or having made accurate predictions in the past
(\citeproc{ref-mellersIdentifyingCultivatingSuperforecasters2015}{Mellers
et al., 2015}; for children, see, e.g.,
\citeproc{ref-liuSelectiveTrustChildrens2013}{Liu et al., 2013}).

One reliable cue to competence is possessing specific pieces of
knowledge: people see others who share valuable ideas as more competent
(\citeproc{ref-altayItMyIdea2020}{Altay et al., 2020}). With trivia
questions, it has been shown that people have accurate perceptions of
whether something is hard to know or not, and that they use this
information to infer someone's competence
(\citeproc{ref-dubourgUsingNestedStructure2025}{Dubourg et al., 2025}):
knowing a rare piece of information indicates a high likelihood of
knowing more information in the same domain. In the case of science,
Pfänder, De Rouilhan, et al.
(\citeproc{ref-pfanderTrustingForgettingImpressive2025}{2025}) showed
that participants perceive some scientific findings as more impressive
than others. Reading about the more impressive scientific findings
increased participants' perceptions of both the scientists' competence
and the trustworthiness of their discipline.

For an information to be impressive, at least two criteria should be
met: (i) it is perceived as rare or hard to uncover, (ii) it is believed
to be true. Past research has shown that people have very accurate
intuitions regarding (i) (see, in particular,
\citeproc{ref-dubourgUsingNestedStructure2025}{Dubourg et al., 2025}),
but little is known about which features of an information exactly
trigger this intuition. For example, most people would probably only be
mildly impressed by someone telling them that a given tree has exactly
110,201 leaves. Even though obtaining this information implies an
exhausting counting effort, everyone in principle knows how to do it. By
contrast, finding out that it takes light
\href{https://imagine.gsfc.nasa.gov/features/cosmic/milkyway_info.html}{approximately
100,000 years to travel from one end of the Milky Way to the other} is
probably impressive to most people, as they would not know how such a
distance can be measured. From this view, most scientific knowledge is
likely to be deemed very impressive (and a survey in France showed that
people tended to trust science more if they deemed it more precise,
which is one way of being impressive,
\citeproc{ref-pfanderFrenchTrustMore2025}{Pfänder \& Mercier, 2025}).
Less obvious is how people infer (ii), i.e., that the information is
true, since, as a rule, people cannot evaluate scientific discoveries by
themselves. Below, we describe how perceived consensus might allow
people to infer that a piece of information is true, even if they do not
understand how it was acquired.

\subsection{People infer accuracy from
consensus}\label{people-infer-accuracy-from-consensus}

In order to make the best of communicated information, individuals need
to be able to evaluate it, i.e.~being able to distinguish inaccurate and
harmful from accurate and beneficial information
(\citeproc{ref-maynard-smithAnimalSignals2003}{Maynard-Smith \& Harper,
2003}). It has been argued that humans have evolved a suite of cognitive
mechanisms to serve this function, mechanisms which evaluate both the
source of a piece of information and its content
(\citeproc{ref-mercierNotBornYesterday2020}{Mercier, 2020};
\citeproc{ref-sperberEpistemicVigilance2010}{Sperber et al., 2010}).

In the case of science, if we do not already assume that people trust
scientists (i.e.~the source), it seems that we are left only with
content. However, scientific findings tend to violate our intuitions
(\citeproc{ref-cromerUncommonSenseHeretical1995}{Cromer, 1995};
\citeproc{ref-mccauleyWhyReligionNatural2011a}{McCauley, 2011};
\citeproc{ref-shtulmanScienceblindWhyOur2017}{Shtulman, 2017};
\citeproc{ref-wolpertUnnaturalNatureScience1994}{Wolpert, 1994}), and
thus their content should be intuitively deemed implausible. In some
contexts, people might be able to judge the accuracy of scientific
findings for themselves, for example when they are exposed to accessible
and convincing explanations in school
(\citeproc{ref-lombrozoSimplicityProbabilityCausal2007}{Lombrozo, 2007};
\citeproc{ref-readExplanatoryCoherenceSocial1993}{Read \&
Marcus-Newhall, 1993}; for a review, see
\citeproc{ref-lombrozoStructureFunctionExplanations2006}{Lombrozo,
2006}). For most scientific research, however, people cannot evaluate
the quality of the arguments and evidence for themselves, let alone make
their own observations (e.g.~few people understand complex analysis or
group symmetry, and even fewer have access to a particle accelerator).

It is possible, however, to rationally believe that a piece of
information is true even if it is not intuitively plausible, and if we
don't already trust its source: if enough people agree with it. The
potential of the wisdom of crowds to lead to accurate answer has been
known for centuries
(\citeproc{ref-condorcetEssaiLapplicationLanalyse1785}{Condorcet, 1785};
\citeproc{ref-hastieRobustBeautyMajority2005}{Hastie \& Kameda, 2005})
and, on the whole, people make sound use of this heuristic, being more
likely to accept a piece of information when it is supported by a larger
majority (in relative and absolute terms, for review, see
\citeproc{ref-mercierMajorityRulesHow2019}{Mercier \& Morin, 2019}). The
wisdom of crowds literature, however, assumes that informants--the
individuals providing answers--need to be at least minimally competent
(i.e.~better than chance,
\citeproc{ref-condorcetEssaiLapplicationLanalyse1785}{Condorcet, 1785}).
This is a problem for the rational impression account, as it ultimately
seeks to explain how people come to judge scientists as competent.
Therefore, the account cannot take it for granted that the informants
are deemed competent. However, recently, Pfänder, De Courson, et al.
(\citeproc{ref-pfanderHowWiseCrowd2025}{2025}) have shown that it is
enough to assume that informants are not all biased in exactly the same
way to make justified inferences from their agreement to not only the
accuracy of their answers, but also their competence. Participants who
had no prior beliefs about an answer's plausibility, or the competence
of those who provided it, deemed more convergent answers more plausible,
and those who made them more competent. This was true in abstract
scenarios (\citeproc{ref-pfanderHowWiseCrowd2025}{Pfänder, De Courson,
et al., 2025}), but other research suggests that these inferences are
justified across a wide range of real-world decision making scenarios
(\citeproc{ref-kurversHowDetectHighperforming2019}{Kurvers et al.,
2019}).

To the extent that people perceive a scientific finding as being largely
consensual within the research community, they should thus infer not
only that it is more likely to be correct, but also that the scientists
responsible for the finding are competent. Much evidence shows that, as
a rule, when people are told about the scientific consensus on a given
issue, they change their minds in the direction of the consensus (e.g.,
\citeproc{ref-vanderlindenGatewayBeliefModel2021}{Van Der Linden, 2021};
\citeproc{ref-vanstekelenburgScientificConsensusCommunicationContested2022}{Van
Stekelenburg et al., 2022};
\citeproc{ref-veckalov27countryTestCommunicating2024}{Većkalov et al.,
2024}). Note, however, that participants start these experiments with a
fair degree of trust in science, so that they can rely on that to infer
that the scientists forming the consensus are competent, rather than
inferring their competence from the fact that they agree. Even if people
aren't explicitly told that a scientific consensus exists, they likely
assume that it is the case, at least for issues taken to be settled
science, such as those they are exposed to at school--and they would be
broadly right as the ability to reach a working consensus is a defining
trait of science
(\citeproc{ref-collinsSociologyPhilosophiesGlobal2002}{Collins, 2002}).
In line with this suggestion, people (in France) trust scientists more
when they work in disciplines that people perceive as more consensual
(\citeproc{ref-pfanderFrenchTrustMore2025}{Pfänder \& Mercier, 2025}).
People also trust science more when it successfully replicates---a way
of solidifying consensus
(\citeproc{ref-hendriksReplicationCrisisTrust2020}{Hendriks et al.,
2020}).

So far, we have argued that (i) people are impressed by information that
is difficult to acquire, if they believe it is true and, (ii) that they
can come to believe it is true if they take it to be consensual. Applied
to science, the prediction is that the more people are exposed to
impressive science taken to be consensual, the more they perceive
scientists as competent and, as a result, trust science more. This might
appear similar to the deficit model, in that both models predict that
exposure to science leads to more science knowledge. However, as pointed
out above, the correlation between science knowledge and attitudes
towards science, if it is positive, is weak
(\citeproc{ref-allumScienceKnowledgeAttitudes2008}{Allum et al., 2008}).
To explain this and, more generally, the low levels of knowledge of
science by comparison with trust in science, we argue that people likely
forget most specific science content they had been exposed to, while an
impression of trustworthiness persists.

\subsection{People forget specific knowledge while impressions
persist}\label{people-forget-specific-knowledge-while-impressions-persist}

We commonly form impressions of the people around us while forgetting
the details of how we formed these impressions: If a colleague fixes our
computer, we might forget how they fixed it, yet remember that they are
good at fixing computers. Similarly, people might forget the specific
content of science knowledge they have been exposed to, but retain an
impression of scientists' competence. Several research strands suggest
that impressions can persist, while recall of specific information
fades.

Memory research suggests that implicit memory is more stable than
explicit memory
(\citeproc{ref-parkinDifferentialNatureImplicit1990}{Parkin et al.,
1990}; \citeproc{ref-slomanForgettingPrimedFragment1988}{Sloman et al.,
1988}). It has also been argued that memory encodes information both as
``verbatim'' details--exact words or numbers--information and ``gist''
representations--the essence or bottom-line meaning
(\citeproc{ref-reynaScientificTheoryGist2021}{Reyna, 2021}), and that
the verbatim memory tends to fade faster
(\citeproc{ref-murphyForgettingVerbatimInformation1994}{Murphy \&
Shapiro, 1994}).

More extreme examples supporting the idea that impressions can be
detached from the memory of specific events come from medical research:
patients with severe amnesia, for instance, can continue to experience
emotions linked to events they could not recall
(\citeproc{ref-feinsteinSustainedExperienceEmotion2010}{Feinstein et
al., 2010}). Other research has shown that patients with profound
episodic memory impairment due to dementia continue to show capacity for
emotional learning
(\citeproc{ref-evans-robertsRememberingRelationshipsPreserved2010}{Evans-Roberts
\& Turnbull, 2010}).

Some research in the context of science suggests that processes of
impression formation and knowledge retention can be quite detached: in
an experiment, participants found some science-related explanations more
satisfying than others, but this did not predict how well they could
recall the explanations shortly after
(\citeproc{ref-liquinMotivatedLearnAccount2022}{Liquin \& Lombrozo,
2022}). In a study mentioned above
(\citeproc{ref-pfanderTrustingForgettingImpressive2025}{Pfänder, De
Rouilhan, et al., 2025}, which showed that being exposed to impressive
scientific content led to higher trust in the relevant scientific
discipline), another experiment showed that participants immediately
forgot most of the information which had impressed them.

Taken together, these findings make it very plausible that people, after
they have been exposed to science, might retain a positive impression of
scientists while forgetting most of the content that generated the
impression.

\subsection{Additional predictions of the rational impressions
account}\label{additional-predictions-of-the-rational-impressions-account}

The rational impression account of trust in science rests on three
already established cognitive mechanisms: (i) people deem competent
those who possess rare and impressive knowledge; (ii) people deem
opinions others converge on to be true; (iii) people tend to forget how
impressions are formed, while the impressions are maintained. This
account explains why people trust science: when scientists agree on
impressive findings, people deem that to be a good cue that the
scientists are right, and that they are competent. The account also
explains why people trust science despite not understanding or knowing
much of it: first, they don't need to understand science to deem it true
and to be impressed by it, second, the impression of competence and
trust can persist even if they don't remember the scientific knowledge
that gave rise to these impressions.

The argument we have made above suggests that people who have been
exposed to scientific content have good grounds for deeming scientists
competent. This requires that they believe the scientists aren't
conspiring to form a false consensus---but, in basic science, such
aspersions are not very plausible (why would scientists conspire to make
us believe the Milky Way has such and such size?). However, the model
doesn't require that scientists be perceived as particularly benevolent.
Scientists' benevolence should be difficult for people to evaluate, as
few personally know any scientists, few even know \emph{of} any
individual living scientists
(\citeproc{ref-researchamericaMostAmericansCannot2021}{Research!America,
2021}). As a result, there are no obvious reasons why people should rate
scientists particularly high on benevolence or related dimensions. In
line with this prediction, it has been shown that people perceive
scientists as very competent, but not as particularly warm
(\citeproc{ref-fiskeGainingTrustWell2014}{Fiske \& Dupree, 2014}). A
recent Pew survey found that 89\% of Americans viewed research
scientists as intelligent, but only 65\% viewed them as honest
(\citeproc{ref-kennedyPublicTrustScientists2024}{Kennedy \& Brian,
2024}). Beyond the US, a recent study confirmed this tendency on a
global scale (\citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et
al., 2025}): People perceived scientists as highly competent, with 78\%
tending to believe that most scientists are qualified to conduct
high-impact research. By contrast, people held scientists in lower
esteem with regards to their integrity and benevolence: Only 57\% of
people tended to believe that most scientists are honest, and only 56\%
tended to believe that most scientists are concerned about people's
well-being.

A second prediction of the rational impression account is that
education, and more specifically science education, should be the main
predictor of trust in science. Since most people consume very little
news (\citeproc{ref-newmanDigitalNewsReport2023}{Newman et al., 2023}),
and even less scientific news
(\citeproc{ref-funkScienceNewsInformation2017}{Funk et al., 2017}), the
bulk of exposure to science can be assumed to happen during education.
Education, and in particular science education, has been consistently
identified as one of the strongest predictors of trust in science
(\citeproc{ref-noyScienceGoodEffects2019}{Noy \& O'Brien, 2019};
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018},
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{2020}; but
see \citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et al., 2025}
who only find a small positive relationship between education and trust
in science, plausibly because of reduced varaition, as they tested for
tertiary education in particular). This is compatible with the fact that
people, even those who received a science education, do not know much
about science: if we assume that education has some causal effect on
trust in science, this effect does not need to be driven by the
transmission (including remembering) of knowledge and understanding (for
a similar argument, see
\citeproc{ref-bakEducationPublicAttitudes2001}{Bak, 2001}). The
candidate mechanism proposed by the rational impression account is
exposure to impressive scientific content. Students might not understand
all of it, and potentially recall even less later on; but they might
have been impressed by it, to the point that they come to perceive
scientists as competent, and thus as trustworthy. This impression might
persist even when specific knowledge vanishes. In line with this, Motta
(\citeproc{ref-mottaEnduringEffectScientific2018}{2018}) found that, in
the US, the more children were interested in science at age 12--14
years, the more they tended to trust in climate scientists in adulthood
(mid-thirties), irrespective of their political ideology.

A third prediction is that people with a basic science education should
have had ample opportunities to form impressions of trustworthiness of
science, which should have built a solid baseline of trust in science.
People might deviate from this default and distrust science on certain
specific science topics for various reasons (see below), but they should
still trust most of science. This is in line with the finding that in
the US, everyone--even people who say they don't trust science in
general or who hold specific beliefs blatantly violating scientific
knowledge (e.g.~that the earth is flat)--trusts almost all of basic
science knowledge (e.g.~that electrons are smaller than atoms,
\citeproc{ref-pfanderQuasiuniversalAcceptanceBasic2025b}{Pfänder,
Kerzreho, et al., 2025}).

\subsection{Counterarguments}\label{counterarguments}

There are several theoretical and empirical counterarguments that can be
made against the rational impression account. We address the main ones
here.

How is the rational impression account ``rational,'' if it posits that
trust is largely detached from recalling specific knowledge or from
understanding the sources of that knowledge? It is rational in that the
cognitive mechanisms it builds on lead to reliable conclusions as a
rule. Mathematical analyses and simulations show that, unless the
sources are dependent on each other, it is rational to infer from the
fact that answers converge with each other, that the sources (of these
answers) are likely to be right and to be competent. Even forgetting
specific knowledge is not irrational: given the limits on our memory, in
many cases it makes more sense to remember only the gist
(\citeproc{ref-reynaScientificTheoryGist2021}{Reyna, 2021}). Moreover,
it has been argued that one of the main functions of episodic memory is
to justify our beliefs in communication with others
(\citeproc{ref-mahrWhyWeRemember2018}{Mahr \& Csibra, 2018}). As a
result, we should be particularly good at remembering things we might
need to convince others of. In this regard, incentives of remembering
science seem to be weak: Most exposure to science happens at school, and
there is little reason for young learners' minds to anticipate having to
convince others who would disagree about, say, the chemical composition
of table salt (and indeed, basic scientific findings are nearly entirely
uncontroversial,
\citeproc{ref-pfanderQuasiuniversalAcceptanceBasic2025b}{Pfänder,
Kerzreho, et al., 2025}).

The inference from agreement to accuracy requires agreement among
several agents. As a rule, however, people do not compare the opinions
of different scientists for themselves and come to the conclusion that a
purported scientific discovery is consensual. How, then, could the
representation of consensus emerge? A plausible explanation, we believe,
is that education fosters a representation of consensus. This is in line
with observations that during education, children typically perceive
science as a unified, authoritative body of knowledge
(\citeproc{ref-careyUnderstandingNatureScientific1993}{Carey \& Smith,
1993}; \citeproc{ref-driverYoungPeoplesImages1996}{Driver et al.,
1996}). Moreover, the materials covered in science education tend to be
settled science, not the more controversial discoveries at the cutting
edge of science. This could induce a default consensus assumption in
people's perceptions of science---an assumption from which people would
carve out exceptions for domains they perceive as controversial or
politicized.

Another potential argument rests on the specific application of the
rational impression account to science. If the account relies on an
inference from agreement to accuracy, why should it be specific to
science? Such inferences could happen in any context--in fact, the
experimental evidence has been obtained in settings unrelated to science
(\citeproc{ref-pfanderHowWiseCrowd2025}{Pfänder, De Courson, et al.,
2025}). This inference is not necessarily always sound: There are
historic examples where there has been broad agreement, at least within
specific communities, on misbeliefs, such as when Christian theologians
had calculated that the Earth was approximately six thousand years old.
If people were aware of this broad agreement, and believed the
theologians to have reached this number independently of each other,
this might have led them to believe their estimate to be accurate, and
the theologians to be competent. However, maintaining a consensus around
misbeliefs is hard, and, compared to other institutions, science is
exceptionally good at producing a lasting consensus
(\citeproc{ref-collinsSociologyPhilosophiesGlobal2002}{Collins,
2002})--the theologian's value was not accepted for long, being instead
replaced by increasingly accurate scientific estimates.

Another potential issue with the current account is that even people
with no formal education, and thus presumably very little exposure to
science, have some trust in science
(\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018}). This is not naturally explained by the rational
impression account, which suggests that trust in science emerges as
people are exposed to science. A possible explanation for this could be
a two-step model of diffusion, in which some people are exposed to
science, and then talk to the people who haven't, e.g., younger
generations who attend school and discuss it with their parents (on
two-step models in general, see
\citeproc{ref-katzTwoStepFlowCommunication1957}{Katz, 1957}; in the case
of science communication in particular, see
\citeproc{ref-nisbetTwoStepFlowInfluence2009}{Nisbet \& Kotcher, 2009};
on knowledge diffusion to previous generations, see
\citeproc{ref-scribnerLiteracySchoolingTesting1978}{Scribner \& Cole,
1978}). This minimal exposure to science (which seems necessary for
people to even know what science is, which is true for all the
participants who answered questions about trust in science in the
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018}) might be sufficient to generate the low levels of
trust in science typically observed in unschooled participants.

Finally, why do some people distrust science even though they have been
extensively exposed to science ? In the global north, where essentially
everyone has been exposed to science through a basic science education,
some people do not trust some aspects of science, or say they don't
trust science in general (even if that is not true for basic science,
which everyone does trust, see
\citeproc{ref-pfanderQuasiuniversalAcceptanceBasic2025b}{Pfänder,
Kerzreho, et al., 2025}). In these cases, trust in science is likely to
be a default state, but other cognitive mechanisms can lead people to
deviate from this default, in particular for specific issues. When
answering questions about trust in science in general, people's answers
would then be overly driven by their feelings in relation to these
specific issues. Suggestions on why people tend to systematically
mistrust science on specific issues have been made, for instance about
vaccination
(\citeproc{ref-mitonCognitiveObstaclesProVaccination2015}{Miton \&
Mercier, 2015}), GMOs
(\citeproc{ref-blanckeFatalAttractionIntuitive2015}{Blancke et al.,
2015}), or nuclear energy
(\citeproc{ref-hacquinDisgustSensitivityPublic2021}{Hacquin et al.,
2021}), but more research is needed to better understand what motivates
distrust in specific aspects of science
(\citeproc{ref-hornseyWhyFactsAre2020}{Hornsey, 2020}).

\section{Discussion}\label{discussion}

In every country investigated, most people trust science at least to
some extent (\citeproc{ref-colognaTrustScientistsTheir2025}{Cologna et
al., 2025};
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}{Wellcome
Global Monitor, 2018},
\citeproc{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}{2020}).
This is true even though people, as a rule, don't understand or know
much about science, and don't consume much scientific information. The
rational impression account of trust in science accounts for these facts
by suggesting that, when people are exposed to science, mostly during
their education, they rationally develop a positive impression of
science--in particular of the competence of scientists---while then they
forget most of the information that gave rise to this impression.

The rational impression account of trust in science is supported by work
in cognitive psychology suggesting people (i) infer competence from
possessing rare and impressive knowledge; (ii) infer accuracy from
consensus; (iii) remember broad impressions without recalling the
details of the information that generated them. It is also coherent with
the observation that science education---by far the main exposure to
science for most people---is the best predictor of trust in science, and
with the fact that people who have received a science education seem to
accept nearly all of basic science.

In its stress on consensus among scientists, the rational impression
account is coherent with accounts of how science manages to yield
accurate understanding of the world, accounts that stress the importance
of having convergent results from multiple sources and methods
(\citeproc{ref-cartwrightTangleScienceReliability2022}{Cartwright et
al., 2022}; \citeproc{ref-oreskesWhyTrustScience2019}{Oreskes, 2019}).
In the present account, people are not able, in most cases, to verify
for themselves that science is accurate. However, by tracking what is
consensual among scientists, which tends to track what is accurate,
people's beliefs can remain broadly accurate (at least when they are
informed of the scientific consensus). As discussed above, this is an
ideal case scenario, when nothing gets in the way of accepting the
scientific consensus. However, people still reject some specific
scientific knowledge, for instance because it is perceived as
particularly inconvenient (e.g., knowledge of climate change which would
urge us to change our behavior), or because it is rejected by other
authorities---political, religious---which people trust (e.g.~the
rejection of evolution among some religious communities).~

The current account is very much situated at the micro-level, attempting
to explain the cognitive mechanisms through which people develop trust
in science. This account should be seen as complementary with
macro-level analyses of trust in science that raise broader historical,
sociological, or political issues about trust in science. For instance,
the rational impression account might help explain trust in science in
modern societies, in which people are massively exposed to science in
their education, but it doesn't explain why this exposure exists in the
first place, and the political forces that led to the development of
universal science education in many countries
(\citeproc{ref-atkinScienceEducationReform2003}{Atkin \& Black, 2003};
\citeproc{ref-childsCurriculumDevelopmentScience2015}{Childs, 2015}).
Moreover, the current account focuses on the competence dimension of
trust in science. People might have reasons to believe that scientists,
albeit competent, do not work for everyone's best interests, either
because they belong to groups which have been neglected or ill-treated
by science (e.g.~African Americans, see,
\citeproc{ref-brandtRacismResearchCase1978}{Brandt, 1978};
\citeproc{ref-scharffMoreTuskegeeUnderstanding2010}{Scharff et al.,
2010}), or because they feel a more general alienation, a ``general
disenchantment with late modernity, mainly, the limitations associated
with codified expertise, rational bureaucracy, and institutional
authority'' (\citeproc{ref-gauchatCulturalAuthorityScience2011}{Gauchat,
2011, p. 2}).

The rational impression account suggests that people develop trust in
science when they are exposed to impressive scientific findings believed
to be consensual among scientists. It might seem to follow that, in
order to foster trust in science, science communicators should focus on
impressive and consensual findings. While this might be effective, it is
important that the consensual nature of the findings not be exaggerated:
When uncertainty or dissensus exist, they must be acknowledged
(\citeproc{ref-druckmanCommunicatingPolicyRelevantScience2015}{Druckman,
2015}). For example, in the context of Covid-19 vaccines, Petersen et
al. (\citeproc{ref-petersenTransparentCommunicationNegative2021}{2021})
have shown that communicating uncertainty is crucial for building long
term trust in health authorities. Nor should the impressiveness of
scientific findings be exaggerated: intellectual humility has been shown
to increase trust in scientists
(\citeproc{ref-koetkeEffectSeeingScientists2024}{Koetke et al., 2024}).

The fact that people appear to forget most of the details they have
learnt about science should not discourage science communicators and
educators from attempting to properly explain science and simply ``wow''
people with impressive findings. Even if many people forget much science
knowledge, some don't, and it can prove important in their lives.
Moreover, the current mechanism isn't the only cognitive mechanism
through which people develop a trust in science. In particular in the
context of science education, students also properly understand some
scientific findings, giving them other reasons to trust science.

With these caveats in mind, we believe that the rational impressions
account offers optimism for science communication and education.
Exposure to science might be the foundation of public trust in science,
making science education and communication its main pillars. Low
scientific literacy levels should not discourage education and
communication efforts, as they are not necessarily a good indicator of
the value added in terms of fostering trust in science.

Beyond science education and communication, trust in science ultimately
rests on scientists producing accurate knowledge that can garner a
consensus and pass the test of time. We should therefore expect that
efforts at improving science methodology---the introduction of mandatory
pre-registration for clinical trials for instance
(\citeproc{ref-kaplanLikelihoodNullEffects2015}{Kaplan \& Irvin,
2015})---will result in increases in trust in science, not only when
people are aware of them (see,
\citeproc{ref-songTrustingShouldersOpen2022}{Song et al., 2022}), but
also when they are not, through the generation of more accurate, robust,
and consensual scientific knowledge.

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-alganTrustScientistsTimes2021}
Algan, Y., Cohen, D., Davoine, E., Foucault, M., \& Stantcheva, S.
(2021). Trust in scientists in times of pandemic: Panel evidence from 12
countries. \emph{Proceedings of the National Academy of Sciences},
\emph{118}(40), e2108576118.
\url{https://doi.org/10.1073/pnas.2108576118}

\bibitem[\citeproctext]{ref-allumScienceKnowledgeAttitudes2008}
Allum, N., Sturgis, P., Tabourazi, D., \& Brunton-Smith, I. (2008).
Science knowledge and attitudes across cultures: a meta-analysis.
\emph{Public Understanding of Science}, \emph{17}(1), 35--54.
\url{https://doi.org/10.1177/0963662506070159}

\bibitem[\citeproctext]{ref-altayItMyIdea2020}
Altay, S., Majima, Y., \& Mercier, H. (2020). It's my idea! Reputation
management and idea appropriation. \emph{Evolution and Human Behavior},
\emph{41}(3), 235--243.
\url{https://doi.org/10.1016/j.evolhumbehav.2020.03.004}

\bibitem[\citeproctext]{ref-archerScienceCapitalConceptual2015}
Archer, L., Dawson, E., DeWitt, J., Seakins, A., \& Wong, B. (2015).
{``}Science capital{''}: A conceptual, methodological, and empirical
argument for extending bourdieusian notions of capital beyond the arts.
\emph{Journal of Research in Science Teaching}, \emph{52}(7), 922--948.
\url{https://doi.org/10.1002/tea.21227}

\bibitem[\citeproctext]{ref-atkinScienceEducationReform2003}
Atkin, J. M., \& Black, P. J. (2003). \emph{Inside science education
reform: a history of curricular and policy change}. Teachers College
Press.

\bibitem[\citeproctext]{ref-bakEducationPublicAttitudes2001}
Bak, H.-J. (2001). Education and Public Attitudes toward Science:
Implications for the {``}Deficit Model{''} of Education and Support for
Science and Technology. \emph{Social Science Quarterly}, \emph{82}(4),
779--795. \url{https://doi.org/10.1111/0038-4941.00059}

\bibitem[\citeproctext]{ref-bauerWhatCanWe2007}
Bauer, M. W., Allum, N., \& Miller, S. (2007). What can we learn from 25
years of PUS survey research? Liberating and expanding the agenda.
\emph{Public Understanding of Science}, \emph{16}(1), 79--95.
\url{https://doi.org/10.1177/0963662506071287}

\bibitem[\citeproctext]{ref-beckRiskSocietyNew1992}
Beck, U. (1992). \emph{Risk society: towards a new modernity} (Repr).
Sage.

\bibitem[\citeproctext]{ref-besleyReassessingVariablesUsed2021a}
Besley, J. C., Lee, N. M., \& Pressgrove, G. (2021). Reassessing the
Variables Used to Measure Public Perceptions of Scientists.
\emph{Science Communication}, \emph{43}(1), 3--32.
\url{https://doi.org/10.1177/1075547020949547}

\bibitem[\citeproctext]{ref-blanckeFatalAttractionIntuitive2015}
Blancke, S., Van Breusegem, F., De Jaeger, G., Braeckman, J., \& Van
Montagu, M. (2015). Fatal attraction: the intuitive appeal of GMO
opposition. \emph{Trends in Plant Science}, \emph{20}(7), 414--418.
\url{https://doi.org/10.1016/j.tplants.2015.03.011}

\bibitem[\citeproctext]{ref-bogertEffectTrustScience2024}
Bogert, J. M., Buczny, Harvey, \& Ellers, J. and. (2024). The effect of
trust in science and media use on public belief in anthropogenic climate
change: A meta-analysis. \emph{Environmental Communication},
\emph{18}(4), 484--509.
\url{https://doi.org/10.1080/17524032.2023.2280749}

\bibitem[\citeproctext]{ref-bormannTrustTrustingPractices2019}
Bormann, I., \& Thies, B. (2019). Trust and trusting practices during
transition to higher education: Introducing a framework of habitual
trust. \emph{Educational Research}, \emph{61}(2), 161--180.
\url{https://doi.org/10.1080/00131881.2019.1596036}

\bibitem[\citeproctext]{ref-bourdieuOutlineTheoryPractice1977}
Bourdieu, P. (1977). \emph{Outline of a theory of practice} (1st ed.).
Cambridge University Press.
\url{https://doi.org/10.1017/CBO9780511812507}

\bibitem[\citeproctext]{ref-brandtRacismResearchCase1978}
Brandt, A. M. (1978). Racism and Research: The Case of the Tuskegee
Syphilis Study. \emph{The Hastings Center Report}, \emph{8}(6), 21.
\url{https://doi.org/10.2307/3561468}

\bibitem[\citeproctext]{ref-careyUnderstandingNatureScientific1993}
Carey, S., \& Smith, C. (1993). On understanding the nature of
scientific knowledge. \emph{Educational Psychologist}, \emph{28}(3),
235--251. \url{https://doi.org/10.1207/s15326985ep2803_4}

\bibitem[\citeproctext]{ref-cartwrightTangleScienceReliability2022}
Cartwright, N., Hardie, J., Montuschi, E., Soleiman, M., \& Thresher, A.
C. (2022). \emph{The tangle of science: Reliability beyond method,
rigour, and objectivity}. Oxford University Press.

\bibitem[\citeproctext]{ref-castelainEvidenceThatTwoYearOld2018a}
Castelain, T., Bernard, S., \& Mercier, H. (2018). Evidence that
Two{-}Year{-}Old Children are Sensitive to Information Presented in
Arguments. \emph{Infancy}, \emph{23}(1), 124--135.
\url{https://doi.org/10.1111/infa.12202}

\bibitem[\citeproctext]{ref-childsCurriculumDevelopmentScience2015}
Childs, P. E. (2015). Curriculum development in science - past, present
and future. \emph{Lumat: International Journal of Math, Science and
Technology Education}, \emph{3}(3), 381--400.
\url{https://doi.org/10.31129/lumat.v3i3.1036}

\bibitem[\citeproctext]{ref-collinsSociologyPhilosophiesGlobal2002}
Collins, R. (2002). \emph{The sociology of philosophies: a global theory
of intellectual change} (4. print., 1. Harvard Univ. Pr. paperback ed.,
2000). Belknap Press of Harvard Univ. Press.

\bibitem[\citeproctext]{ref-colognaTrustScientistsTheir2025}
Cologna, V., Mede, N. G., Berger, S., Besley, J., Brick, C., Joubert,
M., Maibach, E. W., Mihelj, S., Oreskes, N., Schäfer, M. S., Linden, S.
van der, Abdul Aziz, N. I., Abdulsalam, S., Shamsi, N. A., Aczel, B.,
Adinugroho, I., Alabrese, E., Aldoh, A., Alfano, M., \ldots{} Zwaan, R.
A. (2025). Trust in scientists and their role in society across 68
countries. \emph{Nature Human Behaviour}, 1--18.
\url{https://doi.org/10.1038/s41562-024-02090-5}

\bibitem[\citeproctext]{ref-colognaRoleTrustClimate2020}
Cologna, V., \& Siegrist, M. (2020). The role of trust for climate
change mitigation and adaptation behaviour: A meta-analysis.
\emph{Journal of Environmental Psychology}, \emph{69}, 101428.
\url{https://doi.org/10.1016/j.jenvp.2020.101428}

\bibitem[\citeproctext]{ref-condorcetEssaiLapplicationLanalyse1785}
Condorcet, N. (1785). \emph{Essai sur l'application de l'analyse à la
probabilité des décisions rendues à la pluralité des voix}. Paris:
L{'}imprimerie royale.

\bibitem[\citeproctext]{ref-cromerUncommonSenseHeretical1995}
Cromer, A. H. (1995). \emph{Uncommon Sense: The Heretical Nature of
Science} (1st ed). Oxford University Press, Incorporated.

\bibitem[\citeproctext]{ref-cuddyBIASMapBehaviors2007}
Cuddy, A. J. C., Fiske, S. T., \& Glick, P. (2007). The BIAS map:
Behaviors from intergroup affect and stereotypes. \emph{Journal of
Personality and Social Psychology}, \emph{92}(4), 631--648.
\url{https://doi.org/10.1037/0022-3514.92.4.631}

\bibitem[\citeproctext]{ref-driverYoungPeoplesImages1996}
Driver, R., Leach, J., Millar, R., \& Scott, P. (1996). \emph{Young
people's images of science}. McGraw-Hill Education (UK).

\bibitem[\citeproctext]{ref-druckmanCommunicatingPolicyRelevantScience2015}
Druckman, J. N. (2015). Communicating Policy-Relevant Science. \emph{PS:
Political Science \& Politics}, \emph{48}(S1), 58--69.
\url{https://doi.org/10.1017/S1049096515000438}

\bibitem[\citeproctext]{ref-druckmanThreatsSciencePoliticization2022}
Druckman, J. N. (2022). Threats to Science: Politicization,
Misinformation, and Inequalities. \emph{The ANNALS of the American
Academy of Political and Social Science}, \emph{700}(1), 8--24.
\url{https://doi.org/10.1177/00027162221095431}

\bibitem[\citeproctext]{ref-dubourgUsingNestedStructure2025}
Dubourg, E., Dheilly, T., Mercier, H., \& Morin, O. (2025). Using the
Nested Structure of Knowledge to Infer What Others Know.
\emph{Psychological Science}, \emph{36}(6), 443--450.
\url{https://doi.org/10.1177/09567976251339633}

\bibitem[\citeproctext]{ref-durantPublicUnderstandingScience1989}
Durant, J. R., Evans, G. A., \& Thomas, G. P. (1989). The public
understanding of science. \emph{Nature}, \emph{340}(6228), 11--14.
\url{https://doi.org/10.1038/340011a0}

\bibitem[\citeproctext]{ref-evans-robertsRememberingRelationshipsPreserved2010}
Evans-Roberts, C. E. Y., \& Turnbull, O. H. (2010). Remembering
Relationships: Preserved Emotion-Based Learning in Alzheimer's Disease.
\emph{Experimental Aging Research}, \emph{37}(1), 1--16.
\url{https://doi.org/10.1080/0361073X.2011.536750}

\bibitem[\citeproctext]{ref-feinsteinSustainedExperienceEmotion2010}
Feinstein, J. S., Duff, M. C., \& Tranel, D. (2010). Sustained
experience of emotion after loss of memory in patients with amnesia.
\emph{Proceedings of the National Academy of Sciences}, \emph{107}(17),
7674--7679. \url{https://doi.org/10.1073/pnas.0914054107}

\bibitem[\citeproctext]{ref-fiskeGainingTrustWell2014}
Fiske, S. T., \& Dupree, C. (2014). Gaining trust as well as respect in
communicating to motivated audiences about science topics.
\emph{Proceedings of the National Academy of Sciences},
\emph{111}(Supplement{\_}4), 13593--13597.
\url{https://doi.org/10.1073/pnas.1317505111}

\bibitem[\citeproctext]{ref-funkScienceNewsInformation2017}
Funk, C., Gottfried, J., \& Mitchell, A. (2017). \emph{Science news and
information today}.
\url{https://www.pewresearch.org/science/2017/09/20/science-news-and-information-today/}

\bibitem[\citeproctext]{ref-funkPublicConfidenceScientists2020}
Funk, C., \& Kennedy, B. (2020). \emph{Public confidence in scientists
has remained stable for decades}.
\url{https://www.pewresearch.org/short-reads/2020/08/27/public-confidence-in-scientists-has-remained-stable-for-decades/}

\bibitem[\citeproctext]{ref-funkScienceScientistsHeld2020}
Funk, C., Tyson, A., Kennedy, B., \& Johnson, C. (2020). \emph{Science
and scientists held in high esteem across global publics}.
\url{https://www.pewresearch.org/science/2020/09/29/science-and-scientists-held-in-high-esteem-across-global-publics/}

\bibitem[\citeproctext]{ref-gauchatCulturalAuthorityScience2011}
Gauchat, G. (2011). The cultural authority of science: Public trust and
acceptance of organized science. \emph{Public Understanding of Science},
\emph{20}(6), 751--770. \url{https://doi.org/10.1177/0963662510365246}

\bibitem[\citeproctext]{ref-giddensModernitySelfidentitySelf1991}
Giddens, A. (1991). \emph{Modernity and self-identity: Self and society
in the late modern age}. Stanford University Press.

\bibitem[\citeproctext]{ref-habermasJurgenHabermasSociety1989}
Habermas, J. (1989). \emph{Jürgen Habermas on society and politics: a
reader} (S. Seidman, Ed.; 1. {[}print.{]}). Beacon Press.

\bibitem[\citeproctext]{ref-hacquinDisgustSensitivityPublic2021}
Hacquin, A.-S., Altay, S., Aarøe, L., \& Mercier, H. (2021). Disgust
sensitivity and public opinion on nuclear energy. \emph{Journal of
Environmental Psychology}, 101749.

\bibitem[\citeproctext]{ref-hastieRobustBeautyMajority2005}
Hastie, R., \& Kameda, T. (2005). The Robust Beauty of Majority Rules in
Group Decisions. \emph{Psychological Review}, \emph{112}(2), 494--508.
\url{https://doi.org/10.1037/0033-295X.112.2.494}

\bibitem[\citeproctext]{ref-hendriksMeasuringLaypeoplesTrust2015}
Hendriks, F., Kienhues, D., \& Bromme, R. (2015). Measuring
Laypeople{'}s Trust in Experts in a Digital Age: The Muenster Epistemic
Trustworthiness Inventory (METI). \emph{PLOS ONE}, \emph{10}(10),
e0139309. \url{https://doi.org/10.1371/journal.pone.0139309}

\bibitem[\citeproctext]{ref-hendriksReplicationCrisisTrust2020}
Hendriks, F., Kienhues, D., \& Bromme, R. (2020). Replication crisis =
trust crisis? The effect of successful vs failed replications on
laypeople{'}s trust in researchers and research. \emph{Public
Understanding of Science}, \emph{29}(3), 270--288.
\url{https://doi.org/10.1177/0963662520902383}

\bibitem[\citeproctext]{ref-hornseyWhyFactsAre2020}
Hornsey, M. J. (2020). Why Facts Are Not Enough: Understanding and
Managing the Motivated Rejection of Science. \emph{Current Directions in
Psychological Science}, \emph{29}(6), 583--591.
\url{https://doi.org/10.1177/0963721420969364}

\bibitem[\citeproctext]{ref-hornseyAttitudeRootsJiu2017a}
Hornsey, M. J., \& Fielding, K. S. (2017). Attitude roots and Jiu Jitsu
persuasion: Understanding and overcoming the motivated rejection of
science. \emph{American Psychologist}, \emph{72}(5), 459--473.
\url{https://doi.org/10.1037/a0040437}

\bibitem[\citeproctext]{ref-hornseyMetaanalysesDeterminantsOutcomes2016}
Hornsey, M. J., Harris, E. A., Bain, P. G., \& Fielding, K. S. (2016).
Meta-analyses of the determinants and outcomes of belief in climate
change. \emph{Nature Climate Change}, \emph{6}(6), 622--626.
\url{https://doi.org/10.1038/nclimate2943}

\bibitem[\citeproctext]{ref-intemannScienceCommunicationPublic2023}
Intemann, K. (2023). Science communication and public trust in science.
\emph{Interdisciplinary Science Reviews}, \emph{48}(2), 350--365.
\url{https://doi.org/10.1080/03080188.2022.2152244}

\bibitem[\citeproctext]{ref-kaplanLikelihoodNullEffects2015}
Kaplan, R. M., \& Irvin, V. L. (2015). Likelihood of Null Effects of
Large NHLBI Clinical Trials Has Increased over Time. \emph{PLOS ONE},
\emph{10}(8), e0132382.
\url{https://doi.org/10.1371/journal.pone.0132382}

\bibitem[\citeproctext]{ref-katzTwoStepFlowCommunication1957}
Katz, E. (1957). The two-step flow of communication: An up-to-date
report on an hypothesis. \emph{Public Opinion Quarterly}, \emph{21}(1,
Anniversary Issue Devoted to Twenty Years of Public Opinion Research),
61. \url{https://doi.org/10.1086/266687}

\bibitem[\citeproctext]{ref-kendalSocialLearningStrategies2018}
Kendal, R. L., Boogert, N. J., Rendell, L., Laland, K. N., Webster, M.,
\& Jones, P. L. (2018). Social Learning Strategies: Bridge-Building
between Fields. \emph{Trends in Cognitive Sciences}, \emph{22}(7),
651--665. \url{https://doi.org/10.1016/j.tics.2018.04.003}

\bibitem[\citeproctext]{ref-kennedyPublicTrustScientists2024}
Kennedy, A. T., \& Brian. (2024). \emph{Public trust in scientists and
views on their role in policymaking}.
\url{https://www.pewresearch.org/science/2024/11/14/public-trust-in-scientists-and-views-on-their-role-in-policymaking/}

\bibitem[\citeproctext]{ref-koetkeEffectSeeingScientists2024}
Koetke, J., Schumann, K., Bowes, S. M., \& Vaupotič, N. (2024). The
effect of seeing scientists as intellectually humble on trust in
scientists and their research. \emph{Nature Human Behaviour}, 1--14.
\url{https://doi.org/10.1038/s41562-024-02060-x}

\bibitem[\citeproctext]{ref-koetkeTrustScienceIncreases2021}
Koetke, J., Schumann, K., \& Porter, T. (2021). Trust in science
increases conservative support for social distancing. \emph{Group
Processes \& Intergroup Relations}, \emph{24}(4), 680--697.
\url{https://doi.org/10.1177/1368430220985918}

\bibitem[\citeproctext]{ref-kurversHowDetectHighperforming2019}
Kurvers, R. H., Herzog, S. M., Hertwig, R., Krause, J., Moussaid, M.,
Argenziano, G., Zalaudek, I., Carney, P. A., \& Wolf, M. (2019). How to
detect high-performing individuals and groups: Decision similarity
predicts accuracy. \emph{Science Advances}, \emph{5}(11), eaaw9011.

\bibitem[\citeproctext]{ref-lewandowskyRoleConspiracistIdeation2013}
Lewandowsky, S., Gignac, G. E., \& Oberauer, K. (2013). The Role of
Conspiracist Ideation and Worldviews in Predicting Rejection of Science.
\emph{PLOS ONE}, \emph{8}(10), e75637.
\url{https://doi.org/10.1371/journal.pone.0075637}

\bibitem[\citeproctext]{ref-lewandowskyMotivatedRejectionScience2016}
Lewandowsky, S., \& Oberauer, K. (2016). Motivated Rejection of Science.
\emph{Current Directions in Psychological Science}, \emph{25}(4),
217--222. \url{https://doi.org/10.1177/0963721416654436}

\bibitem[\citeproctext]{ref-lewandowskyWorldviewmotivatedRejectionScience2021}
Lewandowsky, S., \& Oberauer, K. (2021). Worldview-motivated rejection
of science and the norms of science. \emph{Cognition}, \emph{215},
104820. \url{https://doi.org/10.1016/j.cognition.2021.104820}

\bibitem[\citeproctext]{ref-lindholtPublicAcceptanceCOVID192021}
Lindholt, M. F., Jørgensen, F., Bor, A., \& Petersen, M. B. (2021).
Public acceptance of COVID-19 vaccines: cross-national evidence on
levels and individual-level predictors using observational data.
\emph{BMJ Open}, \emph{11}(6), e048172.
\url{https://doi.org/10.1136/bmjopen-2020-048172}

\bibitem[\citeproctext]{ref-liquinMotivatedLearnAccount2022}
Liquin, E. G., \& Lombrozo, T. (2022). Motivated to learn: An account of
explanatory satisfaction. \emph{Cognitive Psychology}, \emph{132},
101453. \url{https://doi.org/10.1016/j.cogpsych.2021.101453}

\bibitem[\citeproctext]{ref-liuSelectiveTrustChildrens2013}
Liu, D., Vanderbilt, K. E., \& Heyman, G. D. (2013). Selective trust:
Children's use of intention and outcome of past testimony.
\emph{Developmental Psychology}, \emph{49}(3), 439--445.
\url{https://doi.org/10.1037/a0031615}

\bibitem[\citeproctext]{ref-lombrozoStructureFunctionExplanations2006}
Lombrozo, T. (2006). The structure and function of explanations.
\emph{Trends in Cognitive Sciences}, \emph{10}(10), 464--470.
\url{https://doi.org/10.1016/j.tics.2006.08.004}

\bibitem[\citeproctext]{ref-lombrozoSimplicityProbabilityCausal2007}
Lombrozo, T. (2007). Simplicity and probability in causal explanation.
\emph{Cognitive Psychology}, \emph{55}(3), 232--257.
\url{https://doi.org/10.1016/j.cogpsych.2006.09.006}

\bibitem[\citeproctext]{ref-lupiaTrendsUSPublic2024}
Lupia, A., Allison, D. B., Jamieson, K. H., Heimberg, J., Skipper, M.,
\& Wolf, S. M. (2024). Trends in US public confidence in science and
opportunities for progress. \emph{Proceedings of the National Academy of
Sciences}, \emph{121}(11), e2319488121.
\url{https://doi.org/10.1073/pnas.2319488121}

\bibitem[\citeproctext]{ref-mahrWhyWeRemember2018}
Mahr, J. B., \& Csibra, G. (2018). Why do we remember? The communicative
function of episodic memory. \emph{Behavioral and Brain Sciences},
\emph{41}, e1. \url{https://doi.org/10.1017/S0140525X17000012}

\bibitem[\citeproctext]{ref-mayerIntegrativeModelOrganizational1995a}
Mayer, R. C., Davis, J. H., \& Schoorman, F. D. (1995). An Integrative
Model of Organizational Trust. \emph{The Academy of Management Review},
\emph{20}(3), 709. \url{https://doi.org/10.2307/258792}

\bibitem[\citeproctext]{ref-maynard-smithAnimalSignals2003}
Maynard-Smith, J., \& Harper, D. (2003). \emph{Animal signals}. Oxford
University Press.

\bibitem[\citeproctext]{ref-mccauleyWhyReligionNatural2011a}
McCauley, R. N. (2011). \emph{Why religion is natural and science is
not}. Oxford University Press.

\bibitem[\citeproctext]{ref-mellersIdentifyingCultivatingSuperforecasters2015}
Mellers, B., Stone, E., Murray, T., Minster, A., Rohrbaugh, N., Bishop,
M., Chen, E., Baker, J., Hou, Y., Horowitz, M., Ungar, L., \& Tetlock,
P. (2015). Identifying and Cultivating Superforecasters as a Method of
Improving Probabilistic Predictions. \emph{Perspectives on Psychological
Science}, \emph{10}(3), 267--281.
\url{https://doi.org/10.1177/1745691615577794}

\bibitem[\citeproctext]{ref-mercierNotBornYesterday2020}
Mercier, H. (2020). \emph{Not born yesterday: the science of who we
trust and what we believe}. \url{https://doi.org/10.1515/9780691198842}

\bibitem[\citeproctext]{ref-mercierMajorityRulesHow2019}
Mercier, H., \& Morin, O. (2019). Majority rules: how good are we at
aggregating convergent opinions? \emph{Evolutionary Human Sciences},
\emph{1}, e6. \url{https://doi.org/10.1017/ehs.2019.6}

\bibitem[\citeproctext]{ref-millerMeasurementCivicScientific1998a}
Miller, J. D. (1998). The measurement of civic scientific literacy.
\emph{Public Understanding of Science}, \emph{7}(3), 203--223.
\url{https://doi.org/10.1088/0963-6625/7/3/001}

\bibitem[\citeproctext]{ref-millerPublicUnderstandingAttitudes2004}
Miller, J. D. (2004). Public Understanding of, and Attitudes toward,
Scientific Research: What We Know and What We Need to Know. \emph{Public
Understanding of Science}, \emph{13}(3), 273--294.
\url{https://doi.org/10.1177/0963662504044908}

\bibitem[\citeproctext]{ref-mitonCognitiveObstaclesProVaccination2015}
Miton, H., \& Mercier, H. (2015). Cognitive Obstacles to Pro-Vaccination
Beliefs. \emph{Trends in Cognitive Sciences}, \emph{19}(11), 633--636.
\url{https://doi.org/10.1016/j.tics.2015.08.007}

\bibitem[\citeproctext]{ref-mottaEnduringEffectScientific2018}
Motta, M. (2018). The enduring effect of scientific interest on trust in
climate scientists in the United States. \emph{Nature Climate Change},
\emph{8}(6), 485--488. \url{https://doi.org/10.1038/s41558-018-0126-9}

\bibitem[\citeproctext]{ref-murphyForgettingVerbatimInformation1994}
Murphy, G. L., \& Shapiro, A. M. (1994). Forgetting of verbatim
information in discourse. \emph{Memory \& Cognition}, \emph{22}(1),
85--94. \url{https://doi.org/10.3758/BF03202764}

\bibitem[\citeproctext]{ref-nationalacademiesofsciencesUnderstandingAddressingMisinformation2024}
National Academies of Sciences, E. (2024). \emph{Understanding and
addressing misinformation about science}.

\bibitem[\citeproctext]{ref-nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016}
National Academies of Sciences, Engineering, and Medicine. (2016).
\emph{Science Literacy: Concepts, Contexts, and Consequences} (C. E.
Snow \& K. A. Dibner, Eds.). National Academies Press.
\url{https://doi.org/10.17226/23595}

\bibitem[\citeproctext]{ref-nationalscienceboardnationalsciencefoundationScienceTechnologyPublic2024}
National Science Board, National Science Foundation. (2024).
\emph{Science and technology: Public perceptions, awareness, and
information sources.} \url{https://ncses.nsf.gov/pubs/nsb20244}

\bibitem[\citeproctext]{ref-newmanDigitalNewsReport2023}
Newman, N., Fletcher, R., Eddy, K., Robertson, C. T., \& Nielsen, R. K.
(2023). \emph{Digital news report 2023}.

\bibitem[\citeproctext]{ref-nisbetTwoStepFlowInfluence2009}
Nisbet, M. C., \& Kotcher, J. E. (2009). A Two-Step Flow of Influence?:
Opinion-Leader Campaigns on Climate Change. \emph{Science
Communication}, \emph{30}(3), 328--354.
\url{https://doi.org/10.1177/1075547008328797}

\bibitem[\citeproctext]{ref-noyScienceGoodEffects2019}
Noy, S., \& O'Brien, T. L. (2019). Science for good? The effects of
education and national context on perceptions of science. \emph{Public
Understanding of Science}, \emph{28}(8), 897--916.
\url{https://doi.org/10.1177/0963662519863575}

\bibitem[\citeproctext]{ref-oreskesWhyTrustScience2019}
Oreskes, N. (2019). \emph{Why trust science?} Princeton University
Press.

\bibitem[\citeproctext]{ref-pardoCognitiveDimensionPublic2004}
Pardo, R., \& Calvo, F. (2004). The Cognitive Dimension of Public
Perceptions of Science: Methodological Issues. \emph{Public
Understanding of Science}, \emph{13}(3), 203--227.
\url{https://doi.org/10.1177/0963662504045002}

\bibitem[\citeproctext]{ref-parkinDifferentialNatureImplicit1990}
Parkin, A. J., Reid, T. K., \& Russo, R. (1990). On the differential
nature of implicit and explicit memory. \emph{Memory \& Cognition},
\emph{18}(5), 507--514. \url{https://doi.org/10.3758/BF03198483}

\bibitem[\citeproctext]{ref-petersenTransparentCommunicationNegative2021}
Petersen, M. B., Bor, A., Jørgensen, F., \& Lindholt, M. F. (2021).
Transparent communication about negative features of COVID-19 vaccines
decreases acceptance but increases trust. \emph{Proceedings of the
National Academy of Sciences}, \emph{118}(29), e2024597118.
\url{https://doi.org/10.1073/pnas.2024597118}

\bibitem[\citeproctext]{ref-pfanderHowWiseCrowd2025}
Pfänder, J., De Courson, B., \& Mercier, H. (2025). How wise is the
crowd: Can we infer people are accurate and competent merely because
they agree with each other? \emph{Cognition}, \emph{255}, 106005.
\url{https://doi.org/10.1016/j.cognition.2024.106005}

\bibitem[\citeproctext]{ref-pfanderTrustingForgettingImpressive2025}
Pfänder, J., De Rouilhan, S., \& Mercier, H. (2025). \emph{Trusting but
forgetting impressive science}.
\url{https://doi.org/10.31219/osf.io/argq5_v1}

\bibitem[\citeproctext]{ref-pfanderQuasiuniversalAcceptanceBasic2025b}
Pfänder, J., Kerzreho, L., \& Mercier, H. (2025). Quasi-universal
acceptance of basic science in the United States. \emph{Public
Understanding of Science}, 09636625251364407.
\url{https://doi.org/10.1177/09636625251364407}

\bibitem[\citeproctext]{ref-pfanderFrenchTrustMore2025}
Pfänder, J., \& Mercier, H. (2025). \emph{The french trust more the
sciences they perceive as precise and consensual}.
\url{https://doi.org/10.31219/osf.io/k9m6e_v1}

\bibitem[\citeproctext]{ref-readExplanatoryCoherenceSocial1993}
Read, S. J., \& Marcus-Newhall, A. (1993). Explanatory coherence in
social explanations: A parallel distributed processing account.
\emph{Journal of Personality and Social Psychology}, \emph{65}(3),
429--447. \url{https://doi.org/10.1037/0022-3514.65.3.429}

\bibitem[\citeproctext]{ref-reimerUseHeuristicsPersuasion2004}
Reimer, T., Mata, R., \& Stoecklin, M. (2004). The use of heuristics in
persuasion: Deriving cues on source expertise from argument quality.
\emph{Current Research in Social Psychology}, \emph{10}(6), 69--84.

\bibitem[\citeproctext]{ref-researchamericaMostAmericansCannot2021}
Research!America. (2021). \emph{Most americans cannot name a living
scientist or a research institution}.
\url{https://www.researchamerica.org/blog/survey-most-americans-cannot-name-a-living-scientist-or-a-research-institution/}

\bibitem[\citeproctext]{ref-reynaScientificTheoryGist2021}
Reyna, V. F. (2021). A scientific theory of gist communication and
misinformation resistance, with implications for health, education, and
policy. \emph{Proceedings of the National Academy of Sciences},
\emph{118}(15), e1912441117.
\url{https://doi.org/10.1073/pnas.1912441117}

\bibitem[\citeproctext]{ref-rousseauIntroductionSpecialTopic1998}
Rousseau, D. M., Sitkin, S. B., Burt, R. S., \& Camerer, C. (1998).
Introduction to Special Topic Forum: Not so Different after All: A
Cross-Discipline View of Trust. \emph{The Academy of Management Review},
\emph{23}(3), 393--404. \url{http://www.jstor.org/stable/259285}

\bibitem[\citeproctext]{ref-rutjensConspiracyBeliefsScience2022}
Rutjens, B. T., \& Većkalov, B. (2022). Conspiracy beliefs and science
rejection. \emph{Current Opinion in Psychology}, \emph{46}, 101392.
\url{https://doi.org/10.1016/j.copsyc.2022.101392}

\bibitem[\citeproctext]{ref-scharffMoreTuskegeeUnderstanding2010}
Scharff, D. P., Mathews, K. J., Jackson, P., Hoffsuemmer, J., Martin,
E., \& Edwards, D. (2010). More than Tuskegee: Understanding Mistrust
about Research Participation. \emph{Journal of Health Care for the Poor
and Underserved}, \emph{21}(3), 879--897.
\url{https://doi.org/10.1353/hpu.0.0323}

\bibitem[\citeproctext]{ref-scheufeleThirtyYearsScience2022}
Scheufele, D. A. (2022). Thirty years of science{\textendash}society
interfaces: What{'}s next? \emph{Public Understanding of Science},
\emph{31}(3), 297--304. \url{https://doi.org/10.1177/09636625221075947}

\bibitem[\citeproctext]{ref-scheufeleScienceAudiencesMisinformation2019}
Scheufele, D. A., \& Krause, N. M. (2019). Science audiences,
misinformation, and fake news. \emph{Proceedings of the National Academy
of Sciences}, \emph{116}(16), 7662--7669.
\url{https://doi.org/10.1073/pnas.1805871115}

\bibitem[\citeproctext]{ref-scribnerLiteracySchoolingTesting1978}
Scribner, S., \& Cole, M. (1978). Literacy without Schooling: Testing
for Intellectual Effects. \emph{Harvard Educational Review},
\emph{48}(4), 448--461.
\url{https://doi.org/10.17763/haer.48.4.f44403u05l72x375}

\bibitem[\citeproctext]{ref-shtulmanScienceblindWhyOur2017}
Shtulman, A. (2017). \emph{Scienceblind: why our intuitive theories
about the world are so often wrong}. Basic Books.

\bibitem[\citeproctext]{ref-slomanForgettingPrimedFragment1988}
Sloman, S. A., Hayman, C. A., Ohta, N., Law, J., \& Tulving, E. (1988).
Forgetting in primed fragment completion. \emph{Journal of Experimental
Psychology: Learning, Memory, and Cognition}, \emph{14}(2), 223.

\bibitem[\citeproctext]{ref-smithTrendsPublicAttitudes2013}
Smith, T. W., \& Son, J. (2013). Trends in public attitudes about
confidence in institutions. \emph{General Social Survey Final Report}.

\bibitem[\citeproctext]{ref-songTrustingShouldersOpen2022}
Song, H., Markowitz, D. M., \& Taylor, S. H. (2022). Trusting on the
shoulders of open giants? Open science increases trust in science for
the public and academics. \emph{Journal of Communication}, \emph{72}(4),
497--510. \url{https://doi.org/10.1093/joc/jqac017}

\bibitem[\citeproctext]{ref-sperberEpistemicVigilance2010}
Sperber, D., Clément, F., Heintz, C., Mascaro, O., Mercier, H., Origgi,
G., \& Wilson, D. (2010). Epistemic vigilance. \emph{Mind \& Language},
\emph{25}(4), 359--393.

\bibitem[\citeproctext]{ref-sturgisScienceSocietyReEvaluating2004}
Sturgis, P., \& Allum, N. (2004). Science in Society: Re-Evaluating the
Deficit Model of Public Attitudes. \emph{Public Understanding of
Science}, \emph{13}(1), 55--74.
\url{https://doi.org/10.1177/0963662504042690}

\bibitem[\citeproctext]{ref-sturgisTrustScienceSocial2021}
Sturgis, P., Brunton-Smith, I., \& Jackson, J. (2021). Trust in science,
social consensus and vaccine confidence. \emph{Nature Human Behaviour},
\emph{5}(11), 1528--1534.
\url{https://doi.org/10.1038/s41562-021-01115-7}

\bibitem[\citeproctext]{ref-todorovUnderstandingEvaluationFaces2008}
Todorov, A., Said, C. P., Engell, A. D., \& Oosterhof, N. N. (2008).
Understanding evaluation of faces on social dimensions. \emph{Trends in
Cognitive Sciences}, \emph{12}(12), 455--460.
\url{https://doi.org/10.1016/j.tics.2008.10.001}

\bibitem[\citeproctext]{ref-vanderlindenGatewayBeliefModel2021}
Van Der Linden, S. (2021). The Gateway Belief Model (GBM): A review and
research agenda for communicating the scientific consensus on climate
change. \emph{Current Opinion in Psychology}, \emph{42}, 7--12.
\url{https://doi.org/10.1016/j.copsyc.2021.01.005}

\bibitem[\citeproctext]{ref-vanstekelenburgScientificConsensusCommunicationContested2022}
Van Stekelenburg, A., Schaap, G., Veling, H., Van 'T Riet, J., \&
Buijzen, M. (2022). Scientific-Consensus Communication About Contested
Science: A Preregistered Meta-Analysis. \emph{Psychological Science},
\emph{33}(12), 1989--2008.
\url{https://doi.org/10.1177/09567976221083219}

\bibitem[\citeproctext]{ref-veckalov27countryTestCommunicating2024}
Većkalov, B., Geiger, S. J., Bartoš, F., White, M. P., Rutjens, B. T.,
Harreveld, F. van, Stablum, F., Akın, B., Aldoh, A., Bai, J., Berglund,
F., Bratina Zimic, A., Broyles, M., Catania, A., Chen, A., Chorzępa, M.,
Farahat, E., Götz, J., Hoter-Ishay, B., \ldots{} Linden, S. van der.
(2024). A 27-country test of communicating the scientific consensus on
climate change. \emph{Nature Human Behaviour}, 1--14.
\url{https://doi.org/10.1038/s41562-024-01928-2}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2018}
Wellcome Global Monitor. (2018). \emph{Wellcome Global Monitor 2018}.
\url{https://wellcome.org/reports/wellcome-global-monitor/2018}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorWellcomeGlobalMonitor2020}
Wellcome Global Monitor. (2020). \emph{Wellcome Global Monitor 2020:
Covid-19}.
\url{https://wellcome.org/reports/wellcome-global-monitor-covid-19/2020}

\bibitem[\citeproctext]{ref-wellcomeglobalmonitorPublicTrustScientists2021}
Wellcome Global Monitor. (2021). \emph{Public trust in scientists rose
during the Covid-19 pandemic \textbar{} News}.
\url{https://wellcome.org/news/public-trust-scientists-rose-during-covid-19-pandemic}

\bibitem[\citeproctext]{ref-wintterlinPredictingPublicTrust2022}
Wintterlin, F., Hendriks, F., Mede, N. G., Bromme, R., Metag, J., \&
Schäfer, M. S. (2022). Predicting public trust in science: The role of
basic orientations toward science, perceived trustworthiness of
scientists, and experiences with science. \emph{Frontiers in
Communication}, \emph{6}.
\url{https://doi.org/10.3389/fcomm.2021.822757}

\bibitem[\citeproctext]{ref-wolpertUnnaturalNatureScience1994}
Wolpert, L. (1994). \emph{The unnatural nature of science} (1st pbk ed.,
2nd print). Harvard University Press.

\end{CSLReferences}






\end{document}
